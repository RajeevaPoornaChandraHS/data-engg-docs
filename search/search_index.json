{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>Hello World! My name is Vedanth V Baliga, I'm a Data Engineer @ StoneX. </p> <p>This is a complete documentation of Data Engineering Tools that I'm learning from courses and on the job. </p> <p>Feel free to connect with me on LinkedIn</p>"},{"location":"#tools-and-tech-stack","title":"Tools and Tech Stack","text":"Tool / Technology Usage Notes Airflow Astronomer Data Orchestration Astronomer Azure Databricks Data Processing Databricks Pyspark Databricks Course Data Processing PySpark Databricks Spark YouTube Stuff Data Processing PySpark YouTube"},{"location":"Azure/","title":"Microsoft Azure - DP 203","text":"<p>DP 203 Study Guide - PDF</p>"},{"location":"Azure/#intro-to-de-with-azure","title":"Intro to DE with Azure","text":""},{"location":"Azure/#introduction","title":"Introduction","text":"<p>In most organizations, a data engineer is the primary role responsible for integrating, transforming, and consolidating data from various structured and unstructured data systems into structures that are suitable for building analytics solutions. </p> <p>An Azure data engineer also helps ensure that data pipelines and data stores are high-performing, efficient, organized, and reliable, given a specific set of business requirements and constraints.</p>"},{"location":"Azure/#types-of-data","title":"Types of Data","text":""},{"location":"Azure/#data-operations","title":"Data Operations","text":""},{"location":"Azure/#data-integration","title":"Data Integration","text":"<p>Data Integration involves establishing links between operational and analytical services and data sources to enable secure, reliable access to data across multiple systems. </p> <p>For example, a business process might rely on data that is spread across multiple systems, and a data engineer is required to establish links so that the required data can be extracted from all of these systems.</p>"},{"location":"Azure/#data-transformation","title":"Data Transformation","text":"<p>Operational data usually needs to be transformed into suitable structure and format for analysis, often as part of an extract, transform, and load (ETL) process; though increasingly a variation in which you extract, load, and transform (ELT) the data is used to quickly ingest the data into a data lake and then apply \"big data\" processing techniques to transform it. Regardless of the approach used, the data is prepared to support downstream analytical needs.</p>"},{"location":"Azure/#data-consolidation","title":"Data Consolidation","text":"<p>Data consolidation is the process of combining data that has been extracted from multiple data sources into a consistent structure - usually to support analytics and reporting. Commonly, data from operational systems is extracted, transformed, and loaded into analytical stores such as a data lake or data warehouse.</p>"},{"location":"Azure/#important-concepts","title":"Important Concepts","text":""},{"location":"Azure/#operational-and-analytical-data","title":"Operational and Analytical Data","text":"<p>Operational data is usually transactional data that is generated and stored by applications, often in a relational or non-relational database. Analytical data is data that has been optimized for analysis and reporting, often in a data warehouse.</p>"},{"location":"Azure/#streaming-data","title":"Streaming Data","text":"<p>Streaming data refers to perpetual sources of data that generate data values in real-time, often relating to specific events. Common sources of streaming data include internet-of-things (IoT) devices and social media feeds.</p> <p>Data engineers often need to implement solutions that capture real-time stream of data and ingest them into analytical data systems, often combining the real-time data with other application data that is processed in batches.</p>"},{"location":"Azure/#data-pipelines","title":"Data Pipelines","text":"<p>Data pipelines are used to orchestrate activities that transfer and transform data. Pipelines are the primary way in which data engineers implement repeatable extract, transform, and load (ETL) solutions that can be triggered based on a schedule or in response to events.</p>"},{"location":"Azure/#data-lakes","title":"Data Lakes","text":"<p>A data lake is a storage repository that holds large amounts of data in native, raw formats. Data lake stores are optimized for scaling to massive volumes (terabytes or petabytes) of data. The data typically comes from multiple heterogeneous sources, and may be structured, semi-structured, or unstructured.</p> <p>The idea with a data lake is to store everything in its original, untransformed state. This approach differs from a traditional data warehouse, which transforms and processes the data at the time of ingestion.</p>"},{"location":"Azure/#data-warehouses","title":"Data Warehouses","text":"<p>A data warehouse is a centralized repository of integrated data from one or more disparate sources. Data warehouses store current and historical data in relational tables that are organized into a schema that optimizes performance for analytical queries.</p> <p>Data engineers are responsible for designing and implementing relational data warehouses, and managing regular data loads into tables.</p>"},{"location":"Azure/#apache-spark","title":"Apache Spark","text":"<p>Apache Spark is a parallel processing framework that takes advantage of in-memory processing and a distributed file storage. It's a common open-source software (OSS) tool for big data scenarios.</p>"},{"location":"Azure/#microsoft-azure-data-engineering-pipeline","title":"Microsoft Azure Data Engineering Pipeline","text":""},{"location":"Azure/#quiz","title":"Quiz","text":""},{"location":"Azure/#achievement","title":"Achievement","text":""},{"location":"Azure/#azure-data-lake-gen2","title":"Azure Data Lake Gen2","text":"<p>Many BI solutions have lost out on opportunities to store unstructured data due to cost and complexity in these types of data in databases.</p> <p>Data lakes have become a common solution to this problem. A data lake provides file-based storage, usually in a distributed file system that supports high scalability for massive volumes of data. </p> <p>Organizations can store structured, semi-structured, and unstructured files in the data lake and then consume them from there in big data processing technologies, such as Apache Spark.</p> <p>Azure Data Lake Storage Gen2 provides a cloud-based solution for data lake storage in Microsoft Azure, and underpins many large-scale analytics solutions built on Azure.</p>"},{"location":"Azure/#azure-data-lake-storage-gen2","title":"Azure Data Lake Storage Gen2","text":"<p>Azure Data Lake Storage combines a file system with a storage platform to help you quickly identify insights into your data. Data Lake Storage builds on Azure Blob storage capabilities to optimize it specifically for analytics workloads. </p> <p>This integration enables analytics performance, the tiering and data lifecycle management capabilities of Blob storage, and the high-availability, security, and durability capabilities of Azure Storage.</p>"},{"location":"Azure/#benefits","title":"Benefits","text":""},{"location":"Azure/#security","title":"Security","text":"<p>Data Lake Storage supports access control lists (ACLs) and Portable Operating System Interface (POSIX) permissions that don't inherit the permissions of the parent directory. In fact, you can set permissions at a directory level or file level for the data stored within the data lake, providing a much more secure storage system.</p> <p>POSIX Style in Gen2</p> <p>In the POSIX-style model that's used by Data Lake Storage Gen2, permissions for an item are stored on the item itself. In other words, permissions for an item cannot be inherited from the parent items if the permissions are set after the child item has already been created. </p> <p>Permissions are only inherited if default permissions have been set on the parent items before the child items have been created.</p>"},{"location":"Azure/#performance","title":"Performance","text":"<p>Azure Data Lake Storage organizes the stored data into a hierarchy of directories and subdirectories, much like a file system, for easier navigation. As a result, data processing requires less computational resources, reducing both the time and cost.</p>"},{"location":"Azure/#data-redundancy","title":"Data Redundancy","text":"<p>Data Lake Storage takes advantage of the Azure Blob replication models that provide data redundancy in a single data center with locally redundant storage (LRS), or to a secondary region by using the Geo-redundant storage (GRS) option.</p>"},{"location":"Azure/#azure-data-lake-store-vs-azure-blob-storage","title":"Azure Data Lake Store vs Azure Blob Storage","text":"<p>In Azure Blob storage, you can store large amounts of unstructured (\"object\") data in a flat namespace within a blob container. Blob names can include \"/\" characters to organize blobs into virtual \"folders\", but in terms of blob manageability the blobs are stored as a single-level hierarchy in a flat namespace.</p> <p></p> <p>Azure Data Lake Storage Gen2 builds on blob storage and optimizes I/O of high-volume data by using a hierarchical namespace that organizes blob data into directories, and stores metadata about each directory and the files within it. This structure allows operations, such as directory renames and deletes, to be performed in a single atomic operation.</p> <p></p> <p>Flat namespaces, by contrast, require several operations proportionate to the number of objects in the structure. Hierarchical namespaces keep the data organized, which yields better storage and retrieval performance for an analytical use case and lowers the cost of analysis.</p> <p></p>"},{"location":"Azure/#when-to-use-what","title":"When to Use What?","text":""},{"location":"Azure/#stages-of-processing-big-data","title":"Stages of Processing Big Data","text":""},{"location":"Azure/#data-lakehouses","title":"Data Lakehouses","text":"<p>In some cases, the data warehouse uses external tables to define a relational metadata layer over files in the data lake and create a hybrid \"data lakehouse\" or \"lake database\" architecture. The data warehouse can then support analytical queries for reporting and visualization.</p>"},{"location":"Azure/#etl-architecture","title":"ETL Architecture","text":"<ul> <li> <p>Azure Synapse Analytics can host pipelines to perform ETL processing using Azure Data Factory.</p> </li> <li> <p>These processes can then load data from operational data sources and load it into a data lake hosted in Azure Data Lake Gen2.</p> </li> <li> <p>The data is then processed and loaded into a relational data warehouse in an Azure Synapse Analytics dedicated SQL pool, from where it can support data visualization and reporting using Microsoft Power BI. </p> </li> </ul>"},{"location":"Azure/#realtime-streaming-architecture","title":"Realtime Streaming Architecture","text":"<p>Increasingly, businesses and other organizations need to capture and analyze perpetual streams of data, and analyze it in real-time (or as near to real-time as possible). </p> <p>These streams of data can be generated from connected devices (often referred to as internet-of-things or IoT devices) or from data generated by users in social media platforms or other applications. Unlike traditional batch processing workloads, streaming data requires a solution that can capture and process a boundless stream of data events as they occur.</p> <p>Streaming Events Streaming events are often captured in a queue for processing. There are multiple technologies you can use to perform this task, including Azure Event Hubs as shown in the image. </p> <p>From here, the data is processed, often to aggregate data over temporal windows (for example to count the number of social media messages with a given tag every five minutes, or to calculate the average reading of an Internet connected sensor per minute). </p>"},{"location":"Azure/#data-science-and-ml","title":"Data Science and ML","text":"<p>Data science involves the statistical analysis of large volumes of data, often using tools such as Apache Spark and scripting languages such as Python. Azure Data Lake Storage Gen 2 provides a highly scalable cloud-based data store for the volumes of data required in data science workloads.</p> <p>Machine learning is a subarea of data science that deals with training predictive models. Model training requires huge amounts of data, and the ability to process that data efficiently.</p>"},{"location":"Azure/#quiz_1","title":"Quiz","text":""},{"location":"Azure/#achievement_1","title":"Achievement","text":""},{"location":"Azure/#azure-synapse-analytics","title":"Azure Synapse Analytics","text":"<p>Azure Synapse Analytics provides a single, cloud-scale platform that supports multiple analytical technologies; enabling a consolidated and integrated experience for data engineers, data analysts, data scientists, and other professionals who need to work with data.</p>"},{"location":"Azure/#analytical-workloads-handled-by-synapse-analytics","title":"Analytical Workloads Handled by Synapse Analytics","text":"<p>Azure Synapse Analytics provides a cloud platform for all of these analytical workloads through support for multiple data storage, processing, and analysis technologies in a single, integrated solution. </p> <p>The integrated design of Azure Synapse Analytics enables organizations to leverage investments and skills in multiple commonly used data technologies, including SQL, Apache Spark, and others; while providing a centrally managed service and a single, consistent user interface.</p>"},{"location":"Azure/#azure-synapse-analytics-exercise","title":"Azure Synapse Analytics Exercise","text":"<p>This is an exercise in the MS Learn Path link but its paid.</p> <p></p>"},{"location":"Azure/#sql-query-engines","title":"SQL Query Engines","text":"<p>Structured Query Language (SQL) is a ubiquitous language for querying and manipulating data, and is the foundation for relational databases, including the popular Microsoft SQL Server database platform. Azure Synapse Analytics supports SQL-based data querying and manipulation through two kinds of SQL pool.</p> <ul> <li> <p>A built-in serverless pool that is optimized for using relational SQL semantics to query file-based data in a data lake.</p> </li> <li> <p>Custom dedicated SQL pools that host relational data warehouses. The Azure Synapse SQL system uses a distributed query processing model to parallelize SQL operations, resulting in a highly scalable solution for relational data processing. </p> </li> </ul> <p>You can use the built-in serverless pool for cost-effective analysis and processing of file data in the data lake, and use dedicated SQL pools to create relational data warehouses for enterprise data modeling and reporting.</p> <p></p>"},{"location":"Azure/#exploring-synapse-analytics","title":"Exploring Synapse Analytics","text":"<p>Here is the link to experiment with Synapse Analytics.</p>"},{"location":"Azure/#quiz_2","title":"Quiz","text":""},{"location":"Azure/#achievement_2","title":"Achievement","text":""},{"location":"Azure/#azure-databricks-module","title":"Azure Databricks Module","text":""},{"location":"Azure/#what-is-databricks","title":"What is Databricks?","text":"<p>Azure Databricks is a fully managed, cloud-based data analytics platform, which empowers developers to accelerate AI and innovation by simplifying the process of building enterprise-grade data applications. Built as a joint effort by Microsoft and the team that started Apache Spark, Azure Databricks provides data science, engineering, and analytical teams with a single platform for big data processing and machine learning.</p> <p>By combining the power of Databricks, an end-to-end, managed Apache Spark platform optimized for the cloud, with the enterprise scale and security of Microsoft's Azure platform, Azure Databricks makes it simple to run large-scale Spark workloads.</p>"},{"location":"Azure/#databricks-workload-types","title":"Databricks Workload Types","text":"<p>Azure Databricks is a comprehensive platform that offers many data processing capabilities. While you can use the service to support any workload that requires scalable data processing, Azure Databricks is optimized for three specific types of data workload and associated user personas:</p> <ul> <li>Data Science and Engineering</li> <li>Machine Learning</li> <li>SQL</li> </ul>"},{"location":"Azure/#key-concepts","title":"Key Concepts","text":"<ul> <li> <p>Apache Spark clusters - Spark is a distributed data processing solution that makes use of clusters to scale processing across multiple compute nodes. Each Spark cluster has a driver node to coordinate processing jobs, and one or more worker nodes on which the processing occurs. This distributed model enables each node to operate on a subset of the job in parallel; reducing the overall time for the job to complete.</p> </li> <li> <p>Databricks File System - Databricks File System (DBFS) - While each cluster node has its own local file system (on which operating system and other node-specific files are stored), the nodes in a cluster have access to a shared, distributed file system in which they can access and operate on data files. The Databricks File System (DBFS) enables you to mount cloud storage and use it to work with and persist file-based data.</p> </li> <li> <p>Hive Metastore - Hive is an open source technology used to define a relational abstraction layer of tables over file-based data. The tables can then be queried using SQL syntax. The table definitions and details of the file system locations on which they're based is stored in the metastore for a Spark cluster.</p> </li> <li> <p>Delta Lake builds on the relational table schema abstraction over files in the data lake to add support for SQL semantics commonly found in relational database systems. Capabilities provided by Delta Lake include transaction logging, data type constraints, and the ability to incorporate streaming data into a relational table.</p> </li> <li> <p>SQL Warehouses are relational compute resources with endpoints that enable client applications to connect to an Azure Databricks workspace and use SQL to work with data in tables. The results of SQL queries can be used to create data visualizations and dashboards to support business analytics and decision making. </p> </li> </ul>"},{"location":"Azure/#quiz_3","title":"Quiz","text":""},{"location":"Azure/#achievement_3","title":"Achievement","text":""},{"location":"Azure/#using-spark-in-databricks","title":"Using Spark in Databricks","text":"<ul> <li>Describe key elements of the Apache Spark architecture.</li> <li>Create and configure a Spark cluster.</li> <li>Describe use cases for Spark.</li> <li>Use Spark to process and analyze data stored in files.</li> <li>Use Spark to visualize data.</li> </ul>"},{"location":"Azure/#high-level-overview","title":"High Level Overview","text":"<ul> <li> <p>From a high level, the Azure Databricks service launches and manages Apache Spark clusters within your Azure subscription. Apache Spark clusters are groups of computers that are treated as a single computer and handle the execution of commands issued from notebooks. </p> </li> <li> <p>Clusters enable processing of data to be parallelized across many computers to improve scale and performance. They consist of a Spark driver and worker nodes. The driver node sends work to the worker nodes and instructs them to pull data from a specified data source.</p> </li> <li> <p>In Databricks, the notebook interface is typically the driver program. This driver program contains the main loop for the program and creates distributed datasets on the cluster, then applies operations to those datasets. Driver programs access Apache Spark through a SparkSession object regardless of deployment location.</p> </li> </ul> <p></p>"},{"location":"Azure/#how-does-spark-execute-jobs","title":"How does spark execute jobs?","text":"<ul> <li> <p>Work submitted to the cluster is split into as many independent jobs as needed. This is how work is distributed across the Cluster's nodes. Jobs are further subdivided into tasks. The input to a job is partitioned into one or more partitions. These partitions are the unit of work for each slot.</p> </li> <li> <p>The secret to Spark's high performance is parallelism. Scaling vertically (by adding resources to a single computer) is limited to a finite amount of RAM, Threads and CPU speeds; but clusters scale horizontally, adding new nodes to the cluster as needed.</p> </li> </ul>"},{"location":"Azure/#parallelism-in-spark","title":"Parallelism in Spark","text":"<ul> <li> <p>The first level of parallelization is the executor - a Java virtual machine (JVM) running on a worker node, typically, one instance per node.</p> </li> <li> <p>The second level of parallelization is the slot - the number of which is determined by the number of cores and CPUs of each node.</p> </li> <li> <p>Each executor has multiple slots to which parallelized tasks can be assigned.</p> </li> <li> <p>The JVM is naturally multi-threaded, but a single JVM, such as the one coordinating the work on the driver, has a finite upper limit. By splitting the work into tasks, the driver can assign units of work to *slots in the executors on worker nodes for parallel execution.</p> </li> <li> <p>Additionally, the driver determines how to partition the data so that it can be distributed for parallel processing. So, the driver assigns a partition of data to each task so that each task knows which piece of data it is to process. Once started, each task will fetch the partition of data assigned to it.</p> </li> </ul>"},{"location":"Azure/#jobs-and-stages","title":"Jobs and Stages","text":"<p>Depending on the work being performed, multiple parallelized jobs may be required. Each job is broken down into stages. A useful analogy is to imagine that the job is to build a house:</p> <ul> <li>The first stage would be to lay the foundation.</li> <li>The second stage would be to erect the walls.</li> <li>The third stage would be to add the roof.</li> </ul> <p>Attempting to do any of these steps out of order just doesn't make sense, and may in fact be impossible. Similarly, Spark breaks each job into stages to ensure everything is done in the right order.</p>"},{"location":"Azure/#azure-cluster-architecture","title":"Azure Cluster Architecture","text":"<p>When you create an Azure Databricks workspace, a Databricks appliance is deployed as an Azure resource in your subscription. </p> <p>When you create a cluster in the workspace, you specify the types and sizes of the virtual machines (VMs) to use for both the driver and worker nodes, and some other configuration options, but Azure Databricks manages all other aspects of the cluster.</p> <p>The Databricks appliance is deployed into Azure as a managed resource group within your subscription. This resource group contains the driver and worker VMs for your clusters, along with other required resources, including a virtual network, a security group, and a storage account. </p> <p>All metadata for your cluster, such as scheduled jobs, is stored in an Azure Database with geo-replication for fault tolerance.</p> <p></p>"},{"location":"Azure/#pyspark-code-snippets","title":"PySpark Code Snippets","text":""},{"location":"Azure/#loading-data-into-dataframe","title":"Loading Data into Dataframe","text":"<pre><code>df = spark.read.load('/data/products.csv',\n    format='csv',\n    header=True\n)\ndisplay(df.limit(10))\n</code></pre>"},{"location":"Azure/#specifying-a-database-schema","title":"Specifying a Database Schema","text":"<pre><code>from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nproductSchema = StructType([\n    StructField(\"ProductID\", IntegerType()),\n    StructField(\"ProductName\", StringType()),\n    StructField(\"Category\", StringType()),\n    StructField(\"ListPrice\", FloatType())\n    ])\n\ndf = spark.read.load('/data/product-data.csv',\n    format='csv',\n    schema=productSchema,\n    header=False)\ndisplay(df.limit(10))\n</code></pre>"},{"location":"Azure/#filter-and-group-columns","title":"Filter and Group Columns","text":"<pre><code>pricelist_df = df.select(\"ProductID\", \"ListPrice\")\n</code></pre>"},{"location":"Azure/#chaining-operations","title":"Chaining Operations","text":"<pre><code>bikes_df = df.select(\"ProductName\", \"ListPrice\").where((df[\"Category\"]==\"Mountain Bikes\") | (df[\"Category\"]==\"Road Bikes\"))\ndisplay(bikes_df)\n</code></pre>"},{"location":"Azure/#group-by-aggregation","title":"Group By + Aggregation","text":"<pre><code>counts_df = df.select(\"ProductID\", \"Category\").groupBy(\"Category\").count()\ndisplay(counts_df)\n</code></pre>"},{"location":"Azure/#sql-create-db-objects-in-catalog","title":"SQL - Create db objects in catalog","text":"<p>The Spark catalog is a metastore for relational data objects such as views and tables. The Spark runtime can use the catalog to seamlessly integrate code written in any Spark-supported language with SQL expressions that may be more natural to some data analysts or developers.</p> <pre><code>df.createOrReplaceTempView(\"products\")\n</code></pre>"},{"location":"Azure/#external-tables","title":"External Tables","text":""},{"location":"Azure/#spark-api-to-access-data","title":"Spark API to Access Data","text":"<pre><code>bikes_df = spark.sql(\"SELECT ProductID, ProductName, ListPrice \\\n                      FROM products \\\n                      WHERE Category IN ('Mountain Bikes', 'Road Bikes')\")\ndisplay(bikes_df)\n</code></pre>"},{"location":"Azure/#use-sql-code-directly","title":"Use SQL Code Directly","text":"<pre><code>%sql\n\nSELECT Category, COUNT(ProductID) AS ProductCount\nFROM products\nGROUP BY Category\nORDER BY Category\n</code></pre>"},{"location":"Azure/#visualizing-data","title":"Visualizing Data","text":"<pre><code>from matplotlib import pyplot as plt\n\n# Get the data as a Pandas dataframe\ndata = spark.sql(\"SELECT Category, COUNT(ProductID) AS ProductCount \\\n                  FROM products \\\n                  GROUP BY Category \\\n                  ORDER BY Category\").toPandas()\n\n# Clear the plot area\nplt.clf()\n\n# Create a Figure\nfig = plt.figure(figsize=(12,8))\n\n# Create a bar plot of product counts by category\nplt.bar(x=data['Category'], height=data['ProductCount'], color='orange')\n\n# Customize the chart\nplt.title('Product Counts by Category')\nplt.xlabel('Category')\nplt.ylabel('Products')\nplt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\nplt.xticks(rotation=70)\n\n# Show the plot area\nplt.show()\n</code></pre>"},{"location":"Azure/#exercise-exploring-spark","title":"Exercise : Exploring Spark","text":"<p>Here is the link for the Databricks workspace that has an introduction to spark.</p>"},{"location":"Azure/#achievement_4","title":"Achievement","text":""},{"location":"Azure/#using-delta-lake-in-azure-databricks","title":"Using Delta Lake in Azure Databricks","text":"<p>Linux foundation Delta Lake is an open-source storage layer for Spark that enables relational database capabilities for batch and streaming data. By using Delta Lake, you can implement a data lakehouse architecture in Spark to support SQL_based data manipulation semantics with support for transactions and schema enforcement.</p>"},{"location":"Azure/#benefits-of-delta-lake","title":"Benefits of Delta Lake","text":"<p>Relational tables that support querying and data modification - With Delta Lake, you can store data in tables that support CRUD (create, read, update, and delete) operations. In other words, you can select, insert, update, and delete rows of data in the same way you would in a relational database system.</p> <p>Support for ACID transactions - Relational databases are designed to support transactional data modifications that provide atomicity (transactions complete as a single unit of work), consistency (transactions leave the database in a consistent state), isolation (in-process transactions can't interfere with one another), and durability (when a transaction completes, the changes it made are persisted). Delta Lake brings this same transactional support to Spark by implementing a transaction log and enforcing serializable isolation for concurrent operations.</p> <p>Data versioning and time travel -  Because all transactions are logged in the transaction log, you can track multiple versions of each table row, and even use the time travel feature to retrieve a previous version of a row in a query.</p> <p>Support for batch and streaming data - While most relational databases include tables that store static data, Spark includes native support for streaming data through the Spark Structured Streaming API. Delta Lake tables can be used as both sinks (destinations) and sources for streaming data.</p> <p>Standard formats and interoperability - The underlying data for Delta Lake tables is stored in Parquet format, which is commonly used in data lake ingestion pipelines.</p>"},{"location":"Azure/#creating-delta-lake-tables","title":"Creating Delta Lake Tables","text":""},{"location":"Azure/#create-delta-lake-table-from-a-dataframe","title":"Create Delta Lake Table From A Dataframe","text":"<pre><code># Load a file into a dataframe\ndf = spark.read.load('/data/mydata.csv', format='csv', header=True)\n\n# Save the dataframe as a delta table\ndelta_table_path = \"/delta/mydata\"\ndf.write.format(\"delta\").save(delta_table_path)\n</code></pre>"},{"location":"Azure/#making-conditional-updates","title":"Making Conditional Updates","text":"<p>While you can make data modifications in a dataframe and then replace a Delta Lake table by overwriting it, a more common pattern in a database is to insert, update or delete rows in an existing table as discrete transactional operations. To make such modifications to a Delta Lake table, you can use the DeltaTable object in the Delta Lake API, which supports update, delete, and merge operations.</p> <pre><code>from delta.tables import *\nfrom pyspark.sql.functions import *\n\n# Create a deltaTable object\ndeltaTable = DeltaTable.forPath(spark, delta_table_path)\n\n# Update the table (reduce price of accessories by 10%)\ndeltaTable.update(\n    condition = \"Category == 'Accessories'\",\n    set = { \"Price\": \"Price * 0.9\" })\n</code></pre> <p>The updates ae stored in the transaction log.</p>"},{"location":"Azure/#query-a-previous-version-of-the-table","title":"Query a Previous Version of the table","text":"<p>Delta Lake tables support versioning through the transaction log. The transaction log records modifications made to the table, noting the timestamp and version number for each transaction. You can use this logged version data to view previous versions of the table - a feature known as time travel.</p> <pre><code>df = spark.read.format(\"delta\").option(\"timestampAsOf\", '2022-01-01').load(delta_table_path)\n</code></pre>"},{"location":"Azure/#catalog-tables","title":"Catalog Tables","text":""},{"location":"Azure/#external-vs-managed-tables","title":"External vs Managed Tables","text":"<ul> <li> <p>A managed table is defined without a specified location, and the data files are stored within the storage used by the metastore. Dropping the table not only removes its metadata from the catalog, but also deletes the folder in which its data files are stored.</p> </li> <li> <p>An external table is defined for a custom file location, where the data for the table is stored. The metadata for the table is defined in the Spark catalog. Dropping the table deletes the metadata from the catalog, but doesn't affect the data files.</p> </li> </ul>"},{"location":"Azure/#creating-catalog-table-from-dataframe","title":"Creating Catalog Table From Dataframe","text":"<pre><code># Save a dataframe as a managed table\ndf.write.format(\"delta\").saveAsTable(\"MyManagedTable\")\n\n## specify a path option to save as an external table\ndf.write.format(\"delta\").option(\"path\", \"/mydata\").saveAsTable(\"MyExternalTable\")\n</code></pre>"},{"location":"Azure/#creating-table-with-sql","title":"Creating Table with SQL","text":"<pre><code>spark.sql(\"CREATE TABLE MyExternalTable USING DELTA LOCATION '/mydata'\")\n</code></pre>"},{"location":"Azure/#defining-the-table-schema","title":"Defining the Table Schema","text":"<pre><code>%sql\n\nCREATE TABLE ManagedSalesOrders\n(\n    Orderid INT NOT NULL,\n    OrderDate TIMESTAMP NOT NULL,\n    CustomerName STRING,\n    SalesTotal FLOAT NOT NULL\n)\nUSING DELTA\n</code></pre>"},{"location":"Azure/#how-to-use-catalog-tables","title":"How to use catalog tables?","text":"<p>Catalog Tables can be used like the normal relational tables.</p> <pre><code>%sql\n\nSELECT orderid, salestotal\nFROM ManagedSalesOrders\n</code></pre>"},{"location":"Azure/#spark-structured-streaming","title":"Spark Structured Streaming","text":"<p>A typical stream processing solution involves constantly reading a stream of data from a source, optionally processing it to select specific fields, aggregate and group values, or otherwise manipulate the data, and writing the results to a sink.</p> <p>Spark includes native support for streaming data through Spark Structured Streaming, an API that is based on a boundless dataframe in which streaming data is captured for processing. A Spark Structured Streaming dataframe can read data from many different kinds of streaming source, including network ports, real time message brokering services such as Azure Event Hubs or Kafka.</p>"},{"location":"Azure/#using-delta-lake-as-a-streaming-source","title":"Using Delta Lake as a Streaming Source","text":"<pre><code>from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Load a streaming dataframe from the Delta Table\nstream_df = spark.readStream.format(\"delta\") \\\n    .option(\"ignoreChanges\", \"true\") \\\n    .load(\"/delta/internetorders\")\n\n# Now you can process the streaming data in the dataframe\n# for example, show it:\nstream_df.show()\n</code></pre> <p>By default, when delta lake table is used as a streaming source, only append operations are allowed. </p>"},{"location":"Azure/#using-delta-table-as-streaming-sink","title":"Using Delta Table as streaming sink","text":"<pre><code>from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Create a stream that reads JSON data from a folder\nstreamFolder = '/streamingdata/'\njsonSchema = StructType([\n    StructField(\"device\", StringType(), False),\n    StructField(\"status\", StringType(), False)\n])\nstream_df = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\n\n# Write the stream to a delta table\ntable_path = '/delta/devicetable'\ncheckpoint_path = '/delta/checkpoint'\ndelta_stream = stream_df.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpoint_path).start(table_path)\n</code></pre>"},{"location":"Azure/#where-are-external-and-managed-tables-stored","title":"Where are External and Managed Tables Stored?","text":"<ul> <li>External tables that are defined by the path to the parquet files containing the table data.</li> <li>Managed tables, that are defined in the Hive metastore for the Spark cluster.</li> </ul>"},{"location":"Azure/#exercise-delta-tables","title":"Exercise : Delta Tables","text":"<p>Here is the link to the exercise </p> <p>Didnt Understand : Streaming Data and Delta Tables</p>"},{"location":"Azure/#quiz_4","title":"Quiz","text":""},{"location":"Azure/#achievement_5","title":"Achievement","text":""},{"location":"Azure/#sql-warehouse-in-databricks","title":"SQL Warehouse in Databricks","text":""},{"location":"Azure/#configurations-in-sql-warehouses","title":"Configurations in SQL Warehouses","text":""},{"location":"Azure/#creating-tables-and-databases","title":"Creating tables and databases","text":"<p>All SQL Warehouses contain a default database schema named default. You can use create tables in this schema in order to analyze data. However, if you need to work with multiple tables in a relational schema, or you have multiple analytical workloads where you want to manage the data (and access to it) separately, you can create custom database schema. To create a database, use the SQL editor to run a <code>CREATE DATABASE</code> or <code>CREATE SCHEMA</code> SQL statement.</p> <pre><code>CREATE SCHEMA salesdata;\n</code></pre> <pre><code>CREATE TABLE salesdata.salesorders\n(\n    orderid INT,\n    orderdate DATE,\n    customerid INT,\n    ordertotal DECIMAL\n)\nUSING DELTA\nLOCATION '/data/sales/';\n</code></pre>"},{"location":"Azure/#exercise","title":"Exercise","text":"<p>Check out the complete exercise here</p>"},{"location":"Azure/#achievement_6","title":"Achievement","text":""},{"location":"Azure/#running-azure-databricks-notebooks-in-azure-data-factory","title":"Running Azure Databricks Notebooks in Azure Data Factory","text":"<p>dapi2cb79aec4d38d78911008d521a5ecac3</p> <ol> <li> <p>Search for Data Factory and Create a new instance.</p> </li> <li> <p>Click on 'Launch Studio'</p> </li> <li> <p>Go to Databricks &gt; Notebook</p> </li> <li> <p>Under Azure Databricks &gt; Create Linked Service &gt; Enter all the parameters</p> </li> </ol>"},{"location":"Azure/#parameters-from-the-pipeline-to-notebook","title":"Parameters from the Pipeline to Notebook","text":"<p>You can use parameters to pass variable values to a notebook from the pipeline. Parameterization enables greater flexibility than using hard-coded values in the notebook code.</p>"},{"location":"Azure/#below-code-passes-the-value-data-to-the-folder-variable","title":"Below code passes the value data to the folder variable","text":"<pre><code>dbutils.widgets.text(\"folder\", \"data\")\n</code></pre>"},{"location":"Azure/#get-the-value-for-the-parameter","title":"Get the value for the parameter","text":"<pre><code>folder = dbutils.widgets.get(\"folder\")\n</code></pre>"},{"location":"Azure/#passing-output-values-in-a-notebook","title":"Passing output values in a notebook","text":"<pre><code>path = \"dbfs:/{0}/products.csv\".format(folder)\ndbutils.notebook.exit(path)\n</code></pre>"},{"location":"Azure/#exercise_1","title":"Exercise","text":"<p>Here is the link to the exercise.</p> <p>I'm getting this error about not enough nodes to run the pipeline</p> <p><code>ADD_NODES_FAILED</code> </p> <p>The pipeline is running from the ADF Studio </p>"},{"location":"Azure/#achievement_7","title":"Achievement","text":""},{"location":"Spark_Databricks_Course/","title":"Spark Databricks Course","text":""},{"location":"Spark_Databricks_Course/#pyspark-internals-for-large-scale-data-processing","title":"PySpark Internals for Large Scale Data Processing","text":""},{"location":"Spark_Databricks_Course/#internal-working-and-architecture-of-spark","title":"Internal Working and Architecture of Spark","text":""},{"location":"Spark_Databricks_Course/#high-level-overview","title":"High Level Overview","text":""},{"location":"Spark_Databricks_Course/#flow-of-the-logic","title":"Flow of the Logic","text":""},{"location":"Spark_Databricks_Course/#worker-node-architecture","title":"Worker Node Architecture","text":"<p> - There are total of 64 cores, so 64 processes can execute parallelly and each executor will have 16 processes running. - There can be partitions within each executor that can remain idle without any process running so we need to define a parameter to define how many min number of processes need to be run on each executor at once. - To avoid having idle nodes we need to keep the number of partitions as a multiple of the core processor.</p>"},{"location":"Spark_Databricks_Course/#summary-of-spark-application","title":"Summary of Spark Application","text":""},{"location":"Spark_Databricks_Course/#attributes-of-spark","title":"Attributes of Spark","text":"<ul> <li>Its Scalable.</li> <li>Its Fault Tolerant and follows lazy evaluation. First Spark makes logical plans to process the data, then it builds a DAG and hence if one worker node goes down we can still recover the data.</li> <li>Spark supports multiple programming languages like Python and Scala.</li> <li>Spark can handle streaming data also.</li> <li>Spark is known for its speed due to in memory computation model.</li> <li>Spark has rich libraries for ML and Data Processsing.</li> </ul>"},{"location":"Spark_Databricks_Course/#common-terminologies","title":"Common Terminologies","text":"<p>Spark does not immediately process the data as mentioned above. When an action is called by the developer for storage or transformation purposes, it processes the data according to the DAG and return the output to the user.</p> <p></p> <p>To store data in on heap memory we need to serialize the data.</p>"},{"location":"Spark_Databricks_Course/#stages-and-tasks","title":"Stages and Tasks","text":""},{"location":"Spark_Databricks_Course/#libraries-suppoprted-by-spark","title":"Libraries Suppoprted by Spark","text":"<ul> <li>SQL</li> <li>Streaming</li> <li>MLLib</li> <li>SparkX</li> </ul>"},{"location":"Spark_Databricks_Course/#driver-node-architecture","title":"Driver Node Architecture","text":""},{"location":"Spark_Databricks_Course/#worker-node-architecture_1","title":"Worker Node Architecture","text":""},{"location":"Spark_Databricks_Course/#on-heap-memory-architecture","title":"On Heap Memory Architecture","text":"<ul> <li>Out of 32gb, around 300mb is used by spark for disaster recovery and cannot be used by the user or the processes.</li> <li>Of the remaining (32gb - 300mb) we allocate 40% of the heap memory as the user memory and then 60% of the heap memory as the unified memory.</li> <li>In unified memory 50% is used for scheduling and 50% of the memory is used for executing.</li> </ul>"},{"location":"Spark_Databricks_Course/#api-architecture-in-spark","title":"API Architecture in Spark","text":"<p>Dataset = Best of RDD + Best Of DataFrame </p> <ul> <li> <p>RDDs were always stored on the on heap memory but dataframes can be stored on the off heap memory also.</p> </li> <li> <p>All types of datasets here are immutable.</p> </li> </ul> <p></p> <ul> <li>Strong Typed feature ensures certain things like mismatch in the datatypes is detected in compile time.</li> </ul>"},{"location":"Spark_Databricks_Course/#transformations-and-actions","title":"Transformations and Actions","text":""},{"location":"Spark_Databricks_Course/#lazy-evaluation-and-dag-logic-flow","title":"Lazy Evaluation and DAG Logic Flow","text":""},{"location":"Spark_Databricks_Course/#narrow-transformation","title":"Narrow Transformation","text":"<p>Each and every executor can work independently on the data and don't require any dependency from the others.</p> <p></p>"},{"location":"Spark_Databricks_Course/#wide-transformation","title":"Wide Transformation","text":"<p>This type of transformation involves the shuffling of data.</p> <p></p> <p><code>df.collect</code> is used to perform action after various transformations have been applied to the data.</p>"},{"location":"Spark_Databricks_Course/#on-heap-vs-off-heap-memory","title":"On heap vs Off Heap Memory","text":"<p>On Heap Memory Architecture</p> <p>Each and every executor has its own On Heap Memory </p> <p>Off Heap Memory Architecture</p> <p></p> <p>The Off Heap Memory is managed by the OS and shared by all the executors when the on heap memory runs out of space.</p> <p>The performance can be hit when we use the On Heap Memory because in the middle of any process if the on heap memory is full, then the Garbage Colector has to scan theentire memory to check and remove the unwanted memory space and then resume the process again.</p> <p></p>"},{"location":"Spark_Databricks_Course/#clusters-in-pyspark","title":"Clusters in PySpark","text":"<ul> <li> <p>If multiple users are using All Purpose Clusters the resources are shared between them.</p> </li> <li> <p>It is used in notebooks where we have to check the output of every command after executing it.</p> </li> <li> <p>Job Clusters are used for schedulle jobs and the clusters are created duing runtime.</p> </li> <li> <p>Pools are used to create and combine multiple clusters where we can command how many clusters must be active all the time. So there is no bootup time and is used for cost cutting.</p> </li> </ul>"},{"location":"Spark_Databricks_Course/#cluster-modes","title":"Cluster Modes","text":""},{"location":"Spark_Databricks_Course/#spark-architecture-diagram","title":"Spark Architecture Diagram","text":""},{"location":"Spark_Databricks_Course/#apache-spark-internals","title":"Apache Spark Internals","text":""},{"location":"Spark_Databricks_Course/#how-does-spark-execute-queries","title":"How does Spark Execute Queries?","text":""},{"location":"Spark_Databricks_Course/#spark-cluster-execution","title":"Spark Cluster Execution","text":"<ul> <li>Executor is a JVM virtual machine that runs on the workers.</li> <li>The 625mb file is divided into memory partitions and then sent to the workers for execution.</li> <li>Its an automatic parallelism process.</li> </ul>"},{"location":"Spark_Databricks_Course/#hive-metastore","title":"Hive Metastore","text":""},{"location":"Spark_Databricks_Course/#parquet-file-format","title":"Parquet File Format","text":"<ul> <li> <p>We prefer the parquet format because consider a dataset with 100 columns in it and we want to only fetch data of first three columns. We can use parquet format to do it faster compared to csv.</p> </li> <li> <p>Search for files with java in it with Linux <code>%sh ps grep 'java'</code></p> </li> <li> <p>How to read markdown files use bash <code>%fs head /databricks-dataset/README.md</code></p> </li> <li> <p>Display all mount points <code>%fs mounts</code></p> </li> <li> <p>Declare a python variable in the spark context which SQL commands can access. <code>spark.sql(f\"SET c.events_path = {events_path}\")</code></p> </li> <li> <p>Creating a table form the files and load it as a table <pre><code>CREATE TABLE IF NOT EXISTS \nevents\nUSING DELTA OPTIONS\n{path \"${c.events_path}\"}\n</code></pre></p> </li> <li>Add notebook params as widgets</li> </ul> <p><pre><code>CREATE WIDGET TEXT state default \"KA\"\n</code></pre> <pre><code>SELECT * FROM events WHERE state = getArgument(\"state\")\n</code></pre></p>"},{"location":"Spark_Databricks_Course/#what-is-there-in-spark-sql","title":"What is there in Spark SQL?","text":""},{"location":"Spark_Databricks_Course/#lazy-evaluation","title":"Lazy Evaluation","text":""},{"location":"Spark_Databricks_Course/#explicit-vs-implicit-vs-infer-schema","title":"Explicit vs Implicit vs Infer Schema","text":"<p>Fastest one is explicit since we don't need to read the data files.</p> <p></p>"},{"location":"Spark_Databricks_Course/#query-execution-process","title":"Query Execution Process","text":"<p>![](</p>"},{"location":"Spark_Databricks_Course/#dataframe-action","title":"DataFrame Action","text":"<ul> <li>anything we specify as options are actions.</li> </ul>"},{"location":"Spark_Databricks_Course/#inferring-json-schema","title":"Inferring JSON Schema","text":"<pre><code>from pyspark.sql.types import ArrayType, DoubleType, IntegerType, LongType, StringType, StructType, StructField\n\n\n\nuser_defined_schema = StructType([\n\nStructField(\"device\", StringType(), True),\n\nStructField(\"ecommerce\", StructType([\n\nStructField(\"purchaseRevenue\", DoubleType(), True),\n\nStructField(\"total_item_quantity\", LongType(), True),\n\nStructField(\"unique_items\", LongType(), True)\n\n]), True),\n\nStructField(\"event_name\", StringType(), True),\n\nStructField(\"event_previous_timestamp\", LongType(), True),\n\nStructField(\"event_timestamp\", LongType(), True),\n\nStructField(\"geo\", StructType([\n\nStructField(\"city\", StringType(), True),\n\nStructField(\"state\", StringType(), True)\n\n]), True),\n\nStructField(\"items\", ArrayType(\n\nStructType([\n\nStructField(\"coupon\", StringType(), True),\n\nStructField(\"item_id\", StringType(), True),\n\nStructField(\"item_name\", StringType(), True),\n\nStructField(\"item_revenue_in_usd\", DoubleType(), True),\n\nStructField(\"price_in_usd\", DoubleType(), True),\n\nStructField(\"quantity\", LongType(), True)\n\n])\n\n), True),\n\nStructField(\"traffic_source\", StringType(), True),\n\nStructField(\"user_first_touch_timestamp\", LongType(), True),\n\nStructField(\"user_id\", StringType(), True)\n\n])\n\n\n\nevents_df = (spark\n\n.read\n\n.schema(user_defined_schema)\n\n.json(events_json_path)\n\n)\n</code></pre> <p>There are no jobs that are spanned while running the above code since we give all the data to infer that spark needs.</p>"},{"location":"Spark_Databricks_Course/#write-dataframes-to-tables","title":"Write Dataframes to tables","text":"<pre><code>events_df.write.mode(\"overwrite\").saveAsTable(\"events\")\n</code></pre>"},{"location":"Spark_Databricks_Course/#reading-complex-json-and-performing-operations","title":"Reading Complex JSON and performing operations","text":"<pre><code>rev_df = (events_df\n\n.filter(col(\"ecommerce.purchase_revenue_in_usd\").isNotNull())\n.withColumn(\"purchase_revenue\", (col(\"ecommerce.purchase_revenue_in_usd\") * 100).cast(\"int\"))\n.withColumn(\"avg_purchase_revenue\", col(\"ecommerce.purchase_revenue_in_usd\") / col(\"ecommerce.total_item_quantity\"))\n\n.sort(col(\"avg_purchase_revenue\").desc())\n\n)\ndisplay(rev_df)\n</code></pre>"},{"location":"Spark_Databricks_Course/#selectexpr","title":"<code>selectExpr()</code>","text":"<p><pre><code>apple_df = events_df.selectExpr(\"user_id\", \"device in ('macOS', 'iOS') as apple_user\")\n\ndisplay(apple_df)\n</code></pre> </p>"},{"location":"Spark_Databricks_Course/#drop-multiple-columns","title":"Drop multiple columns","text":"<pre><code>anonymous_df = events_df.drop(\"user_id\", \"geo\", \"device\")\ndisplay(anonymous_df)\n</code></pre>"},{"location":"Spark_Databricks_Course/#create-new-columns","title":"Create New Columns","text":"<pre><code>mobile_df = events_df.withColumn(\"mobile\", col(\"device\").isin(\"iOS\", \"Android\"))\ndisplay(mobile_df)\n</code></pre>"},{"location":"Spark_Databricks_Course/#filter-to-subset-rows","title":"<code>filter()</code> to subset rows","text":"<pre><code>purchases_df = events_df.filter(\"ecommerce.total_item_quantity &gt; 0\")\ndisplay(purchases_df)   \n</code></pre> <pre><code>revenue_df = events_df.filter(col(\"ecommerce.purchase_revenue_in_usd\").isNotNull())\ndisplay(revenue_df)\n</code></pre>"},{"location":"Spark_Databricks_Course/#sort-vs-order_by","title":"<code>sort</code> vs <code>order_by</code>","text":"<p><code>sort</code> will run on individual memory partitions and <code>order_by</code> will sort all the memory partitions together.</p> <pre><code>revenue_df = events_df.filter(col(\"ecommerce.purchase_revenue_in_usd\").isNotNull())\ndisplay(revenue_df)\n</code></pre> <pre><code>decrease_sessions_df = events_df.sort(col(\"user_first_touch_timestamp\").desc(), col(\"event_timestamp\"))\ndisplay(decrease_sessions_df)\n</code></pre>"},{"location":"Spark_Databricks_Course/#aggregations","title":"Aggregations","text":""},{"location":"Spark_Databricks_Course/#group-by-operations","title":"Group By Operations","text":"<pre><code>df.groupBy(\"geo.state\", \"geo.city\")\n</code></pre>"},{"location":"Spark_Databricks_Course/#how-group-by-works","title":"How Group By Works?","text":""},{"location":"Spark_Databricks_Course/#group-data-methods","title":"Group Data Methods","text":"<p>Average Purchase Revenue for each state</p> <p><pre><code>avg_state_purchases_df = df.groupBy(\"geo.state\").avg(\"ecommerce.purchase_revenue_in_usd\")\ndisplay(avg_state_purchases_df)\n</code></pre> </p> <p>Total Quantity and sum of purchase revenue and quantity for each combo of city and state</p> <p><pre><code>city_purchase_quantities_df = df.groupBy(\"geo.state\", \"geo.city\").sum(\"ecommerce.total_item_quantity\", \"ecommerce.purchase_revenue_in_usd\")\ndisplay(city_purchase_quantities_df)\n</code></pre> </p>"},{"location":"Spark_Databricks_Course/#list-of-aggregation-functions","title":"List of Aggregation Functions","text":"<p>Multiple Aggregate Functions</p> <pre><code>from pyspark.sql.functions import avg, approx_count_distinct\nstate_aggregates_df = (df\n.groupBy(\"geo.state\")\n.agg(avg(\"ecommerce.total_item_quantity\").alias(\"avg_quantity\"),\napprox_count_distinct(\"user_id\").alias(\"distinct_users\"))\n\n)\ndisplay(state_aggregates_df)\n</code></pre>"},{"location":"Spark_Databricks_Course/#unix-timestamps","title":"Unix Timestamps","text":""},{"location":"Spark_Databricks_Course/#datetime-functions","title":"Datetime Functions","text":"<p>Refer this docs</p> <p></p> <p>Add and Subtract Dates</p> <pre><code>plus_2_df = timestamp_df.withColumn(\"plus_two_days\", date_add(col(\"timestamp\"), 2))\n</code></pre> <p>String Built In Functions</p> <p></p> <p></p> <p>Complex Data types</p> <p></p> <p>Review</p> <p></p>"},{"location":"Spark_Databricks_Course/#collection-functions","title":"Collection Functions","text":"<p><code>array_contains</code></p> <p></p> <p><code>exlpode()</code></p> <p></p> <p><code>element_at</code></p> <p></p> <p><code>collect_set</code></p> <p></p> <p>Split to extract email address</p> <pre><code>display(df.select(split(df.email, '@', 0).alias('email_handle')))\n</code></pre>"},{"location":"Spark_Databricks_Course/#collection-functions-review","title":"Collection Functions Review","text":"<p>Create a column for the size of mattress</p> <p><pre><code>mattress_df = (details_df\n.filter(array_contains(col(\"details\"), \"Mattress\"))\n.withColumn(\"size\", element_at(col(\"details\"), 2)))\ndisplay(mattress_df)\n</code></pre> </p> <p>For each email, check what mattress type they purchased.</p> <p><pre><code>size_df = mattress_df.groupBy(\"email\").agg(collect_set(\"size\").alias(\"size options\"))\ndisplay(size_df)\n</code></pre> </p>"},{"location":"Spark_Databricks_Course/#miscellaneous-functions","title":"Miscellaneous Functions","text":"<p>Joins Demo</p> <p></p> <p>Handling Null Values</p> <p></p>"},{"location":"Spark_Databricks_Course/#how-udfs-run-in-scala-and-python","title":"How UDFs run in Scala and Python?","text":"<ul> <li>In case of Scala UDFs, it lies inside the executor so there is no inter process communication is required.</li> <li>But in case of Python UDF, we will have a driver program and an executor but the Python UDFs run outside the Executor.</li> <li>This means that the Spark DataFrame rows are deserialized, sent row by row to the python UDF that transforms it, serializes it row by row and sends it back to the executors.</li> <li>The UDFs are registered on Python Interpreters.</li> </ul>"},{"location":"Spark_Databricks_Course/#transform-function-execution","title":"Transform function execution","text":"<ul> <li>The custom UDF that was written took twice as long to execute due to the extra work involved. </li> <li>The problem with UDFs is that if we write it in Python, there is extra overhead of converting it to Java bytecode and providing it to the executor.</li> </ul>"},{"location":"Spark_Databricks_Course/#how-to-register-udfs","title":"How to register UDFs?","text":""},{"location":"Spark_Databricks_Course/#python-vs-pandas-udf","title":"Python vs Pandas UDF","text":""},{"location":"Spark_Databricks_Course/#sql-udf","title":"SQL UDF","text":""},{"location":"Spark_Databricks_Course/#apache-spark-architecture","title":"Apache Spark Architecture","text":""},{"location":"Spark_Databricks_Course/#cluster-architecture","title":"Cluster Architecture","text":"<ul> <li>Each worker will have only one executor</li> </ul> <p>There is one task for each memory partition.</p> <p></p>"},{"location":"Spark_Databricks_Course/#drivers-work","title":"Driver's work","text":"<p>The rest of the memory partitions do not have any cores to execute on and wait in the queue.</p> <p></p> <p>A,E,H and J have finished working. They are idle so worker node assigns the memory partitions 13,14,15 and 16 to them.</p> <p></p> <p>Once all of them complete the work, the driver sends the answer set to the client.</p> <p></p> <ul> <li>The intermediate result sets mentioned as shuffle write and shuffle read are then sent to the worker node hard drives.</li> </ul> <p></p>"},{"location":"Spark_Databricks_Course/#deep-dive-into-shuffle","title":"Deep Dive Into Shuffle","text":"<ul> <li> <p>The memory used went from 20mb to 560 bytes because we are only storing the key value pairs and not the entire data. The key value pairs indicate the color and the number of rows that they are part of. The data is written to the disk under shuffle write.</p> </li> <li> <p>In stage 2 we build shuffle partition with Green, Red and Yellow rows.</p> </li> </ul> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"Spark_Databricks_Course/#summary","title":"Summary","text":""},{"location":"Spark_Databricks_Course/#query-optimization","title":"Query Optimization","text":"<ul> <li>RDD is resilient distributed dataset that is just an array of data.</li> </ul>"},{"location":"Spark_Databricks_Course/#example","title":"Example","text":""},{"location":"Spark_Databricks_Course/#example-2","title":"Example 2","text":""},{"location":"Spark_Databricks_Course/#adaptive-query-optimization","title":"Adaptive Query Optimization","text":""},{"location":"Spark_Databricks_Course/#shuffle-partitions-with-and-without-aqe","title":"Shuffle Partitions : With and Without AQE","text":""},{"location":"Spark_Databricks_Course/#predicate-pushdown","title":"Predicate Pushdown","text":"<ul> <li>In this case, less RAM is used so query is faster.</li> </ul> <ul> <li>If we don't remove things from cache that is not needed, when there is only one core in the executor and a query is called in, then it will be given precedence and the cache will be evicted.</li> </ul>"},{"location":"Spark_Databricks_Course/#memory-partitioning-guidelines","title":"Memory Partitioning Guidelines","text":"<p>In the below example we have 8 memory partitions and 200 shuffle partitions.</p> <p></p> <p>Check the number of cores in the cluster</p> <p></p> <p>Check the number of memory partitions</p> <p></p> <p>Repartitioning Dataset</p> <p></p> <p>Repartitioning is always a wide transformation.</p>"},{"location":"Spark_Databricks_Course/#aqe-and-shuffle-partitions","title":"AQE and Shuffle Partitions","text":"<p>We can set the <code>spark.sql.shuffle.partitions</code> based on the largest dataset that our application can process.</p>"},{"location":"Spark_YT/","title":"Spark YouTube","text":""},{"location":"Spark_YT/#spark-concepts-and-code","title":"Spark Concepts and Code.","text":""},{"location":"Spark_YT/#lecture-1-what-is-apache-spark","title":"Lecture 1 : What is Apache Spark","text":""},{"location":"Spark_YT/#unified","title":"Unified :","text":""},{"location":"Spark_YT/#computing-engine","title":"Computing Engine:","text":"<p>Spark is not storage platform we can store the data in hdfs, rdbms etc...</p> <p>Spark can process terabytes of data in distributed manner.</p>"},{"location":"Spark_YT/#compute-cluster","title":"Compute Cluster:","text":"<ul> <li>each slave has 16 GB RAM, 1TB storage and 4 core CPU</li> <li>even master has some data and RAM</li> <li>the above cluster can compute 64 gb of data at a time.</li> <li>the master divides the data among the slave nodes and then slaves process the data.</li> </ul>"},{"location":"Spark_YT/#lecture-2-why-apache-spark","title":"Lecture 2 : Why Apache Spark?","text":"<p>Different Databases </p> <p>new formats like video, audio, json,avro started coming in but rdms cannot handle it.</p> <p>volume of data also increased. </p>"},{"location":"Spark_YT/#what-is-big-data","title":"What is Big Data?","text":"<p>Data Lake works on Extract Load Transform architecture</p>"},{"location":"Spark_YT/#issues-with-rdbms","title":"Issues with RDBMS","text":"<ul> <li>Storage</li> <li>Processing - RAM and CPU</li> </ul> <p>Enter Spark...</p> <p></p>"},{"location":"Spark_YT/#monolith-vs-microservice-architecture","title":"Monolith vs Microservice Architecture","text":""},{"location":"Spark_YT/#lecture-3-hadoop-vs-spark","title":"Lecture 3 : Hadoop vs Spark","text":""},{"location":"Spark_YT/#misconception","title":"Misconception:","text":"<ul> <li>Hadoop is a database - its not a database just a filesystem (hdfs)</li> <li>Spark is 100 times faster than hadoop</li> <li>Spark processes data in RAM but Hadoop doesnt</li> </ul>"},{"location":"Spark_YT/#differences","title":"Differences","text":""},{"location":"Spark_YT/#performance","title":"Performance","text":"<p>Hadoop does lot of read write IO operations and sends data back and forth to the disk. </p> <p>But in spark each executor has its own memory. </p> <p>Where is there no difference?</p> <p>When we have very less data like 10 GB, there is no difference because the hadoop cluster also doesnt write to the disk it fits first time in memory.</p>"},{"location":"Spark_YT/#batch-vs-stream-processing","title":"Batch vs Stream Processing","text":""},{"location":"Spark_YT/#ease-of-use","title":"Ease of Use","text":"<p>Spark has both low level and high level API in Python which is easier than using Hive. Low level programming is on RDD level.</p>"},{"location":"Spark_YT/#security","title":"Security","text":"<ul> <li> <p>Hadoop has in built Kerberos Authentication via YARN whereas Spark doesnt have any security mechanism.</p> </li> <li> <p>The authentication helps create ACL lists at directory level in HDFS.</p> </li> <li> <p>Spark uses HDFS Storage so it gets ACL feature / ability and when it uses YARN it gets Kerberos Authentication.</p> </li> </ul>"},{"location":"Spark_YT/#fault-tolerance","title":"Fault Tolerance","text":"<p>Data Replication in Hadoop </p> <p>HDFS keeps track of which node / rack has the data from A B C and D</p> <p></p> <p>DAG in Spark</p> <ul> <li>So Spark computes / transforms in multiple processes Process 1 -&gt; Process 2 -&gt; Process 3 ....</li> <li>After each process the data is stored in a data structure called RDD which is immutable. So even if there is a failure Spark engine knows how to reconstruct the data for a particular process from the RDD at that stage.</li> </ul>"},{"location":"Spark_YT/#lecture-4-spark-ecosystem","title":"Lecture 4 : Spark Ecosystem","text":"<p>High Level API : We cna write any SQL queries in python,java etc... there are ML and GraphX librries also.</p> <p>We can write code in many languages. Low Level API : we can make RDD's and work on them.</p> <p></p>"},{"location":"Spark_YT/#where-does-spark-run","title":"Where does Spark run?","text":"<p>Spark Engine would need some memory for transformation.</p> <ul> <li>suppose it needs 4 worker nodes each 20 GB and a driver node of 20 gb.</li> <li>it goes to the cluster manager and asks for total 100 GB of memory, if available then the manager will assign that muuch storage.</li> <li>cluster manager is also called YARN, K8S, Standalone managers</li> </ul>"},{"location":"Spark_YT/#lecture-5-read-modes-in-spark","title":"Lecture 5 : Read Modes in Spark","text":"<p>format -&gt; data file format : csv,json,jdbc and odbc connection. Format is optional parameter, by default its parquet format option -&gt; inferschema, mode and header [optional field] schema -&gt; manual schema can be passed here load -&gt; path from where we need to read the data [not optional]</p>"},{"location":"Spark_YT/#dataframereader-api","title":"DataframeReader API","text":"<p>Access it using 'spark.read' in the spark session</p> <p></p>"},{"location":"Spark_YT/#mode-in-spark","title":"<code>mode</code> in Spark","text":""},{"location":"Spark_YT/#lecture-6-spark-architecture","title":"Lecture 6 : Spark Architecture","text":""},{"location":"Spark_YT/#spark-cluster","title":"Spark Cluster","text":"<ul> <li>20 core per machine and 100 GB RAM / each machine</li> <li>Total Cluster : 200 cores and 1TB RAM</li> </ul> <ul> <li>The master is controlled by Resource Manager and the workers are controlled by Node Manager.</li> </ul>"},{"location":"Spark_YT/#what-happens-when-user-submits-code","title":"What happens when user submits code?","text":"<ul> <li>The user submits some Spark code for execution to the Resource Manager. It needs 20 GB RAM, 25 GB executor, 5 total executors and 5 CPU cores.</li> <li>So the manager goes to W5 and asks to create 20GB container as the driver node.</li> </ul>"},{"location":"Spark_YT/#what-happens-inside-the-container","title":"What happens inside the container?","text":""},{"location":"Spark_YT/#driver-allocation","title":"Driver Allocation","text":"<p>Now this 20 GB driver is called Application Master </p> <p>There are two main() functions inside the master, one is for PySpark and other is for JVM like Java,Scala etc...</p> <p>The JVM main() is called Application Driver.</p> <p>The Spark Core has a Java Wrapper and the Java Wrapper has a Python Wrapper.</p> <p>When we write code in PySpark it gets converted to Java Wrapper.</p> <p>The PySpark driver is not a requirement but the Java Wrapper is required to run any code.</p>"},{"location":"Spark_YT/#worker-allocation","title":"Worker Allocation","text":"<ul> <li>Now the Application master asks for the executors to be assigned and the resource manager allocates.</li> </ul>"},{"location":"Spark_YT/#executor-container","title":"Executor Container","text":"<p>Each executor has 5 core CPU and 25GB RAM.</p> <p>THe above is when we have pure Java code and dont use Python UDF.</p> <p>But what if we use Python UDF functions?</p> <p> We need a Python worker inside the executor to be able to run the code.</p>"},{"location":"Spark_YT/#lecture-7-schema-in-spark","title":"Lecture 7 : Schema in Spark","text":""},{"location":"Spark_YT/#structtype-and-structfield","title":"StructType and StructField","text":"<p>Example:</p> <p></p> <p></p> <p>How to skip the header row?</p> <pre><code>df = spark.read.option(\"skipRows\", 2).csv(\"file.csv\")\n</code></pre>"},{"location":"Spark_YT/#lecture-8-handling-corrupter-records-in-spark","title":"Lecture 8 : Handling Corrupter Records in Spark","text":""},{"location":"Spark_YT/#how-many-records-in-each-mode","title":"How many records in each mode?","text":""},{"location":"Spark_YT/#permissive-mode","title":"Permissive Mode","text":""},{"location":"Spark_YT/#dropmalformed","title":"DropMalformed","text":""},{"location":"Spark_YT/#how-to-print-corrupted-records","title":"How to Print Corrupted Records","text":"<p>Output </p>"},{"location":"Spark_YT/#how-to-store-corrupted-records","title":"How to Store Corrupted Records","text":"<p>The corrupted records are in json format </p>"},{"location":"Spark_YT/#lecture-9-transformations-and-actions-in-spark","title":"Lecture 9 : Transformations and Actions in Spark","text":""},{"location":"Spark_YT/#types-of-transformations","title":"Types of Transformations","text":"<ul> <li>Narrow Transformation</li> <li>Wide Transformation</li> </ul> <p>Example: </p> <p>Suppose data is of 200MB. 200MB / 128MB = 2 partitions</p> <p></p> <p>Let's say both partitions go to separate executors.</p> <p>Q1 : Filtering Records  There is no data movement here.</p> <p>Q2: Find Total Income of each employee </p> <p>One id = 2 record is in one partition and the other is in the second partition so we need to do wide transformation </p> <p>Data needs to be shuffled and records with same id must be moved to same partition.</p> <ul> <li>filter,select,union etc are narrow transformations</li> <li>join,groupby,distinct</li> </ul>"},{"location":"Spark_YT/#lecture-10-dag-and-lazy-evaluation-in-spark","title":"Lecture 10 : DAG and Lazy Evaluation in Spark","text":"<ul> <li>For every action there is a new job, here there are three actions : read,inferSchema,sum and show</li> <li>When used with groupBy().sum(): It is considered an action because it triggers computation to aggregate data across partitions and produce a result. This operation forces Spark to execute the transformations leading up to it, effectively creating a job.</li> <li> <p>When used as a column expression df.select(sum(\"value\")): It acts more like a transformation in Spark's context, especially if part of a larger query or pipeline that does not immediately trigger execution. In this case, it only defines the operation and does not create a job until an action (like show() or collect()) is called.</p> </li> <li> <p>Job for reading file  Whole Stage Codegen - generate Java ByteCode</p> </li> <li> <p>Inferschema </p> </li> <li> <p>GroupBy and Count As explained above this is an action.</p> </li> <li> <p>Show Final action to display df</p> </li> </ul> <p> After we read the csv and inferSchema there are no jobs created since filter and repartition both are transformations not actions.</p> <p>When there are two filters on same dataset</p> <p></p> <p>This is the job </p>"},{"location":"Spark_YT/#optimizations-on-the-filter","title":"Optimizations on the Filter","text":"<p>Both the filters are on the same task  The optimizations can be applied because Spark is lazily evaluated.</p>"},{"location":"Spark_YT/#lecture-11-working-with-json-data-in-spark","title":"Lecture 11: Working with JSON Data in Spark","text":"<p>Two types of JSON notation:</p> <ul> <li> <p>Line Delimited JSON </p> </li> <li> <p>Multi Line JSON </p> </li> </ul> <pre><code>[\n{\n  \"name\": \"Manish\",\n  \"age\": 20,\n  \"salary\": 20000\n},\n{\n  \"name\": \"Nikita\",\n  \"age\": 25,\n  \"salary\": 21000\n},\n{\n  \"name\": \"Pritam\",\n  \"age\": 16,\n  \"salary\": 22000\n},\n{\n  \"name\": \"Prantosh\",\n  \"age\": 35,\n  \"salary\": 25000\n},\n{\n  \"name\": \"Vikash\",\n  \"age\": 67,\n  \"salary\": 40000\n}\n]\n</code></pre> <p>Line Delimited JSON is more efficient in terms of performance because the compiler knows that each line has one JSON record whereas in multiline json the compiler needs to keept track of where the record ends and the next one starts.</p>"},{"location":"Spark_YT/#different-number-of-keys-in-each-line","title":"Different number of keys in each line","text":"<p>Here what happens is that the line with the extra key has the value while for the rest its null </p>"},{"location":"Spark_YT/#multiline-incorrect-json","title":"Multiline Incorrect JSON","text":"<p>We dont pass a list here rather its just dictionaries <pre><code>{\n  \"name\": \"Manish\",\n  \"age\": 20,\n  \"salary\": 20000\n},\n{\n  \"name\": \"Nikita\",\n  \"age\": 25,\n  \"salary\": 21000\n},\n{\n  \"name\": \"Pritam\",\n  \"age\": 16,\n  \"salary\": 22000\n},\n{\n  \"name\": \"Prantosh\",\n  \"age\": 35,\n  \"salary\": 25000\n},\n{\n  \"name\": \"Vikash\",\n  \"age\": 67,\n  \"salary\": 40000\n}\n</code></pre> When we process the json it just reads the first dictionary as a record and the rest is not processed.</p> <p></p>"},{"location":"Spark_YT/#corrupted-records","title":"Corrupted Records","text":"<p>We dont need to define <code>_corrupted_record</code> in the schema, it will add the column on its ownn</p> <p></p>"},{"location":"Spark_YT/#lecture-12-spark-sql-engine","title":"Lecture 12: Spark SQL Engine","text":""},{"location":"Spark_YT/#how-is-spark-code-compiled","title":"How is Spark Code compiled?","text":"<ul> <li>The catalyst optimizer creates a plan and creates RDD lineage</li> </ul>"},{"location":"Spark_YT/#phases-in-catalyst-optimizer","title":"Phases in Catalyst Optimizer","text":""},{"location":"Spark_YT/#workflow-diagram","title":"Workflow Diagram","text":"<ul> <li>Unresolved Logical Plan : Bunch of crude steps to execute the SQL code</li> <li>Catalog : The table, files and database metadata information si stored in the catalog. Suppose we call read.csv on file that doesnt exist. The procedure that gives / throws the error is assisted via the catalog. In Analysis phase, we go through these steps. If some file/table is not found then we get Analysis Exception This error occurs when the Logical plan provided is not able to be resolved.</li> <li>Reoslved Logical Plan : This is the phase when we finished analysing the catalog objects.</li> <li>Logical Optimization: There are many examples. Suppose we need just two columns in select output, the spark engine does not fetch all the columns rather jsut fetches the two columns from memory that we need. Another example is when we use multiple filters on the same column in different lines of code. When we execute this code, we see that all of it is executed with or statements in one single line of code.</li> <li>Physical Plan: This involves taking decision like the type of join to use: Broadcast Join is one example. From the logical plan, we can build multiple physical plans. Thebest Physical Plan is a set of RDDs to be run on different executors on the cluster.</li> </ul>"},{"location":"Spark_YT/#lecture-13-resilient-distributed-dataset","title":"Lecture 13: Resilient Distributed Dataset","text":""},{"location":"Spark_YT/#data-storage-of-list","title":"Data Storage of List","text":""},{"location":"Spark_YT/#data-storage-in-rdd","title":"Data Storage in RDD","text":"<p>Suppose we have 500MB of data and 128MB partition, so we will have 4 partitions.</p> <p>The data is scattered on various executors. </p> <p>Its not in single contiguous location like elements of a list. The data structure used ot process this data is called RDD </p> <p></p> <p>Why is RDD recoverable?</p> <ul> <li> <p>RDD is immutable. If we apply multiple filters each dataset after filtering is a different dataset </p> </li> <li> <p>In below case if rdd2 fails then we can restore rdd1 because of the lineage. </p> </li> </ul>"},{"location":"Spark_YT/#disadvantage-of-rdd","title":"Disadvantage of RDD","text":"<ul> <li>No optimization done by Spark on RDD. The dev must specify explicitly on how to optimize RDD.</li> </ul>"},{"location":"Spark_YT/#advantage","title":"Advantage","text":"<ul> <li>Works well with unstructured data where there are no columns and rows / key-value pairs</li> <li>RDD is type safe, we get error on compile time rather than runtime which happens with Dataframe API.</li> </ul>"},{"location":"Spark_YT/#avoiding-rdds","title":"Avoiding RDDs","text":"<ul> <li>RDD : How to do? Dataframe API: Just specify what to do?</li> </ul> <p> You can see in above case that we have a join and filter but we are specifically saying that first join then filter so it triggers a shuffle first and then filter which is not beneficial.</p>"},{"location":"Spark_YT/#lecture-14-parquet-file-internals","title":"Lecture 14 : Parquet File Internals","text":"<p>There are two types of file formats:</p> <ul> <li>Columnar Based and Row Based</li> </ul>"},{"location":"Spark_YT/#physical-storage-of-data-on-disk","title":"Physical Storage of Data on Disk","text":""},{"location":"Spark_YT/#write-once-read-many","title":"Write Once Read Many","text":"<p>The funda of big data is write once read many.</p> <ul> <li>We dont need all the columns for analytics of big data, so columnar storage is the best.</li> <li>If we store in row based format then we need to jump many memory racks to be able to get the data we need.</li> </ul> <p></p> <ul> <li>OLTP generally use row base4d file format.</li> <li>I/O should be reduced so OLAP uses columnar format.</li> </ul>"},{"location":"Spark_YT/#why-columnar-format-may-not-be-the-best","title":"Why Columnar format may not be the best?","text":"<p>In above case we can get col1 and col2 easily but for col10 we still need to scan the entire file.</p> <p>To tackle this:</p> <p>Let's say we have 100 million total rows.</p> <p>We can store 100,000 records at a time, continuously in one row, then the next 100,000 records in next row and so on in hybrid format.</p> <p></p>"},{"location":"Spark_YT/#logical-partitioning-in-parquet","title":"Logical Partitioning in Parquet","text":"<p>Let's day we have 500mb data, each row group by default has 128 mb data, so we will have 4 row groups. Each row group will have some metadata attached to it.</p> <p>In our example let's say one row group has 100000 records. The column is futher stored as a page.</p>"},{"location":"Spark_YT/#runlength-encoding-and-bitpacking","title":"Runlength Encoding and Bitpacking","text":"<p>Suppose we have 10 lakh records but there can be say 4 countries.</p> <p>So parquet actually creates a dictionary of key value pair with key as int starting from 0 to 3 and then in the dictionary encoded data, we can see the keys being used insted of country name.</p>"},{"location":"Spark_YT/#demo","title":"Demo","text":"<p><code>parquet-tools inspect &lt;filename&gt;</code></p> <p> </p> <p>Gives some file and brief column level metadata.</p> <p><code>parquet_file.metadata.row_group(0).column_group(0)</code> </p> <p>Compression is GZIP </p> <p>Encoding is explained on top.</p>"},{"location":"Spark_YT/#bitpacking-advantage","title":"Bitpacking Advantage","text":"<ul> <li>Bitpacking helps in compressing the bits so in above case we just have 4 unique values and hence we need just 2 bytes.</li> <li>Query in seconds for running select on csv,parquet etc..</li> </ul>"},{"location":"Spark_YT/#summary","title":"Summary","text":"<ul> <li> <p>Here the actual data is stored in the pages and it has metadata like min,max and count.</p> </li> <li> <p>Let's say we need to find out people less than 18 years age</p> </li> </ul> <p></p> <p>Here when we divide data into row groups, we dont need to do any IO read operation on Row group 2, it saves lot of time and optimize performance.</p> <p>The above concept is called Predicate Pushdown.</p>"},{"location":"Spark_YT/#projection-pruning","title":"Projection Pruning","text":"<p>Projection Pruning means we dont read IO from columns that are not part of the select query or that arent required for any join.</p>"},{"location":"Spark_YT/#lecture-15-how-to-write-data-on-the-disk","title":"Lecture 15 : How to write data on the disk?","text":""},{"location":"Spark_YT/#modes-to-write-data","title":"Modes to write data","text":"<p>Create three files </p> <pre><code>  write_df = read_df.repartition(3).write.format(\"csv\")\\\n    .option(\"header\", \"True\")\\\n    .mode(\"overwrite\")\\  # Using .mode() instead of .option() for overwrite mode\n    .option(\"path\", \"/FileStore/tables/Write_Data/\")\\\n    .save()\n</code></pre>"},{"location":"Spark_YT/#lecture-16-partitioning-and-bucketing","title":"Lecture 16: Partitioning and Bucketing","text":"<p>In above data we cannot partition by any column because there is no similarity but we can bucket the data.</p> <p></p> <p> In above data we can partition by the country, but again we might have more data in India partition and less data in Japan.</p>"},{"location":"Spark_YT/#example-of-partitioning","title":"Example of Partitioning","text":"<p>The advantage is that the entire data is not scanned and only few partitions are scanned based on the filters.</p> <p>Partitioning by Id</p> <p></p> <p>Here we can see that we have created partitions by ID and since ID is low cardinality column partitioning is not efficient and we need to use bucketing.</p>"},{"location":"Spark_YT/#partitioning-by-address-and-gender","title":"Partitioning by Address and Gender","text":""},{"location":"Spark_YT/#bucketing-by-id","title":"Bucketing by Id","text":"<p>Dividing into 3 buckets </p>"},{"location":"Spark_YT/#tasks-vs-buckets","title":"Tasks vs Buckets","text":"<ul> <li>If we have 200 partitions we will have 200 tasks and each task will create 5 buckets each, to tackle this we first repartition into 5 partitions and then bucketBy 5.</li> </ul>"},{"location":"Spark_YT/#how-does-bucketing-help-with-joins","title":"How does bucketing help with joins?","text":"<ul> <li>Here we can see that since we have same column bucket on both tables the ids can be easily mapped and there is no shuffling.</li> </ul>"},{"location":"Spark_YT/#bucket-pruning","title":"Bucket Pruning","text":"<p> Suppose we have 1M Aadhar Ids and we divide into 100,000 each bucket so when we divide the above aadhar id by 100000 then we get the exact bucket where the number lies in.</p>"},{"location":"Spark_YT/#lecture-17-spark-session-vs-spark-context","title":"Lecture 17 : Spark Session vs Spark Context","text":"<ul> <li>Spark Session is entry point to the Spark cluster where we provide the parameters to create and operate our cluster.</li> <li>Spark session will have different context like one for SQL, PySpark etc...</li> </ul>"},{"location":"Spark_YT/#lecture-18-job-stage-and-tasks","title":"Lecture 18: Job, Stage and Tasks","text":"<ul> <li>One Application is created.</li> <li>One job is created per action.</li> <li>One stage is defined for every transformation like filter.</li> <li>Task is the actually activity on the data that's happening.</li> </ul>"},{"location":"Spark_YT/#example-of-jobaction-and-task","title":"Example of Job,Action and Task","text":""},{"location":"Spark_YT/#complete-flow-diagram","title":"Complete flow diagram","text":"<p>Every job has minimum one stage and one task.</p> <p> Repartition to filter is one job because we dont hit an action in between.</p> <p>Every wide dependency transformation has its own stage. All narrow dependency transformations come in one stage as a DAG.</p> <p></p>"},{"location":"Spark_YT/#how-do-tasks-get-created-read-and-write-exchange","title":"How do tasks get created? [Read and Write Exchange]","text":"<ul> <li>The repartition stage actually is a wide dependency transformation and creates two partitions from one, its a Write exchange of data.</li> <li>Now the filter and select stage reads this repartitioned data(Read exchange) and filter creates two tasks because we have two partitions.</li> <li>Next we need to find out how many folks earn &gt; 90000 and age &gt; 25 so we need to do a groupby that's a wide dependency transformation and it creates another stage. By default there are 200 partitions created.</li> <li>So some partitions may have data and some wont.</li> </ul>"},{"location":"Spark_YT/#lecture-17-dataframe-transformations-in-spark-part-1","title":"Lecture 17: Dataframe Transformations in Spark Part 1","text":"<p> Data gets stored in Row() format in the form of bytes</p> <p></p> <p>Columns are expressions. Expressions are set of transformations on more than one value in a record.</p>"},{"location":"Spark_YT/#ways-to-select-values-columns","title":"Ways to select values / columns","text":"<p>Column Manipulations</p> <p></p> <p>Other methods </p> <p>selectExpr </p> <p>Aliasing Columns </p>"},{"location":"Spark_YT/#lecutre-18-dataframe-transformations-in-spark-part-ii","title":"Lecutre 18 : Dataframe Transformations in Spark Part II","text":""},{"location":"Spark_YT/#filter-where-no-difference","title":"<code>filter()</code> / <code>where()</code> no difference","text":""},{"location":"Spark_YT/#multiple-filter-conditions","title":"Multiple filter conditions","text":""},{"location":"Spark_YT/#literals-in-spark","title":"Literals in spark","text":"<p>Used to pass same value in all the columns </p>"},{"location":"Spark_YT/#adding-columns","title":"Adding Columns","text":"<p>If the column already exists then it gets overwritten. </p>"},{"location":"Spark_YT/#renaming-columns","title":"Renaming Columns","text":""},{"location":"Spark_YT/#lecture-19-union-vs-unionall","title":"Lecture 19: union vs unionAll()","text":"<p>We can see that here we have a duplicate id </p> <p>In PySpark union and unionAll behaves in the same way, both retain duplicates </p> <p>But in Spark SQL when we do union it drops the duplicate records </p> <p></p>"},{"location":"Spark_YT/#selecting-data-and-unioning-the-same-table","title":"Selecting data and unioning the same table","text":""},{"location":"Spark_YT/#what-happens-when-we-change-the-order-of-the-columns","title":"What happens when we change the order of the columns?","text":"<p><code>wrong_manager_df</code> actually has the wrong order of columns but still we get the union output but in a wrong column values. </p> <p>If we give different number of columns an exception is thrown. </p> <p>If we use unionByName then the column names on both dfs must be the same. </p>"},{"location":"Spark_YT/#lecture-19-repartitioning-and-coalesce","title":"Lecture 19: Repartitioning and Coalesce","text":"<p>Suppose we have 5 partitions and one of them is skewed a lot 100MB, let's say this is the best selling product records. This partition takes lot of time to compute. So the other executors have to wait until this executor finishes processing. </p>"},{"location":"Spark_YT/#repartitioning-vs-coalesce","title":"Repartitioning vs Coalesce","text":""},{"location":"Spark_YT/#repartitioning","title":"Repartitioning","text":"<p>Suppose we have the above partitions and total data is 100mb. let's say we do repartition(5) so we will have 5 partitions now for the data with 40mb per partition.</p>"},{"location":"Spark_YT/#coalesce","title":"Coalesce","text":"<p>In case of coalesce there is no equal splitting of partition memory, rather the already existing partitions get merged together. </p> <p>There is no shuffling in coalesce but in repartitioning there is shuffling of data.</p>"},{"location":"Spark_YT/#pros-and-cons-in-repartitioning","title":"Pros and Cons in repartitioning","text":"<ul> <li>There is evenly distributed data.</li> <li>Con is that IO operations are more, its expensive.</li> <li>Con of coalesce is that the data is unevenly distributed.</li> </ul> <p>Repartitioning can increase or decrease the partitions but coalescing can only decrease the partitions.</p>"},{"location":"Spark_YT/#how-to-get-number-of-partitions","title":"How to get number of partitions?","text":"<p><code>flight_df.rdd.getNumPartitions()</code> gets the initial number of partitions and then we can repartition <code>flight_df.repartition(4)</code>. Data is evenly distributed.</p> <p></p> <p>Repartitioning based on columns</p> <p></p> <p>Since we asked for 300 partitions and we have 255 records some partitions will have null record. </p>"},{"location":"Spark_YT/#coalescing","title":"Coalescing","text":"<p> Suppose we have 8 partitions and we coalesce into 3 partitions. Coalesce has only one arg.</p> <p>Uneven distribution of data in partitions. </p>"},{"location":"Spark_YT/#lecture-20-case-when-if-else-in-spark","title":"Lecture 20 : Case when / if else in Spark","text":""},{"location":"Spark_YT/#apply-logic-on-one-column-then-process-if-else-logic","title":"Apply logic on one column then process if else logic","text":""},{"location":"Spark_YT/#spark-sql-logic","title":"Spark SQL Logic","text":""},{"location":"Spark_YT/#lecture-21-unique-and-sorted-records","title":"Lecture 21 : Unique and Sorted Records","text":""},{"location":"Spark_YT/#distinct","title":"distinct()","text":"<p>Original Data </p> <p>Distinct Data </p> <p>Distinct Based on certain columns </p> <p>\u26a0\ufe0f Distinct takes no arguments we need to select the columns first and then apply distinct.</p>"},{"location":"Spark_YT/#dropping-duplicate-records","title":"Dropping duplicate records","text":"<p>Point to note is that the dataframe <code>manager_df</code> has no changes, it just shows the records after dups have been dropped. </p>"},{"location":"Spark_YT/#sort","title":"sort()","text":"<p>Descending order </p> <p>Sorting on multiple columns</p> <p>Here first the salary is srranged in desc order then we arrange the name in asc order from those records with same salary. </p>"},{"location":"Spark_YT/#lecture-22-aggregate-functions","title":"Lecture 22 : Aggregate functions","text":""},{"location":"Spark_YT/#count-as-both-action-and-transformation","title":"Count as both Action and Transformation","text":"<p>\u26a0\ufe0f When we are doing count on a single column and there is a null in it, its not considered in the count. But for all columns we have nulls in the count. </p> <p>Job created in first case and its not created in second case below. </p>"},{"location":"Spark_YT/#lecture-23-group-by-in-spark","title":"Lecture 23: Group By In Spark","text":"<p>Sample Data</p> <p></p>"},{"location":"Spark_YT/#questions","title":"Questions","text":"<p>Salary per department using groupBy() </p>"},{"location":"Spark_YT/#where-do-we-use-window-functions","title":"Where do we use window functions?","text":"<p>Suppose we need to find out the percentage of total salary from a particular dept that the person is earning. we can use window function to specify the total salary per department in the particular record itself like I've shown below. </p> <p>This way we dont need to perform a join.</p> <p></p>"},{"location":"Spark_YT/#grouping-by-two-columns","title":"Grouping by two columns","text":""},{"location":"Spark_YT/#lecture-24-joins-in-spark-part-1","title":"Lecture 24 : Joins in Spark part 1","text":"<p>Which customers joined platform but never brought anything?</p> <p></p> <p>Whenever we need information from another table, we use joins and there should be some common column.</p> <p>Join is a costly wide dependency operation.</p>"},{"location":"Spark_YT/#how-do-joins-work","title":"How do joins work?","text":"<p>How many records do we get after inner joining the below two tables. </p> <p>We get a total of 9 records. </p> <p>Sometimes data gets duplicated when we do joins, so we should use distinct() but remember distinct is wide dependency transform.</p>"},{"location":"Spark_YT/#lecture-25-types-of-join-in-spark","title":"Lecture 25 : Types of Join in Spark","text":""},{"location":"Spark_YT/#inner-join","title":"Inner Join","text":""},{"location":"Spark_YT/#left-join","title":"Left Join","text":"<p> All records in left table + those that join with right table, whereever we dont get match on right table the columns become null.</p>"},{"location":"Spark_YT/#right-join","title":"Right Join","text":""},{"location":"Spark_YT/#full-outer-join","title":"Full Outer Join","text":""},{"location":"Spark_YT/#left-semi-join","title":"Left Semi Join","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"LeftSemiJoinExample\").getOrCreate()\n\n# Left DataFrame: Orders\norders = spark.createDataFrame([\n    (1, \"iPhone\"),\n    (2, \"Pixel\"),\n    (3, \"OnePlus\"),\n    (4, \"Nokia\")\n], [\"customer_id\", \"product\"])\n\n# Right DataFrame: Valid Customers\nvalid_customers = spark.createDataFrame([\n    (1,), (3,)\n], [\"customer_id\"])\n\n# Perform left semi join\nfiltered_orders = orders.join(valid_customers, on=\"customer_id\", how=\"left_semi\")\nfiltered_orders.show()\n</code></pre> <p>Output</p> <pre><code>+-----------+--------+\n|customer_id|product |\n+-----------+--------+\n|          1|iPhone  |\n|          3|OnePlus |\n+-----------+--------+\n</code></pre>"},{"location":"Spark_YT/#left-anti-join","title":"Left Anti Join","text":"<p>Find out all customers who have never purchased any product.</p>"},{"location":"Spark_YT/#cross-join","title":"Cross Join","text":"<p>Never use cross join! </p> <p></p>"},{"location":"Spark_YT/#lecture-26-join-strategies-in-spark","title":"Lecture 26 : Join Strategies in Spark","text":"<p>Joins are expensive due to shuffling.</p> <p>4 partitions are there in each dataframe.</p> <p></p> <p>Executors in the cluster</p> <p></p> <p>Now we need to join employee and salary df to get the output but they are on different executors, so we need to do data shuffling.</p> <p>Each executor has 200 partitions. Goal is to get all same keys in one executor. </p> <p></p> <p></p> <ul> <li>Since we want to get id for 1 we divide 1/200 = 1 and then send all the data to that executor 1.</li> </ul> <p></p> <p>Suppose we want to map the salary for id = 7 so the data from the employee df with id = 7 and also salary df with id=7 will come into the executor 7.</p> <p>Similarly id = 201 will go into 201/200 = executor no 1.</p>"},{"location":"Spark_YT/#types-of-join-strategies","title":"Types of Join Strategies","text":"<p>Joins generally result in shuffling</p> <p>There are two dataframes df1 and df2 each with 4 partitions.</p> <p></p> <p>We have two executors.</p> <p>In join goal is to join with same keys.</p> <p></p> <p>We can see that red P1 has corresponding id for salary in the other executor.</p> <p></p> <p>We need to get same keys fetched from other executors.</p> <p>When a dataframe is sent to executors by default 200 partitions are created per dataframe.</p> <p></p> <p>Now let's say we want to find salary for id = 1 we can divide 1/200 on blue = 1 and 1/200 on red = 1, so both data will come into executor 1 in the partition 1.</p> <p></p> <p>Similarly for id = 7 also we will send the data on blue and red P7</p> <p>But if id = 201 then 201/200 = 1 so this id will come into P1 only.</p> <p>If we have id = 102 then 102/200 = 102 partition on 2nd executor.</p> <p></p> <p>The executors can be on different worker nodes also, we need to then move data across from one worker node to other.</p>"},{"location":"Spark_YT/#strategies","title":"Strategies","text":"<p>Broadcast nested loop join is costly because we dont do a straight join, rather its based on &lt; an &gt; conditions, its O(n^2)</p>"},{"location":"Spark_YT/#shuffle-sort-merge-join","title":"Shuffle Sort Merge Join","text":"<p>TC : O(nlogn)</p>"},{"location":"Spark_YT/#shuffle-hash-join","title":"Shuffle Hash Join","text":"<p>The smaller table gets a hash table created with hashed keys in memory.</p> <p>Now from df1 we checked which keys match with O(1) lookup using the hash table.</p> <p></p>"},{"location":"Spark_YT/#broadcast-join","title":"Broadcast Join","text":"<p>The tables that are less than 100mb can be broadcast.</p> <p>Scenario : Suppose one table is 1GB size so we will have 1000MB / 128MB = 8 partitions and there is another table of size 5mb.</p> <p>So if we dont broadcast, then the df with 100gb should be shuffled around with 5mb data across executors for joining. Instead of that we will just send the small df in all the executors so that there is no shuffling.</p> <p></p> <p>The amount of data that can be broadcast depends on the memory of executor and driver. Make sure that there is no case where driver memory is 2GB and we are trying to broadcast 1GB data.</p>"},{"location":"Spark_YT/#demo_1","title":"Demo","text":"<p>There are total 200 partitions when we join </p> <p></p> <p>Normal Sort Merge Join Execution Plan</p> <pre><code>== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   Project [sale_id#10484L, sale_date#10485, amount#10486L, country_name#10514]\n   +- SortMergeJoin [country_id#10487L], [country_id#10513L], Inner\n      :- ColumnarToRow\n      :  +- PhotonResultStage\n      :     +- PhotonSort [country_id#10487L ASC NULLS FIRST]\n      :        +- PhotonShuffleExchangeSource\n      :           +- PhotonShuffleMapStage\n      :              +- PhotonShuffleExchangeSink hashpartitioning(country_id#10487L, 1024)\n      :                 +- PhotonFilter isnotnull(country_id#10487L)\n      :                    +- PhotonRowToColumnar\n      :                       +- LocalTableScan [sale_id#10484L, sale_date#10485, amount#10486L, country_id#10487L]\n      +- ColumnarToRow\n         +- PhotonResultStage\n            +- PhotonSort [country_id#10513L ASC NULLS FIRST]\n               +- PhotonShuffleExchangeSource\n                  +- PhotonShuffleMapStage\n                     +- PhotonShuffleExchangeSink hashpartitioning(country_id#10513L, 1024)\n                        +- PhotonFilter isnotnull(country_id#10513L)\n                           +- PhotonRowToColumnar\n                              +- LocalTableScan [country_id#10513L, country_name#10514]\n\n== Photon Explanation ==\nPhoton does not fully support the query because:\n        Unsupported node: SortMergeJoin [country_id#10487L], [country_id#10513L], Inner.\n\nReference node:\n    SortMergeJoin [country_id#10487L], [country_id#10513L], Inner\n</code></pre> <p>Spark UI Diagram</p> <p></p> <p></p> <p>Broadcast Join Execution Plan</p> <pre><code>== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonProject [sale_id#10484L, sale_date#10485, amount#10486L, country_name#10514]\n         +- PhotonBroadcastHashJoin [country_id#10487L], [country_id#10513L], Inner, BuildRight, false, true\n            :- PhotonFilter isnotnull(country_id#10487L)\n            :  +- PhotonRowToColumnar\n            :     +- LocalTableScan [sale_id#10484L, sale_date#10485, amount#10486L, country_id#10487L]\n            +- PhotonShuffleExchangeSource\n               +- PhotonShuffleMapStage\n                  +- PhotonShuffleExchangeSink SinglePartition\n                     +- PhotonFilter isnotnull(country_id#10513L)\n                        +- PhotonRowToColumnar\n                           +- LocalTableScan [country_id#10513L, country_name#10514]\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n</code></pre> <p></p>"},{"location":"Spark_YT/#window-functions-in-spark","title":"Window functions in Spark","text":""},{"location":"Spark_YT/#rank-vs-dense-rank","title":"Rank vs Dense Rank","text":"<p>Dense rank does not leave any gaps between the ranks.</p> <p></p>"},{"location":"Spark_YT/#lead-and-lag","title":"Lead and Lag","text":""},{"location":"Spark_YT/#range-and-row-between","title":"Range and Row Between","text":"<p>Q1</p> <p></p> <p>Using first and last functions let's try to acheive this.</p> <p>Data:</p> <p></p> <p>This solution is wrong, ideally we should get 111000 in all rows of <code>latest_sales</code> column.</p> <p></p> <p>Let's look at explain plan.</p> <p>We can see that the window here is <code>unbounded preceeding and current row</code></p> <p></p> <p>What do these terms mean?</p> <p></p> <ul> <li>Unbounded preceeding : If i'm standing at a current row in a window I will return the result of any operation on the window from here to all the rows before me in the window.</li> <li>current_row : the row im standing at.</li> <li>Unbounded following : opposite of unbounded preceeding.</li> <li>rows_between(start_row,end_row) : basically the row we are currently at is 0, all rows before that are negative numbers and all rows after that is positive numbers.</li> </ul> <p></p> <p>If we dont give anything then it just goes from current row to either unbounded preceeding (first row) of window or unbounded following (last row) of window.</p> <p>Converting from string to unixtime when we have two fields date and time.</p> <p></p> <p><code>emp_df = emp_df.withColumn(\"timestamp\",from_unixtime(unix_timestamp(expr(\"CONCAT(date,' ',time)\"),\"dd-MM-yyyy HH:mm\")))</code></p> <p>The timestamp column is a string.</p> <p></p>"},{"location":"Spark_YT/#spark-memory-management","title":"Spark Memory Management","text":"<p>If we do <code>df.range(100000)</code> and then do <code>df.collect()</code> on 1Gb driver we get OOM error</p> <p></p> <p>Spark Architecture</p> <p></p> <p>Driver memory is of two types:</p> <ul> <li>spark.driver.memory</li> <li>spark.driver.memoryOverhead</li> </ul> <p></p> <p>With collect all records go into the driver. But with show just one partition gets sent to the heap space.</p> <p>\ud83c\udfaf Think of the Spark Driver Like a Worker</p> <p>Imagine the Spark driver is a person doing a big task at a desk.</p> <p>The desk = spark.driver.memory (main memory)</p> <p>The room around the desk = spark.driver.memoryOverhead (extra space to move, store tools, use side tables)</p> <p>\ud83e\udde0 Why Just the Desk Isn\u2019t Enough</p> <p>Let\u2019s say the driver (person) is:</p> <p>Writing on paper (standard Spark tasks)</p> <p>Using a laptop (Python/PySpark or native code)</p> <p>Holding tools and files (temporary data, buffers, network stuff)</p> <p>Only giving them a desk (spark.driver.memory) isn't enough:</p> <p>The laptop (native code, Python UDFs) might need space outside the desk</p> <p>The tools (Spark internals, shuffle, serialization) don\u2019t fit on the desk \u2014 they use off-heap memory</p> <p>If you don\u2019t give them enough room around the desk (memoryOverhead), they might trip over stuff and fail the task.</p> <p>\ud83e\uddea Real Spark Example When you run PySpark like this:</p> <pre><code>df.withColumn(\"double\", my_udf(df[\"col\"]))\n</code></pre> <p>That Python UDF runs outside the JVM. It needs extra native memory, not regular Java memory.</p> <p>Spark says:</p> <p>\u201cI\u2019ll use driver.memory for my JVM, but I need some memoryOverhead for the native stuff.\u201d</p> <p>\u2705 Summary (in 1 line)</p> <pre><code>spark.driver.memory is for Spark's own work (Java),\nspark.driver.memoryOverhead is for everything outside the JVM \u2014 like Python, shuffle, native code.\n</code></pre> <p>The memory overhead is <code>max(384mb,10% of driver memory)</code></p> <p></p> <p>Let's say there is <code>df1</code> and we want to join it with two small tables <code>df2</code> and <code>df3</code>.</p> <p>We send both df2 and df3 to the driver.</p> <p></p> <p>Let's say we now give 5 dayasets worth 250 mb and the total driver space is 1G.</p> <p>If rest 750mb is not enough for other processes then the driver will give OOM exception.</p> <p>\ud83d\udca5 So\u2026 How Can GC Cause Out of Memory (OOM)?</p> <p>You\u2019d think GC helps prevent OOMs \u2014 and it does! But in high-memory-pressure situations, it can actually cause or worsen them.</p> <p>\ud83d\udea8 Here\u2019s how it happens: 1. Too Many Objects / Too Much Data in Memory You load huge datasets or perform wide transformations (e.g., groupBy, join).</p> <p>Spark stores a lot of intermediate data in RAM (JVM heap).</p> <p>\ud83d\udc49 JVM tries to make space by running GC again and again.</p> <ol> <li>GC Takes Too Long If GC runs too often or too long (e.g., &gt; 30s), the JVM thinks something\u2019s wrong.</li> </ol> <p>You get:</p> <pre><code>java.lang.OutOfMemoryError: GC overhead limit exceeded\n</code></pre> <p>This means:</p> <p>\u201cGC is using 98% of the CPU but only recovering 2% of memory \u2014 I give up.\u201d</p> <ol> <li>GC Can\u2019t Free Anything Some objects (like cached RDDs or references from your code) stay in memory.</li> </ol> <p>GC runs but can't collect them because they're still \"referenced\".</p> <p>Eventually, JVM runs out of space and crashes with:</p> <pre><code>java.lang.OutOfMemoryError: Java heap space\n\u26a0\ufe0f Common Scenarios in Spark\nCause   Result\nLarge shuffles / joins  Too many objects in memory\nCaching huge RDDs   Heap filled, GC can't recover\nImproper partitions Few tasks \u2192 huge memory per task\nMemory leaks (bad code) Uncollectable references\n</code></pre> <p>Example code</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.storagelevel import StorageLevel\nimport random\n\nspark = SparkSession.builder \\\n    .appName(\"OOM-GC-Demo\") \\\n    .config(\"spark.driver.memory\", \"1g\") \\\n    .getOrCreate()\n\n# Create a large DataFrame with few partitions (causes memory pressure)\ndata = [(i % 10, random.randint(1, 1000)) for i in range(10_000_000)]  # 10 million rows\ndf = spark.createDataFrame(data, [\"group_id\", \"value\"])\n\n# Force a wide transformation + cache\nresult = df.groupBy(\"group_id\").count().persist(StorageLevel.MEMORY_ONLY)\n\n# Trigger action\nresult.count()\n</code></pre> <p>\u2705 How to Fix</p> <p>Increase spark.executor.memory or spark.driver.memory</p> <p>Use persist(StorageLevel.DISK_ONLY) if RAM is tight</p> <p>Avoid huge wide transformations without enough partitions</p> <p>Tune GC (G1GC is often better for large heaps)</p>"},{"location":"Spark_YT/#executor-memory-oom","title":"Executor Memory OOM","text":"<p>10 GB per executor and 4 cores</p> <p>Expanding one executor</p> <p></p> <p></p> <p>Exceeding either 10GB or 1GB leads to OOM</p> <p></p>"},{"location":"Spark_YT/#how-is-10gb-divided","title":"How is 10GB divided?","text":""},{"location":"Spark_YT/#what-does-each-part-of-the-user-memory-do","title":"What does each part of the user memory do?","text":"<ol> <li>Reserved Memory</li> </ol> <p>Minimum 450mb must be our memory of executor.</p> <p></p> <ol> <li>User Memory </li> </ol> <p></p> <ol> <li>Storage Memory Usage</li> </ol> <p></p> <ol> <li>Executor Memory Usage</li> </ol> <p></p>"},{"location":"Spark_YT/#what-does-each-part-of-the-spark-memory-do","title":"What does each part of the spark memory do?","text":"<p>\u2699\ufe0f Background: Memory in Spark Executors</p> <p>Each executor in Spark has a limited memory budget. This memory is split for:</p> <ul> <li> <p>Execution Memory: used for joins, aggregations, shuffles</p> </li> <li> <p>Storage Memory: used for caching RDDs or DataFrames</p> </li> <li> <p>User Memory: everything else (broadcast vars, UDFs, JVM overhead)</p> </li> </ul> <p>\ud83d\udd04 1. Static Memory Manager (Old)</p> <p>This was Spark's memory model before Spark 1.6.</p> <p>\ud83d\udd27 How It Works:</p> <ul> <li>Fixed memory boundaries set in config.</li> <li>You manually allocate how much memory goes to:</li> <li>Storage (RDD cache)</li> <li>Execution (shuffles, joins)</li> <li>If storage fills up \u2192 cached blocks are evicted.</li> <li>No sharing between execution and storage.</li> </ul> <p>Example fractions</p> <pre><code>spark.storage.memoryFraction = 0.6\nspark.shuffle.memoryFraction = 0.2\n</code></pre> <p>\ud83d\udd04 2. Unified Memory Manager (Modern - Default)</p> <p>Introduced in Spark 1.6+ and is default since Spark 2.0.</p> <p>\ud83d\udd27 How It Works:</p> <p>Combines execution + storage into a single unified memory pool.</p> <p>Dynamic memory sharing: if execution needs more, storage can give up memory \u2014 and vice versa.</p> <p>Much more flexible and efficient.</p> <p>\u2705 Benefits:</p> <ul> <li>Less tuning needed</li> <li>Avoids wasted memory in one region while another needs more</li> <li>Better stability under pressure</li> </ul> <p>In bwlo case execution memory is empty so storage mmemory uses more of execution memory for caching</p> <p></p> <p>Now executor does some work in blue boxes</p> <p></p> <p>Now entire memory is full, so we need to evict some data that has been cached. This happens in LRU fashion.</p> <p></p> <p>Now let's say executor has entire memory used 2.9 something gb... but it needs more memory.</p> <p></p> <p>If the storage pool memory is free it can utilize that.</p> <p></p> <p>If the storage pool is also full, then we get OOM!!!</p>"},{"location":"Spark_YT/#when-can-we-neither-evict-the-data-nor-spill-to-disk","title":"When can we neither evict the data nor spill to disk?","text":"<p>Suppose we have two dataframes df1 and df2 and the key id = 1 is heavily skewed in both dataframes, and its 3GB</p> <p>Since we need to get all the data from df1 and df2 with id = 1 onto the same executor to perform the join, we have just 2.9GB but the data is 3gb so it gives OOM.</p> <p></p> <p></p> <p>We can handle 3-4 cores per executor beyond that we get memory executor error.</p> <p>\u2753 When can Spark neither evict nor spill data from executor memory?</p> <p>This happens when both eviction and spilling are not possible, and it leads to:</p> <p>\ud83d\udca5 OutOfMemoryError in executors.</p> <p>\u2705 These are the main scenarios:</p> <p>\ud83e\uddf1 1. Execution Memory Pressure with No Spill Support</p> <p>Execution memory is used for:</p> <ul> <li>Joins (SortMergeJoin, HashJoin)</li> <li>Aggregations (groupByKey, reduceByKey)</li> <li>Sorts</li> </ul> <p>Some operations (like hash-based aggregations) need a lot of memory, and not all are spillable.</p> <p>\ud83d\udd25 Example:</p> <p><pre><code>df.groupBy(\"user_id\").agg(collect_set(\"event\"))\n</code></pre> If collect_set() builds a huge in-memory structure (e.g., millions of unique events per user)</p> <p>And that structure can\u2019t be spilled to disk</p> <p>And execution memory is full</p> <p>\ud83d\udc49 Spark can\u2019t evict (no caching), and can\u2019t spill (not supported for this op) \u2192 \ud83d\udca3 OOM</p> <p>\ud83d\udd01 2. Execution Takes Priority, So Storage Can't Evict Enough</p> <p>In Unified Memory Manager, execution gets priority over storage.</p> <p>But sometimes, even after evicting all cache, execution still doesn\u2019t get enough memory.</p> <p>\ud83d\udd25 Example: - You cached a large DataFrame. - Then you do a massive join.</p> <p>Spark evicts all cached data, but still can't free enough memory.</p> <p>\ud83d\udc49 No more memory to give \u2192 \ud83d\udca5</p> <p>User Code holding References</p> <p>\ud83c\udf55 Imagine Spark is a Pizza Party Spark is throwing a pizza party. You and your friends (the executors) are each given a plate (memory) to hold some pizza slices (data).</p> <p>The rule is:</p> <p>\u201cEat your slice, then give your plate back so someone else can use it.\u201d</p> <p>\ud83d\ude2c But You Keep Holding Your Plate You finish your slice, but instead of giving the plate back, you say:</p> <p>\u201cHmm\u2026 I might want to lick the plate later,\u201d so you hold on to it.</p> <p>And you keep doing this with every plate \ud83c\udf7d\ufe0f.</p> <p>Now, you have 10 plates stacked up, all empty, but you're still holding them.</p> <p>\ud83c\udf55 But There\u2019s a Problem\u2026 Spark wants to serve more pizza (more data), but now there are no plates left. Even though you\u2019re not using yours, Spark can\u2019t take them back, because you\u2019re still holding on.</p> <p>\ud83d\udca5 Result? Spark gets frustrated and says:</p> <p>\u201cI\u2019m out of plates! I can\u2019t serve any more pizza!\u201d</p> <p>That\u2019s when Spark crashes with a memory error (OOM) \u2014 because it can\u2019t clean up the memory you're holding onto.</p> <p>\u2705 What Should You Do? Let go of the plates as soon as you're done eating (i.e., don\u2019t store data in variables or lists forever).</p> <p>That way, Spark can reuse memory and everyone gets more pizza. \ud83c\udf55</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"HoldingReferencesOOM\") \\\n    .config(\"spark.driver.memory\", \"1g\") \\\n    .getOrCreate()\n\n# Create a large DataFrame\ndf = spark.range(1_000_000)  # 1 million rows\n\n# \u274c BAD: Holding all rows in a Python list\nall_data = df.collect()  # Loads entire DataFrame into driver memory\n\n# Still holding reference to a big object\n# Spark can't clean this up because Python is holding it\n\n# Do more operations\ndf2 = df.selectExpr(\"id * 2 as double_id\")\ndf2.show()\n</code></pre> <p>Spark wants to free memory, but it can\u2019t, because your code is still holding a reference to the list <code>all_list</code> is still a reference and even though we may not use it later Java GC doesnt know that. its like we finish playing with a teddy bear but still hold onto it, the teacher thinks we are still playing with it, so they cant take it back.</p> <pre><code>df = spark.range(1_000_000)\n\n# \u2705 Process data without collecting everything into memory\ndf.filter(\"id % 2 == 0\").show(10)  # only shows first 10 rows\n</code></pre>"},{"location":"astronomer/","title":"Airflow with Astronomer CLI","text":""},{"location":"astronomer/#core-airflow-concepts","title":"Core Airflow Concepts","text":""},{"location":"astronomer/#core-airflow-for-data-pipelines","title":"Core Airflow for Data Pipelines","text":"<p>This is the notes for Airflow tool from coder2j channel on YouTube.</p> <p>A quick note, you can refer this link. It has most of the details from here but the installation steps are much more clear here!!</p>"},{"location":"astronomer/#installation","title":"Installation","text":""},{"location":"astronomer/#using-pip-package","title":"Using pip package","text":"<ol> <li> <p>Python 3.6 is the min version required.</p> </li> <li> <p>Create a Python Virtual Environment <code>python3 -m venv py_env</code></p> </li> <li> <p>Activate the Python Environment py_env <code>source py_env/bin/activate</code></p> </li> <li> <p>Install airflow using <code>pip install apache-airflow</code></p> </li> <li> <p>Now before creating a db, we have to set the path <code>export AIRFLOW_HOME = .</code></p> </li> <li> <p>Initialize the db with <code>airflow db init</code></p> </li> <li> <p>Start Airflow Web Server using <code>airflow webserver -p 8080</code></p> </li> <li> <p>Create a username and password <code>airflow users create --username admin --firstname ved --lastname baliga --role admin --email vedanthvbaliga@gmail.com</code> and set the password.</p> </li> <li> <p>Run <code>airflow scheduler</code> to start the scheduler.</p> </li> </ol>"},{"location":"astronomer/#using-docker-for-installation","title":"Using Docker for Installation","text":"<ol> <li> <p>For Windows, first setup and setup WSL2. Check out the video here.</p> </li> <li> <p>Now download Docker Desktop from the website</p> </li> <li> <p>Use this curl command to dowload Airflow yaml file via Docker Compose.[<code>curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.7.3/docker-compose.yaml'</code>]</p> </li> <li> <p>Change the core executor in yaml file to LocalExecutor. Remove the Celery flower and celery-worker.</p> </li> <li> <p>Initialize the Environment : <code>mkdir -p ./dags ./logs ./plugins ./config echo -e \"AIRFLOW_UID=$(id -u)\" &gt; .env</code></p> </li> <li> <p>Start the docker environment : <code>docker compose up airflow-init</code></p> </li> <li> <p>Run Airflow : <code>docker compose up</code></p> </li> </ol>"},{"location":"astronomer/#core-concepts","title":"Core Concepts","text":"<p>What is Airflow? </p> <p>What is a workflow? </p> <p>DAG, Task and Operator </p> <p>Here A is the downstream task of C and B is the upstream task of A. Task implements an operator and DAG is a collection of operators working together.</p> <p></p>"},{"location":"astronomer/#task-architecture","title":"Task Architecture","text":""},{"location":"astronomer/#complete-task-lifecycle","title":"Complete Task Lifecycle","text":""},{"location":"astronomer/#complete-airflow-architecture-design","title":"Complete Airflow Architecture Design","text":"<p>How does Data Engineer help in the process</p> <p></p>"},{"location":"astronomer/#creating-dags","title":"Creating DAGs","text":"<p>Downstream and Upstream Tasks with DAG </p>"},{"location":"astronomer/#airflow-xcoms-for-information-sharing","title":"Airflow XComs For Information Sharing","text":"<p>Airflow XCom allows us to push info from one task to other and another task can pull information from the XCon.</p> <p>Every Function's return value goes to XCom by default.</p> <p>Here is the code for one value push into XComs.</p> <p>Here is the code for pushing two or more values with keys into XComs.</p>"},{"location":"astronomer/#astronomer-course","title":"Astronomer Course","text":"<p>Airflow Astronomer is a data orchestration tool that allows us to create data pipelines using python and integrate a variety of operators like Databricks, Snowflake and dbt.</p> <p>Here are some notes I took while taking up a fantastic set of courses by Astronomer, a managed cloud service for Airflow.</p> <p>There are a bunch of different Modules but here is the learning path that I followed.</p>"},{"location":"astronomer/#module-1-intro-to-astro-cli","title":"Module 1 - Intro to Astro CLI","text":""},{"location":"astronomer/#setting-up-and-running-astro-cli","title":"Setting Up and Running Astro CLI?","text":"<ul> <li>Astro CLI uses docker to setup and run local dev environment.</li> <li>Two Commands to execute are:<ul> <li><code>dev init</code> to init the local dev envt</li> <li><code>dev start</code> to spin up airflow instance</li> </ul> </li> </ul> <p>There are four docker images Web Server,Scheduler,Database and Triggerer that are part of a Docker Container called Astronomer Runtime.</p>"},{"location":"astronomer/#pushing-the-dag-to-the-production-envt","title":"Pushing the DAG to the Production Envt","text":"<ul> <li> <p>A request is sent to the Airflow API saying that we need to update the airflow instance of a deployment, a docker image is created with our DAGs and pushed into the registry.</p> </li> <li> <p>Then finally the Aiirflow instance in the data plane is restarted with the new docker instances with DAGs</p> </li> </ul> <p></p>"},{"location":"astronomer/#installing-astro-cli","title":"Installing Astro CLI","text":"<p>Docs : Click Here</p> <p></p>"},{"location":"astronomer/#module-2-airflow-concepts","title":"Module 2 - Airflow Concepts","text":""},{"location":"astronomer/#creating-a-project","title":"Creating a Project","text":"<ul> <li> <p><code>mkdir astro &amp;&amp; cs astro</code></p> </li> <li> <p><code>astro dev init</code></p> </li> </ul> <p></p> <ul> <li><code>airflow_settings.yaml</code> allows us to recrete a project without creating variables and connections over and over again.</li> </ul>"},{"location":"astronomer/#running-airflow","title":"Running Airflow","text":"<ul> <li><code>astro dev start</code> spins up airflow and downloads the docker images for airflow components.</li> </ul>"},{"location":"astronomer/#add-additional-providers-in-airflow","title":"Add additional providers in Airflow","text":"<ul> <li> <p>On astro terminal type <code>astro dev run providers list</code> to check the list of all the providers.</p> </li> <li> <p>if anything is not installed, pip install it from the providers page in the UI and add it to the reqs file.</p> </li> </ul>"},{"location":"astronomer/#upgrading-astro-runtime","title":"Upgrading Astro Runtime","text":"<p>Go to Dockerfile and change the version then run <code>asto dev restart</code></p>"},{"location":"astronomer/#non-base-images-vs-base-image","title":"Non Base Images v/s Base Image","text":"<p>By default we have the non base image where the folders and dependencies are auto copied to the docker container but if we want to have full control of everything and add whatever folders we want, we have to use the base image.</p> <p>We can switch to the base image by changing the command in Dockerfile to <code>FROM quay.io/astronomer/astro-runtime:5.3.0-base</code></p>"},{"location":"astronomer/#persisting-variables-after-killing-airflow-image","title":"Persisting Variables after killing Airflow Image","text":"<p>Check how to do this here</p>"},{"location":"astronomer/#environment-variables","title":"Environment Variables","text":"<p>In the <code>.env</code> file we can specify the name of the environment as <code>AIRFLOW__WEBSERVER__INSTANCE__NAME = ProdEnv</code></p> <p>If we want to keep mutiple environments just add one more file <code>.dev</code> and add <code>ENVIRONMENT = dev</code> in that file. </p> <p>Now we can start the server with this env, <code>airflow dev start --env .dev</code></p> <p>This is only valid in our local environment.</p> <p>If we want too export the environment variables to the Astro instance, then we need to add <code>ENV AIFLOW__WEBSERVER__INSTANCE__NAME = ProdEnv</code> to the Dockerfile.</p>"},{"location":"astronomer/#checking-dags-for-errors-before-running-it","title":"Checking DAGs for Errors before Running it","text":"<p>If we dodnt want to wait for 5-6 min for the UI to throw up any import or other errors then we can use <code>astro dev parse</code> to get all the possible errors in  the command line itself.</p> <p>We can also use the pytest library for testing using <code>astro dev pytest</code></p> <p>Another way to run and compile dags in the cli is <code>astro run</code>. This Trigger a single DAG run in a local Airflow environment and  see task success or failure in your terminal. This command compiles your DAG and runs it in a single Airflow worker container based on your  Astro project configurations.</p> <p>More type of testing like backtesting dependencies during updates is here</p>"},{"location":"astronomer/#how-everything-works","title":"How Everything Works?","text":"<p>Step 1 :  </p> <p>Step 2 :  </p> <p>Scheduler processes the DAG and we may need to wait upto 5 min before getting the new DAG on the Airflow UI.</p> <p>Step 3:  The scheduler creates the DAGRun Object that has the states running.</p> <p>Step 4: </p> <p>The scheduler then creates the task instance which is instance of the task at a certain time and it has the state scheduled.</p> <p>Step 5: </p> <p>Now the Task Instance is queued and the scheduler sends the taskInstance object to the executor that executes it and the state of the task is complete.</p> <p></p> <p>Now either the task status is success or failed and it updates the state accordingly.</p> <p>Then the scheduler checks whether the work is done or not.</p> <p></p> <p>Finally the Airflow UI is updated.</p> <p>Check the video also.</p>"},{"location":"astronomer/#module-3-airflow-ui","title":"Module 3  : Airflow UI","text":"<p>Here the long vertical line is the DagRun Object and the short boxes are the Task Instances.</p> <p>Landing time view illustrates how much time each task takes and we can check if optimizations applied are efficient or not.</p>"},{"location":"astronomer/#gantt-charts","title":"Gantt Charts","text":"<p>These charts show how much time it took to run the DAG. </p> <p>Grey color means that the DAG was queued and green means the DAG was running and completed.</p> <p></p> <p>In this image, the second DAG took the longest to run.</p>"},{"location":"astronomer/#quiz-questions","title":"Quiz Questions","text":"<p>Video : Monitor DAG Runs and Task Instances</p> <p></p> <p></p> <p>Video: Overview Of DAG</p> <p></p> <p>So total number of successful DAGs are 4.</p> <p>Same type of logic here as well. Upstream Failed is represented by the orange color.</p> <p></p>"},{"location":"astronomer/#debug-and-rerun-dag","title":"Debug and Rerun DAG","text":"<p>Go to this UI page by going to the link <code>http://localhost:8080/dagrun/list/?_flt_3_dag_id=example_dag_basic</code></p> <p>Add filter equal to failed</p> <p></p> <p>Select the DAGs -&gt; Click on Action -&gt; Clear State to rerun the  DAGs</p> <p></p>"},{"location":"astronomer/#module-4-simple-dag","title":"Module 4 : Simple DAG","text":"<ul> <li>Catchup : Catchup refers to the process of scheduling and executing all the past DAG runs that would have been scheduled if the DAG had been created and running at an earlier point in time.</li> </ul>"},{"location":"astronomer/#create-dag-with-traditional-paradigm","title":"Create DAG with Traditional Paradigm","text":"<p>with is a context manager</p> <pre><code>from airflow import DAG\nfrom datetime import datetime\n\nwith DAG('my_dag', start_date=datetime(2023, 1 , 1),\n         description='A simple tutorial DAG', tags=['data_science'],\n         schedule='@daily', catchup=False):\n</code></pre>"},{"location":"astronomer/#using-the-taskapi","title":"Using the TaskAPI","text":"<p>@dag is a decorator</p> <pre><code>from airflow.decorators import dag\nfrom datetime import datetime\n\n@dag(start_date=datetime(2023, 1, 1), description='A simple tutorial DAG', \n     tags=['data_science'], schedule='@daily', catchup=False)\ndef my_dag():\n    None\n\nmy_dag()\n</code></pre>"},{"location":"astronomer/#defining-a-python-operator-task","title":"Defining a Python Operator Task","text":""},{"location":"astronomer/#dag-without-context-manager-with","title":"DAG without context manager with","text":"<p>Much simpler method with TaskFlowAPI</p> <pre><code>from airflow.decorators import dag, task\nfrom datetime import datetime\n\n@dag(start_date=datetime(2023, 1, 1), description='A simple tutorial DAG', \n     tags=['data_science'], schedule='@daily', catchup=False)\ndef my_dag():\n\n    @task\n    def print_a():\n        print('hi from task a')\n</code></pre>"},{"location":"astronomer/#chain-dependencies","title":"Chain Dependencies","text":"<p>First Import <code>from airflow.util.helpers imoprt chain</code></p> <p><code>task_a &gt;&gt; [task_b,task_c,task_d] &gt;&gt; task_e</code></p> <p></p> <p><code>chain(task_a,[task_b,task_c],[task_d,task_e])</code></p> <p></p>"},{"location":"astronomer/#setting-default-args","title":"Setting Default Args","text":"<pre><code>default_args = {\n    'retries': 3,\n}\n</code></pre>"},{"location":"astronomer/#dependencies-with-task-flow-api","title":"Dependencies with Task Flow API","text":"<pre><code>from airflow.decorators import dag, task\nfrom datetime import datetime\nfrom airflow.utils.helpers import chain\n\n\n@dag(start_date=datetime(2023, 1 , 1),\n         description='A simple tutorial DAG', tags=['data_science'],\n         schedule='@daily', catchup=False)\ndef my_dag():\n\n    @task\n    def print_a():\n        print('hi from task a')\n\n    @task\n    def print_b():\n        print('hi from task b')\n\n    @task\n    def print_c():\n        print('hi from task c')\n\n    @task\n    def print_d():\n        print('hi from task d')\n\n    @task\n    def print_e():\n        print('hi from task e')\n\n\n    print_a() &gt;&gt; print_b() &gt;&gt; print_c() &gt;&gt; print_d() &gt;&gt; print_e()\n\nmy_dag()\n</code></pre>"},{"location":"astronomer/#assignment-creating-dag-with-bash-operator","title":"Assignment : Creating DAG with Bash Operator","text":"<p>The DAG should look like this:</p> <p></p> <pre><code>from airflow import DAG\nfrom datetime import datetime\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(dag_id='check_dag', schedule='@daily', \n        start_date=datetime(2023, 1, 1), catchup=False,\n        description='DAG to check data', tags=['data_engineering']):\n\n    create_file = BashOperator(\n        task_id='create_file',\n        bash_command='echo \"Hi there!\" &gt;/tmp/dummy'\n    )\n\n    check_file_exists = BashOperator(\n        task_id='check_file_exists',\n        bash_command='test -f /tmp/dummy'\n    )\n\n    read_file = PythonOperator(\n        task_id='read_file',\n        python_callable=lambda: print(open('/tmp/dummy', 'rb').read())\n    )\n\n    create_file &gt;&gt; check_file_exists &gt;&gt; read_file\n</code></pre> <p>Quiz Questions</p> <p></p>"},{"location":"astronomer/#module-5-sheduling-dags","title":"Module 5 : Sheduling DAGs","text":""},{"location":"astronomer/#what-is-a-dagrun","title":"What is a DAGRun?","text":"<ul> <li>When the scheduler schedules the tasks to run, a DAG Run object is created with <code>data_interval_start</code> and <code>data_interval_end</code></li> </ul> <p>First the DAGRun is is in the Queued state, once the first task runs its in the running state.</p> <p></p> <p>Final task state determines the end state of the DAGRun.</p> <p></p> <p>Properties of DAG Run</p> <p></p>"},{"location":"astronomer/#how-dags-are-scheduled","title":"How DAGs are scheduled?","text":"<p>Example of three DAG runs</p> <p></p> <p>Thing to Remember</p> <p></p>"},{"location":"astronomer/#the-start_date-parameter","title":"The <code>start_date</code> parameter","text":""},{"location":"astronomer/#how-it-works","title":"How it works?","text":"<p>Scenario I</p> <p></p> <ul> <li> <p>Let's say there are three dag runs from start date until now.</p> </li> <li> <p>Today if we hit the trigger on the dag...</p> </li> <li> <p>Then all the other previous dag runs from the start date till now are run.</p> </li> <li> <p>This happens only for the first time that we run the dag.</p> </li> </ul> <p>Scenario II</p> <p></p> <ul> <li> <p>Let's say we made a mistake and stopped the DAG on 2nd Jan 2022,</p> </li> <li> <p>We fix it and then restart the DAG.</p> </li> <li> <p>In this case Airflow backfills the DAG from the last run and not the start date.</p> </li> <li> <p>So the dag is backfilled only from 2nd Jan and not 1st Jan.</p> </li> </ul>"},{"location":"astronomer/#cron-expressions-for-schedule_interval","title":"Cron Expressions for <code>schedule_interval</code>","text":"<p>use crontab.guru website to construct cron expressions</p> <p>cron expressions take into account day light saving time.</p> <p>how to trigger dags every three days?</p> <p></p> <p>Check the link for a complete summary.</p>"},{"location":"astronomer/#concept-of-catchup","title":"Concept Of Catchup","text":"<p>The scheduler runs all the previous DAGRuns between now and the date @ which the DAG was triggeres or the start date of the DAG.</p> <p></p>"},{"location":"astronomer/#backfilling-process","title":"Backfilling Process","text":"<p>In the figure below we can see that there are DAGRuns that are alreasy triggered and executed from start_date until now.</p> <p>But what if we want to trigger the DAGs that are before the start date?</p> <p>We can do this with the backfilling mechanism.</p> <p></p>"},{"location":"astronomer/#cli-command-to-backfill-the-dag","title":"CLI Command to Backfill the DAG","text":""},{"location":"astronomer/#quiz-questions_1","title":"Quiz Questions","text":""},{"location":"astronomer/#module-6-connections-in-airflow","title":"Module 6 : Connections In Airflow","text":"<p>To Interact with external systems like APIs we need Connections. They are a set of parameters such as login and password that are encrypted.</p> <p></p> <p>If we want to interact with a software appln via a connection we need to install its provider first. dbt has its own provider, snowflake has its provider...</p>"},{"location":"astronomer/#how-to-create-a-connection","title":"How to create a connection?","text":"<p>Go to Admin &gt; Connections &gt; Create New Connection (+ button)</p> <p></p> <p></p> <p>Password is the API Key from Calendarific Docs</p> <p>After Clicking on Save the connecton appears on the page</p> <p></p> <p>Check out the Registry to check the docs and parameters for any tool.</p> <p>We can export environment variables using the .env file and give parameters there, no need UI for this. Check this Snowflake Example</p> <p>To deploy the connections use <code>astro deployment variable create --deployment-id &lt;ID&gt; --load --env .env</code></p>"},{"location":"astronomer/#pain-points-with-connections","title":"Pain Points with Connections","text":""},{"location":"astronomer/#cannot-share-the-connections-from-dev-to-prod-environment","title":"Cannot share the connections from Dev to Prod Environment.","text":"<p>With Astro we can create a Connection Management Environment to manage the connections.</p> <p></p>"},{"location":"astronomer/#specific-forms-for-each-connection-type","title":"Specific Forms For Each Connection Type","text":"<p>Astro unlike Airflow provides us with custom forms for each connection type.</p> <p></p>"},{"location":"astronomer/#there-is-no-inheritance-model","title":"There is no Inheritance Model","text":"<p>In the above image we can see that most of the parameters in Dev Deployment and Prod Deployment are identical except the DB name. But with Airflow we cannot inherit the variables.</p> <p>This is solved by Astro.</p> <p></p>"},{"location":"astronomer/#there-is-no-secret-backend","title":"There is no Secret Backend","text":"<p>There is no secret vault storage that is encrypted in Airflow, we need to create our own but in Astro it comes in built.</p>"},{"location":"astronomer/#use-case-sharing-connections-with-dev-deployment-and-local-dev-environments","title":"Use Case : Sharing Connections with Dev Deployment and Local Dev Environments","text":"<p>You can check out the Snowflake Example of creating a Connection Management System in Astro Cloud, then enabling the local dev environments to access the secrets using a set of commands.</p>"},{"location":"astronomer/#quiz","title":"Quiz","text":""},{"location":"astronomer/#module-7-xcom","title":"Module 7 - XCom","text":"<p>Suppose there are two tasks A and B. We want to send a file from A to B.</p> <p>We can use an external system like S3 bucket where task A can upload it to the bucket and then task B can download it.</p> <p>We can use a native way using XCom(Airflow Meta DB)</p> <p></p>"},{"location":"astronomer/#properties-of-xcom","title":"Properties of XCom","text":""},{"location":"astronomer/#example-of-xcom","title":"Example Of XCom","text":"<p>Go to Admin &gt;&gt; XCom We can see that the variable is created</p> <p></p>"},{"location":"astronomer/#pulling-xcom-values-with-specific-key","title":"Pulling XCom Values with Specific Key","text":"<p>Another example</p>"},{"location":"astronomer/#pulling-multiple-values-once","title":"Pulling Multiple Values @ once","text":"<p>Here we can see that keys for both the tasks are the same, this is allowed because the XComs is defined not only by key but the dag_id and task_id also</p> <p></p>"},{"location":"astronomer/#limitations-of-xcom","title":"Limitations of XCom","text":"<p>If we use SQLLite, we can share at most one gb in a given XCom, for PostGres its 1 gb for a given Xcom.</p> <p>If we use MySQL, we can share atmost 64kb in a given XCom.</p> <p>So XCom is great for small data and it must be JSON Serializable.</p> <p>Example DAG Covering All Concepts</p> <pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nimport pendulum\nfrom airflow.models.taskinstance import TaskInstance as ti\n\n\n\ndef _transform(ti: ti):\n   import requests\n   resp = requests.get(f'https://swapi.dev/api/people/1').json()\n   print(resp)\n   my_character = {}\n   my_character[\"height\"] = int(resp[\"height\"]) - 20\n   my_character[\"mass\"] = int(resp[\"mass\"]) - 13\n   my_character[\"hair_color\"] = \"black\" if resp[\"hair_color\"] == \"blond\" else \"blond\"\n   my_character[\"eye_color\"] = \"hazel\" if resp[\"eye_color\"] == \"blue\" else \"blue\"\n   my_character[\"gender\"] = \"female\" if resp[\"gender\"] == \"male\" else \"female\"\n   ti.xcom_push(\"character_info\", my_character)\n\ndef _transform2(ti: ti):\n   import requests\n   resp = requests.get(f'https://swapi.dev/api/people/2').json()\n   print(resp)\n   my_character = {}\n   my_character[\"height\"] = int(resp[\"height\"]) - 50\n   my_character[\"mass\"] = int(resp[\"mass\"]) - 20\n   my_character[\"hair_color\"] = \"burgundy\" if resp[\"hair_color\"] == \"blond\" else \"brown\"\n   my_character[\"eye_color\"] = \"green\" if resp[\"eye_color\"] == \"blue\" else \"black\"\n   my_character[\"gender\"] = \"male\" if resp[\"gender\"] == \"male\" else \"female\"\n   ti.xcom_push(\"character_info\", my_character)\n\n\ndef _load(values):\n   print(values)\n\nwith DAG(\n   'xcoms_demo_4',\n   schedule = None,\n   start_date = pendulum.datetime(2023,3,1),\n   catchup = False\n):\n   t1 = PythonOperator(\n       task_id = '_transform',\n       python_callable = _transform\n   )\n\n   t2 = PythonOperator(\n       task_id = 'load',\n       python_callable = _load,\n       op_args = [\"{{ ti.xcom_pull(task_ids=['_transform','_transform2'], key='character_info') }}\"]\n   )\n\n   t3 = PythonOperator(\n       task_id = '_transform2',\n       python_callable = _transform2,\n   )\n   [t1,t3] &gt;&gt; t2\n</code></pre>"},{"location":"astronomer/#quiz-answers","title":"Quiz Answers","text":""},{"location":"astronomer/#module-8-variables-in-airflow","title":"Module 8 : Variables in Airflow","text":""},{"location":"astronomer/#introduction","title":"Introduction","text":"<p>Suppose there is an API and many endpoints/tasks like create, read and delete.</p> <p>We will need to hardcode the information about the API in every task we do. </p> <p>If the API changes, then we may need to hardcode and modify the info for every single task.</p> <p>We can solve this problem with variables. We can put all the info the API needs.</p> <p>If something changes, just change the variables, no need to change for every DAG.</p> <p></p> <p>The variable has three properties: key, value and description. All of them have to be JSON serializable.</p>"},{"location":"astronomer/#creating-a-variable","title":"Creating a Variable","text":"<p>Go to Admin &gt;&gt; Variables</p>"},{"location":"astronomer/#method-1-using-the-ui","title":"Method 1 : Using the UI","text":"<p>All the variables are stored in the meta database. The variables created this way using the UI is not very secure.</p>"},{"location":"astronomer/#hiding-the-values","title":"Hiding the Values","text":"<p>If we use a key name like api_key the value is automatically hidden. </p> <p>Variable keywords automatically hiding values</p> <pre><code>access_token\napi_key\napikey\nauthorization\npassphrase\npasswd\npassword\nprivate_key\nsecret\ntoken\nkeyfile_dict\nservice_account\n</code></pre>"},{"location":"astronomer/#method-2-using-environment-variables","title":"Method 2 : Using Environment Variables","text":"<p>In the <code>.env</code> file add the following line to create an environment variable.</p> <p><code>AIRFLOW_ML_MODEL_PARAMS:'{\"param\":[100,200,300]}'</code></p> <p>We can't see them on the Airflow UI.</p> <p>Benefits:</p> <ul> <li> <p>The variables are hidden on airflow UI.</p> </li> <li> <p>Since the variables are in .env they are not stored in the meta database, hence there is no need to create a connection to fetch these values.</p> </li> <li> <p>Easier to version the variables.</p> </li> </ul>"},{"location":"astronomer/#access-variables-from-dags","title":"\u26a0\ufe0f Access Variables From DAGS","text":"<p>We can use the variables in the DAG to perform operations also. The variable can be a single value or a list as seen below. </p> <pre><code>from airflow.operators.python import PythonOperator\nfrom airflow.models import Variable\n\nfrom datetime import datetime\n\ndef _ml_task(ml_parameter):\n    print(ml_parameter)\n\nwith DAG('ml_dag',start_time = datetime(2022,1,1),\n          schedule_interval = '@daily',catchup = False) as dag:\n\n          for ml_parameter in Variable.get('ml_model_parameters',deserialize_json = True)['params']:\n            PythonOperator(\n                task_id = f'ml_task_{ml_parameter}',\n                python_callable = _ml_task,\n                op_kwargs = {\n                    'ml_parameter':ml_parameter\n                }\n            )\n</code></pre>"},{"location":"astronomer/#dag-graph","title":"DAG Graph","text":""},{"location":"astronomer/#jinja-templating-to-access-variables","title":"\u26a0\ufe0f Jinja Templating to Access Variables","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.models import Variable\nfrom datetime import datetime\n\n\ndef _ml_task(ml_parameter):\n   print(ml_parameter)\n\nwith DAG('ml_dag', start_date=datetime(2022, 1, 1),\n   schedule_interval='@daily', catchup=False) as dag:\n   ml_tasks = []\n   for ml_parameter in Variable.get('ml_model_parameters', deserialize_json=True)[\"param\"]:\n       ml_tasks.append(PythonOperator(\n           task_id=f'ml_task_{ml_parameter}',\n           python_callable=_ml_task,\n           op_kwargs={\n               'ml_parameter': ml_parameter\n           }\n       ))\n\nreport = BashOperator(\n   task_id='report',\n   bash_command='echo \"report_{{ var.value.ml_report_name }}\"'\n)\n</code></pre> <p>Using Jinja Templating gives one advantage, we dont need to create a connection to access a variable everytime, its a one time thing. </p>"},{"location":"astronomer/#module-9-debugging-dags","title":"Module 9 : Debugging DAGs","text":""},{"location":"astronomer/#basic-checks","title":"Basic Checks","text":"<ul> <li> <p>Ensure that all your DAG files are located in the dags folder.</p> </li> <li> <p>The .airflowignore file does not have the name of your DAG file as Airflow ignores any files present on it.</p> </li> <li> <p>At the code level, ensure that each DAG:</p> <ul> <li>Has a unique dag_id. if two dags have the same id, Airflow randomly parses one of them.</li> <li>Contains either the word airflow or dag. The scheduler only scans files that meet this requirement.</li> <li>If you are using Airflow decorator to instantiate your dag with the @dag decorator, make sure  the decorated function is invoked at the end.</li> </ul> </li> </ul>"},{"location":"astronomer/#debugging-airflow-settings","title":"Debugging Airflow Settings","text":"<p>We can change the scheduler settings but its not recommended since it may overload the scheduler.</p>"},{"location":"astronomer/#module-error-checks","title":"Module Error Checks","text":"<p>Basically this error means the dag does not exist in the metadata database.</p> <p>To check the dags in your metadata database here is the command.</p> <p><pre><code>airflow dags list\n</code></pre> Just try restarting the scheduler and then check again.</p> <p>If it does not work then check logs of the scheduler.</p> <pre><code>astro dev logs -s\n</code></pre> <p>To check if you have any kind of import errors in any of the DAGs use:</p> <pre><code>astro dev run dags list-import-errors\n</code></pre>"},{"location":"astronomer/#scheduler-error-checks","title":"Scheduler Error Checks","text":"<p>If you have installed some new Provider like Databricks, its important to configure it properly in the yml and .env files, otherwise scheduler crashes and UI is not visible.</p>"},{"location":"astronomer/#module-management-errors","title":"Module Management Errors","text":"<p>Let's consider the following code example</p> <p></p> <p>Here we can see that we are importing task1 from pandas that is a file in the same directory but we still get the error.</p> <p></p> <p>The import is happening from a python package not the file in our directory. </p> <p>How to check if library exists in the airflow envt?</p> <pre><code>astro dev bash\n\npip freeze | grep pandas\n</code></pre> <p>So dont name the file as the name of any existing package.</p>"},{"location":"astronomer/#more-common-issues","title":"More Common Issues","text":"<ul> <li> <p>Refraining from immediately triggering DAGs after making changes to them or any other files in the DAG folder is advisable, as the scheduler may still need to parse the DAG.</p> </li> <li> <p>Confirm that you have unpaused your DAGs to enable them to execute according to their schedule.</p> </li> <li> <p>Ensure that the start_date of your DAG is set to a date in the past else if you trigger the DAG manually you will see a successful DAG run but no successful tasks.</p> </li> <li> <p>Ensure the end_date of your DAG is set to the future else you won\u2019t see any tasks executed like above.</p> </li> </ul> <p>Often if you expect many instances of your DAG or tasks to be running simultaneously, make sure you verify these core airflow settings usually found in airflow.cfg file.</p> <p><code>max_active_runs_per_dag</code> (The maximum number of active DAG runs per DAG). Default \u2192 16.</p> <p><code>max_active_tasks_per_dag</code> (The maximum number of task instances allowed to run concurrently in each DAG). Default \u2192 16.</p> <p><code>parallelism</code> (This defines the maximum number of task instances that can run concurrently per scheduler in Airflow, regardless of the worker count). Default \u2192 32.</p>"},{"location":"astronomer/#improper-behaviour-of-dags","title":"Improper Behaviour Of DAGs","text":"<p>Let's say we have a DAG that uses the Postgres Operator to create a new table(task 1) and also insert data into it(task 2). </p> <p>We can see an error :  </p> <p>The connection is not successful because the security admins restrict the database access based on the IP Addresses. This means the firewall allows conn to db based on the ip address.</p>"},{"location":"astronomer/#default-postgres-connection-id","title":"Default Postgres Connection Id","text":"<p>The default postgres connection id is <code>postgres_default</code></p> <p>So when we are going to use some other connection id like <code>pg_default</code> make sure that its specified during the connection, otherwise a connection would be established with the default one but the tables we need may not exist there.</p> <p>We need to make sure that connection can be made from external system where airflow has been installed.</p>"},{"location":"astronomer/#how-to-avoid-dependency-conflicts","title":"How to Avoid Dependency Conflicts?","text":"<ul> <li> <p>One option is the KubernetesPodOperator, which is suitable for users who operate Airflow on Kubernetes and require greater control over the resources and infrastructure used to run the task, in addition to package management. However, there are some drawbacks, such as a more complicated setup and increased task latency.</p> </li> <li> <p>The ExternalPythonOperator is another choice that enables you to execute certain tasks with a different set of Python libraries than others and the primary Airflow environment. This may be a virtual environment or any pre-installed Python installation that is accessible in the Airflow task's execution environment.</p> </li> <li> <p>Another option is the PythonVirtualenvOperator, which functions similarly as the ExternalPythonOperator . However, it generates and deletes a new virtual environment for each task. This operator is ideal if you don't want to retain your virtual environment. The disadvantage of this operator is that it takes more time to generate the environment each time the task is executed, resulting in higher task latency.</p> </li> </ul>"},{"location":"astronomer/#quiz-questions_2","title":"\u26a0\ufe0f Quiz Questions","text":"<p>Q1. The DAG does not come on UI? Why?</p> <p></p> <p></p> <p></p>"},{"location":"databricks/","title":"Databricks","text":""},{"location":"databricks/#quick-access","title":"Quick Access","text":"<ol> <li> <p>Databricks Lakehouse Fundamentals</p> </li> <li> <p>Data Engineering with Databricks</p> <ul> <li> <p>Getting Started with Workspace</p> </li> <li> <p>Transform Data with Spark</p> </li> <li> <p>Manage Data with Delta Lake</p> </li> <li> <p>Building Data Pipelines wit DLT</p> </li> <li> <p>Workflows in Databricks</p> </li> <li> <p>Data Access with Unity Catelog</p> </li> </ul> </li> <li> <p>Brian Cafferky Training Material</p> <ul> <li> <p>Dimension Modelling Concepts</p> </li> <li> <p>Understanding Delta Event Logs</p> </li> <li> <p>Delta Live Table Pipeline Example</p> </li> <li> <p>Databricks Workflows Features</p> </li> </ul> </li> </ol> <p>Databricks Workspace link</p>"},{"location":"databricks/#databricks-lakehouse-fundamentals","title":"Databricks Lakehouse Fundamentals","text":""},{"location":"databricks/#what-is-a-data-warehouse","title":"What is a Data Warehouse?","text":"<ul> <li>Data Warehouse unlike relational databases provided Business Intelligence, analytics and pre defined schema logic.</li> <li>They were not designed for semi and unstructured data. They can't handle upticks in volume and velocity and had long processing times.</li> </ul>"},{"location":"databricks/#data-lakes","title":"Data lakes","text":"<ul> <li>Data Lakes had flexible data storage capabilities, streaming support and AI ML capabilities.</li> <li>But it introduced concerns and are not supported for transactional data. There is no data reliability and data governance is still a concern.</li> </ul> <p>Hence businesses required two different data platforms. </p> <ul> <li>The data warehouse had structured tables for BI and SQL Analytics, whereas data lakes had unstructured data for Data Science and Data Streaming.</li> <li>They also had different governance and security models and most important there were two separate copies of data.</li> </ul>"},{"location":"databricks/#how-does-the-lakehouse-solve-this-problem","title":"How does the Lakehouse Solve this Problem?","text":"<ul> <li>It can serve all ML, SQL and BI, streaming use cases.</li> <li>One security and governance approach for all data assets on cloud.</li> <li>An open and reliable data platform to efficiently handle all data types.</li> </ul>"},{"location":"databricks/#key-features","title":"Key Features","text":"<ul> <li>Transaction Support</li> <li>Schema Enforcement and Governance </li> <li>BI Support </li> <li>Data Governance</li> <li>Decoupled Storage from Compute and Separate Clusters.</li> <li>Support for Diverse data workload in the same data repository.</li> </ul>"},{"location":"databricks/#problems-with-data-lake","title":"Problems with Data Lake","text":"<ul> <li>Lack of ACID Transaction support</li> <li>Lack of Schema Enforcement</li> <li>Lack of Integration with Data Catalog</li> </ul>"},{"location":"databricks/#delta-lake","title":"Delta Lake","text":"<ul> <li>File based open data format that provides ACID transaction guarantees. We can handle metadata for petabytes of data.</li> <li>Audit history and time travel capabilities.</li> <li>Schema Enforcement ensures there is no wrong data that is in the tables and schema evolution to accommodate ever changing data.</li> <li>Supports Change Data Capture, Slowly Changing Dimensions and  Streaming Upserts.</li> <li>Delta Tables are based on Apache Parquet, common format for structuring data.</li> <li>It has a transaction log and acts as a single source of truth.</li> </ul>"},{"location":"databricks/#photon","title":"Photon","text":"<ul> <li>Next Gen query engine that saves costs, its compatible with Spark APIs. Loading and querying data becomes increasingly faster.</li> <li>Each Spark Executor can have a photon engine that accelerates portion of spark and sql queries.</li> </ul>"},{"location":"databricks/#why-is-a-unified-governance-and-security-model-structure-important","title":"Why is a Unified governance and security model structure important?","text":"<p>The more individual access points added to the system like users, groups or external connectors, the higher the risk of data breaches.</p> <p>Some challenges are diversity of data assets, using two incompatible platforms and fragmented tool usage.</p> <p>Databricks overcomes these challenges by using Unity Catalog for Data Governance, delta sharing to share data across any computing platform.</p> <p>Unlike a few years back when each workspace/team had different Access Controls, User Management and Metastores, with Unity Catalog we can have centralized access controls and user management, including row and column level access permission privileges.</p> <p>We can control access to multiple data items at one time eg. personal info can be tagged and a single rule can be defined to provide access as needed.</p> <p>Unity provides highly detailed audit trails that define who has accessed what data at what time and also highlights the changes made.</p> <p>Data Lineage is provided by Unity Catalog and it includes the history of data, what datasets it came from, who created it and when + the transformations performed on it.</p>"},{"location":"databricks/#data-sharing-with-delta-sharing","title":"Data Sharing With Delta Sharing","text":"<p>Usually the data is shared as tables and not files. So this system is not scalable.</p> <p>We cannot share data across platforms using traditional technology.</p> <p>Delta Sharing allows the data to be moved to any cloud platform securely.</p> <p>Advantages</p> <ul> <li>No new ingestion processes needed to share data and integrates with PowerBI, Tableau, Spark and Pandas.</li> <li>Data is shared live without copying it.</li> <li>Centralized Admin and Governance.</li> <li>The data products can be built and packaged via a central marketplace.</li> <li>There are privacy safe clean rooms to secure data and collaboration between vendors. </li> </ul>"},{"location":"databricks/#divided-security-architecture","title":"Divided Security Architecture","text":""},{"location":"databricks/#control-plane","title":"Control Plane","text":"<ul> <li>Consists of managed backend services that Databricks provides.</li> <li>These live in Databricks own cloud account.</li> <li>It runs the web application and manages the notebooks, applications and clusters.</li> </ul>"},{"location":"databricks/#data-plane","title":"Data Plane","text":"<p>Data plane is where the data is computed. Unless we use serverless compute, the clusters run in the business owner's own cloud account.</p> <p>The information in the control plane is encrypted at rest and in transit.</p> <p>Databricks clusters are shortlived and do not persist after job termination.</p> <p>If there are any security issues coming up, the service request can be generated and the Databricks employees are given access to the workspace for a certain duration of time.</p>"},{"location":"databricks/#user-access-and-identity","title":"User Access and Identity","text":"<ul> <li>Table ACL feature</li> <li>IAM Instance Profiles</li> <li>Securely Store access keys</li> <li>The Secrets API</li> </ul>"},{"location":"databricks/#instant-compute-and-serverless","title":"Instant Compute and Serverless","text":"<p>In normal scenario, we run clusters on the dataplane that's connected to an external storage.</p> <p>But some challenges with this are that:</p> <ul> <li>Cluster Creation is complicated.</li> <li>Environment Setup is slow</li> <li>Capacity and costs of the business cloud account should be managed.</li> </ul> <p>In Serverless Compute, Databricks allows us to run the clusters on their cloud account instead of the business.</p> <p>The environment starts immediately and can scale in seconds.</p> <p>These servers are unassigned to any user, always in a warm state and waiting to run jobs given by the users.</p> <p>The three layers of isolation in the container that is hosting the runtime, virtual machine hosting the container and the virtual network for the workspace.</p> <p>Each of the parts is isolated with no sharing or cross network traffic allowed.</p> <p>Once the job is done, the VM is terminated and not used again for other compute tasks.</p>"},{"location":"databricks/#common-data-lakehouse-terminology","title":"Common Data Lakehouse Terminology","text":""},{"location":"databricks/#unity-catalog-components","title":"Unity Catalog Components","text":"<ol> <li>Metastore: Top level logical container in Unity Catalog. It's a construct that represents the metadata. They offer improved security and other useful features like auditing.</li> <li>Catalog : Top most container for data objects in Unity Catalog. Data Analysts use this to reference data objects in UC.</li> </ol> <p>There are three main namespaces to address the data location names in UC. The format is <code>SELECT * FROM catalog.schema.table</code></p> <ol> <li> <p>Schema : Contains tables and views and is unchanged by UC. Forms the second part of the three level namespace. Catalogs can contain many schemas as desired.</p> </li> <li> <p>Tables : SQL relations with ordered list of columns. They have metadata like comments, tags and list of columns.</p> </li> <li> <p>Views : They are stored queries that are executed when we query the view. They are read only.</p> </li> </ol> <p>Other components are Storage Credentials created by admins and used to authenticate with cloud storage containers.</p> <p>Shares and recipients is related to delta sharing for low overhead sharing over different channels inside or outside organization by linking metastores in different parts of the world. </p> <p>The metastore is essentially a logical construct with Control Plane and Cloud Storage.</p> <p>The metadata information about the data objects and the ACLs are stored in control plane and data related to objects maintained by the metastore is stored in cloud storage.</p>"},{"location":"databricks/#challenges-in-data-engineering-workload","title":"Challenges in Data Engineering Workload","text":"<ul> <li>Complex Data Ingestion Methods</li> <li>Support For data engineering principles</li> <li>Third Party Orchestration Tools</li> <li>Pipeline Performance Tuning</li> <li>Inconsistencies between partners.</li> </ul> <p>The Databricks Lakehouse platform provides us with managed data ingestion, schema detection, enforcement and evaluation along with declarative and auto scaling data flow with a native orchestrator.</p>"},{"location":"databricks/#capabilities-of-de-in-lake-house","title":"Capabilities of DE in Lake House","text":"<ul> <li>easy data ingestion</li> <li>auto etl pipelines</li> <li>data quality checks</li> <li>batch and stream tuning</li> </ul>"},{"location":"databricks/#autoloader","title":"Autoloader","text":"<p>As data loads in the lakehouse, Databricks can infer the schema after processing the data as they arrive in the cloud storage.</p> <p>It auto detects the schema and enforces it guaranteeing data quality.</p> <p>The <code>COPY INTO</code> command is used by data analysts to load data from a folder to the Delta Lake Table.</p>"},{"location":"databricks/#delta-live-tables","title":"Delta Live Tables","text":"<p>ETL framework that uses a simple declarative approach to build reliable pipelines and automatically auto scales the infra so that data folks can spend less time on tooling and get value from data.</p> <ul> <li>We can declaratively express entire data flows in Python.</li> <li>Natively enable software engineering best practices such as separate dev and prod environments and test before deployment in a single API.</li> </ul>"},{"location":"databricks/#workflows","title":"Workflows","text":"<p>Orchestration service embedded in Databricks Lakehouse platform. Allow data teams to build reliable data workflows on any cloud.</p> <p>We can orchestrate pipelines written in DLT or dbt, ML pipelines etc.</p> <p>We can use external tools like Apache Airflow to manage the workflows or even use the API.</p> <p>One example of delta live tables pipeline is using Twitter Stream API to retrieve live tweets to S3, then use delta live tables to ingest, clean and transform tweets and finally do sentiment analysis.</p>"},{"location":"databricks/#data-streaming-workloads","title":"Data Streaming Workloads","text":"<ul> <li> <p>Every organization generates large amounts of real time data. This data includes transaction records, third party news, weather, market data and real time feeds, web clicks, social posts, emails and instant messages.</p> </li> <li> <p>Some applications of real time data are Fraud Detection, Personalized Offers, Smart Pricing, Smart Devices and Predictive maintainence.</p> </li> </ul> <p>Databricks supports real time analytics, real time ML and real time applications.</p> <p>Specific use cases include Retail, Industrial Automation, Healthcare and Financial Instituitions.</p>"},{"location":"databricks/#ml-workloads","title":"ML Workloads","text":"<p>Problems</p> <ul> <li>Multiple Tools Available</li> <li>Hard to track experiments</li> <li>Reproducing Results is hard</li> <li>ML Models are hard to deploy</li> </ul> <p>Solutions</p> <ul> <li>Built in ML Frameworks and model explainability</li> <li>Support for Distributed Training</li> <li>AutoML and Hyperparameter Tuning</li> <li>Support for hardware accelerators</li> </ul>"},{"location":"databricks/#credential","title":"Credential","text":""},{"location":"databricks/#databricks-academy-data-engineer-learning-plan","title":"Databricks Academy : Data Engineer Learning Plan","text":"<p>Link to the course : click here</p>"},{"location":"databricks/#course-1-data-engineering-with-databricks","title":"Course 1 : Data Engineering with Databricks","text":""},{"location":"databricks/#goals","title":"Goals","text":"<ul> <li> <p>Use the Databricks Data Science and Engineering Workspace to perform common code development tasks in a data engineering workflow.</p> </li> <li> <p>Use Spark SQL/PySpark to extract data from a variety of sources, apply common cleaning transformations, and manipulate complex data with advanced functions.</p> </li> <li> <p>Define and schedule data pipelines that incrementally ingest and process data through multiple tables in the lakehouse using Delta Live Tables in Spark SQL/PySpark. </p> </li> <li> <p>Create and manage Databricks jobs with multiple tasks to orchestrate and monitor data workflows. Configure permissions in Unity Catalog to ensure that users have proper access to databases for analytics and dashboarding.</p> </li> </ul>"},{"location":"databricks/#getting-started-with-the-workspace","title":"Getting Started With the Workspace","text":"<p>Architecture and Services</p> <p></p> <ul> <li> <p>The data plane has the compute resources and clusters that is connected to a cloud storage. It can be single or multiple cloud storage accounts.</p> </li> <li> <p>The Control Plane stores the UI, notebooks and jobs and gives the ability to manage clusters and interact with table metadata.</p> </li> <li> <p>Workflow manager allows us to manage tasks and pipelines.</p> </li> <li> <p>Unity Catalog mostly provides with Data Lineage, Data Quality and Data Discovery</p> </li> <li> <p>There are three personas that Databricks provides : Data Science and Engineering Persona, ML Persona and SQL Analyst Persona</p> </li> <li> <p>Cluster is a set of computational resources where workloads can be run as notebooks or jobs.</p> </li> <li> <p>The clusters live in the data plane in the org cloud account but cluster mgmt is fn of control plane.</p> </li> </ul> <p>\ud83e\udded Simple Analogy Imagine a restaurant:</p> <p>The control plane is the manager\u2019s office \u2014 taking orders, scheduling, planning.</p> <p>The data plane is the kitchen \u2014 where the food (your data) is actually cooked and served.</p> <p>\ud83e\udde0 In Databricks Terms: Term    Meaning Control Plane   The brains of the platform \u2014 manages jobs, notebooks, users, UI, APIs Data Plane  The muscles \u2014 where your code runs and your data is processed</p> <p>\ud83d\udd27 Control Plane Hosted by Databricks (in their cloud)</p> <p>Handles:</p> <p>Notebooks, jobs, clusters UI</p> <p>User authentication &amp; access control</p> <p>Job scheduling, monitoring, logging</p> <p>No access to your data</p> <p>Think of it as a remote command center</p> <p>\u2705 Always outside your VPC</p> <p>\ud83d\udcbd Data Plane Runs inside your cloud (VPC) \u2014 AWS, Azure, or GCP</p> <p>Handles:</p> <p>Spark cluster workers</p> <p>Data access from S3/Blob/GCS</p> <p>UDFs, jobs, pipelines</p> <p>\u2705 This is where your actual data lives and is processed</p> <p>\ud83d\udee1\ufe0f Security Separation Databricks control plane never sees your actual data.</p> <p>You can even encrypt traffic between planes or use PrivateLink to limit internet exposure.</p> <p>\ud83e\uddea Example: You submit a notebook in Databricks:</p> <p>Request hits the control plane (UI/API layer)</p> <p>The control plane tells the data plane to spin up a cluster</p> <p>Your code runs on executors in the data plane, accessing your data securely</p>"},{"location":"databricks/#compute-resources","title":"Compute Resources","text":""},{"location":"databricks/#overview","title":"Overview","text":""},{"location":"databricks/#cluster-types","title":"Cluster Types","text":"<ul> <li>Job Clusters cannot be restarted if terminated.</li> <li>All purpose clusters can be started whenever we want it to.</li> </ul>"},{"location":"databricks/#cluster-mode","title":"Cluster Mode","text":""},{"location":"databricks/#databricks-runtime-version","title":"Databricks Runtime Version","text":""},{"location":"databricks/#access-mode","title":"Access Mode","text":"<p>Specifies overall security model of the cluster. </p> <ul> <li>DBFS mounts are supported by single user clusters.</li> </ul>"},{"location":"databricks/#cluster-policies","title":"Cluster Policies","text":""},{"location":"databricks/#access-control-matrix","title":"Access Control Matrix","text":"<ul> <li>On shared security mode multiple users can be granted access.</li> <li>On single user security mode, each user will have their own cluster.</li> </ul>"},{"location":"databricks/#why-use-databricks-notebooks","title":"Why Use Databricks Notebooks?","text":""},{"location":"databricks/#databricks-utilities","title":"Databricks Utilities","text":""},{"location":"databricks/#databricks-repos","title":"Databricks Repos","text":"<p>Some supported operations include:</p> <ul> <li>Cloning a repository, pulling and upstream changes.</li> <li>Adding new items, creating new files, committing and pushing.</li> <li>Creating a new branch.</li> <li>Any changes that are made in a Databricks Repo can be tracked in a Git Repo</li> </ul> <p>We cannot DELETE branches from repos in databricks. It has to be done using Github/Azure Devops.</p> <p>Many operations of the control plane can be versioned using Repos feature like keeping track of versions of notebooks and also to test clusters.</p>"},{"location":"databricks/#transform-data-with-spark","title":"Transform Data With Spark","text":""},{"location":"databricks/#data-objects-in-the-lakehouse","title":"Data Objects in the Lakehouse","text":"<ul> <li>Catalog - Grouping of Databases</li> <li>Schema - Grouping of Objects in catalog</li> <li>Every schema has a table that is managed or external</li> </ul>"},{"location":"databricks/#managed-vs-external-storage","title":"Managed vs External Storage","text":"<ul> <li> <p>Managed Tables are made up of files that are stored in a managed store location configured to the metastore. Dropping the table deletes all the files also.</p> </li> <li> <p>In case of external tables, the data is stored in a cloud storage location. When we drop an external table, this underlying data is retained.</p> </li> <li> <p>View is a saved query against one or more databass. Can be temporary or global. Temp Views are scoped only to the current spark session</p> </li> <li> <p>CTE's only alias the results of the query while that query is being planned or executed.</p> </li> </ul>"},{"location":"databricks/#extracting-data-directly-from-files-with-spark-sql","title":"Extracting Data Directly from Files with Spark SQL","text":"<p>Details in the JSON Clickstream File</p> <p><pre><code>SELECT * FROM json.`${DA.paths.kafka_events}/001.json`\n</code></pre> </p> <p>Querying a Directory of Files</p> <p><pre><code>SELECT * FROM json.`${DA.paths.kafka_events}`\n</code></pre> </p> <p>Create a View for the Files</p> <pre><code>CREATE OR REPLACE VIEW event_view\nAS SELECT * FROM json.`${DA.paths.kafka_events}`\n</code></pre> <p>Create Temporary References</p> <pre><code>CREATE OR REPLACE TEMP VIEW events_temp_view\nAS SELECT * FROM json.`${DA.paths.kafka_events}`\n</code></pre> <p>Common table expressions</p> <p>These only exist while running the cell. CTEs only alias the results of the query while the cell is being planned and executed.</p> <pre><code>WITH cte_json\nAS (SELECT * FROM json.`${DA.paths.kafka_events}`)\nSELECT * FROM cte_json\n</code></pre> <pre><code>SELECT COUNT(*) FROM cte_json\n</code></pre> <p>The Temp Viws are scoped only to the current spark session. </p>"},{"location":"databricks/#working-with-binary-files","title":"Working with Binary Files","text":"<p>Extract the Raw Bytes and Metadata of a File</p> <p>Some workflows may require working with entire files, such as when dealing with images or unstructured data. Using <code>binaryFile</code> to query a directory will provide file metadata alongside the binary representation of the file contents.</p> <p>Specifically, the fields created will indicate the <code>path</code>, <code>modificationTime</code>, <code>length</code>, and <code>content</code>.</p> <pre><code>SELECT * FROM binaryFile.`${DA.paths.kafka_events}`\n</code></pre>"},{"location":"databricks/#providing-options-when-dealing-with-external-data-sources","title":"Providing Options When Dealing with External Data Sources","text":"<p>Directly Querying the csv file</p> <pre><code>SELECT * FROM csv.`${DA.paths.sales_csv}`\n</code></pre> <p>The data is not formatted properly. </p> <p>Registering Tables on External Data with Read Options</p> <p>While Spark will extract some self-describing data sources efficiently using default settings, many formats will require declaration of schema or other options.</p> <p>While there are many additional configurations you can set while creating tables against external sources, the syntax below demonstrates the essentials required to extract data from most formats.</p> <p><code> CREATE TABLE table_identifier (col_name1 col_type1, ...) USING data_source OPTIONS (key1 = val1, key2 = val2, ...) LOCATION = path </code></p> <p>Creating a table using SQL DDL and Providing Options</p> <pre><code>CREATE TABLE IF NOT EXISTS sales_csv\n    (order_id LONG,email STRING,timestamp LONG,total_item_quantity INTEGER,items STRING)\nUSING CSV\nOPTIONS (\n    header = 'true'\n    delimiter = \"|\"\n)\nLOCATION \"${paths.dba_sales.csv}\"\n</code></pre> <p>No data will be moved while creating out tables. the data is just called from the files.</p> <p>NOTE: When working with CSVs as a data source, it's important to ensure that column order does not change if additional data files will be added to the source directory. Because the data format does not have strong schema enforcement, Spark will load columns and apply column names and data types in the order specified during table declaration.</p> <p>Checking the Description of the Table</p> <pre><code>DESCRIBE EXTENDED sales_csv\n</code></pre> <p>IMP!!! : The table that is created using the external source will be in CSV format and not delta.</p> <p></p>"},{"location":"databricks/#limits-of-tables-with-external-data-sources","title":"Limits of Tables with External Data Sources","text":"<ul> <li> <p>When we are using external data sources other than Delta Lake and Data Lakehouse we can't expect the performance to be good always.</p> </li> <li> <p>Delta Lake will always guarantee that we get the most recent data from the storage.</p> </li> </ul> <p>Example</p> <p>Here is an example where external file data is being updated in out sales_csv table.</p> <pre><code>%python\n(spark.read\n      .option(\"header\", \"true\")\n      .option(\"delimiter\", \"|\")\n      .csv(DA.paths.sales_csv)\n      .write.mode(\"append\")\n      .format(\"csv\")\n      .save(DA.paths.sales_csv, header=\"true\"))\n</code></pre> <p>The count method on this will not reflect the newly added rows in the dataset.</p> <p>At the time we previously queried this data source, Spark automatically cached the underlying data in local storage. This ensures that on subsequent queries, Spark will provide the optimal performance by just querying this local cache.</p> <p>Our external data source is not configured to tell Spark that it should refresh this data. </p> <p>We can manually refresh the cache of our data by running the <code>REFRESH TABLE</code> command.</p> <p>Note that refreshing the table will invalidate out cache so it needs to be rescanned again. </p>"},{"location":"databricks/#using-jdbc-to-extract-data-from-sql-databases","title":"Using JDBC to extract data from SQL Databases","text":"<p>SQL databases are an extremely common data source, and Databricks has a standard JDBC driver for connecting with many flavors of SQL.</p> <pre><code>DROP TABLE IF EXISTS users_jdbc\n\nCREATE TABLE users_jdbc\nUSING jdbc\nOPTIONS (\n    url = \"jdbc:sqllite:paths.ecommerce_db\",\n    dtable = 'users'\n)\n</code></pre> <p>Checking if there are any files in the JDBC</p> <p>Table Description </p> <pre><code>%python\nimport python.sql.functions as F\n\nlocation = spark.sql(\"DESCRIBE EXTENDED users_jdbc\").filter(F.col(\"col_name\") == \"Location\").first[\"data_type\"]\nprint(location)\n\nfiles = db.fs.ls(location)\nprint(f\"Found {len(files)} files\"\n</code></pre>"},{"location":"databricks/#how-does-spark-interact-with-external-databases","title":"How does Spark Interact with External Databases","text":"<ul> <li> <p>Move the entire database to Databricks and then execute logic on the currently active cluster.</p> </li> <li> <p>Pushing the query to an external database and only transfer results back to Databricks.</p> </li> <li> <p>There will be network transfer latency while moving data back and forth between databricks and DWH.</p> </li> <li>Queries will not run well on big tables. </li> </ul>"},{"location":"databricks/#cleaning-data-using-spark","title":"Cleaning Data using Spark","text":"<p>Data </p> <p>Check the table counts</p> <p><pre><code>SELECT count(*), count(user_id),count(user_first_timestamp)\nFROM users_dirty\n</code></pre> </p> <p>We can observe that some data is missing.</p> <p><pre><code>SELECT COUNT(*) FROM users_dirty \nWHERE email IS NULL\n</code></pre> 848 records are missing.</p> <p>Using Python the same might be done </p> <pre><code>from pyspark.sql.functions import col\nusersDF = spark.read.table(\"users_dirty\")\n\nusersDF.where(col(\"email\").isNull()).count()\n</code></pre>"},{"location":"databricks/#deduplicating-the-rows-based-on-specific-columns","title":"Deduplicating the Rows Based on Specific Columns","text":"<p>The code below uses <code>GROUP BY</code> to remove duplicate records based on <code>user_id</code> and <code>user_first_touch_timestamp</code> column values. (Recall that these fields are both generated when a given user is first encountered, thus forming unique tuples.)</p> <p>Here, we are using the aggregate function <code>max</code> as a hack to:</p> <ul> <li> <p>Keep values from the <code>email</code> and <code>updated</code> columns in the result of our group by</p> </li> <li> <p>Capture non-null emails when multiple records are present</p> </li> </ul> <p>Steps to Deduplicate</p> <ol> <li>Fetch All the Records [986 records]</li> </ol> <pre><code>CREATE OR REPLACE TEMP VIEW deduped_users AS \nSELECT user_id, user_first_touch_timestamp, max(email) AS email, max(updated) AS updated\nFROM users_dirty\n</code></pre> <ol> <li>Filter records where user_id is not null [983 records]</li> </ol> <pre><code>CREATE OR REPLACE TEMP VIEW deduped_users AS \nSELECT user_id, user_first_touch_timestamp, email AS email, updated AS updated\nFROM users_dirty\nWHERE user_id IS NOT NULL;\n\nSELECT * FROM deduped_users\n</code></pre> <ol> <li>Group by <code>user_id</code> and <code>user_first_timestamp</code></li> </ol> <pre><code>CREATE OR REPLACE TEMP VIEW deduped_users AS \nSELECT user_id, user_first_touch_timestamp, first(email) AS email, first(updated) AS updated\nFROM users_dirty\nWHERE user_id IS NOT NULL\nGROUP BY user_id, user_first_touch_timestamp;\n\nSELECT * FROM deduped_users\n</code></pre> <p>We can use max also since we dont care which value is grouped by for email and updated</p> <pre><code>CREATE OR REPLACE TEMP VIEW deduplicated AS\nSELECT user_id,user_timestamp, max(email) AS email, max(updated) AS updated\nFROM users_dirty\nWHERE user_id IS NOT NULL\nGROUP BY user_id,user_first_touch_timestamp;\n</code></pre> <p>In either case we get 917 records.</p> <p>Check for distinct <code>user_id</code> and <code>user_first_touch_timestamp</code> rows</p> <p><pre><code>SELECT COUNT(DISTINCT(user_id, user_first_touch_timestamp))\nFROM users_dirty\nWHERE user_id IS NOT NULL\n</code></pre> We get 917 rows.</p>"},{"location":"databricks/#validating-duplicates","title":"Validating Duplicates","text":"<p>Based on our manual review above, we've visually confirmed that our counts are as expected.</p> <p>We can also programmatically perform validation using simple filters and <code>WHERE</code> clauses.</p> <p>Validate that the <code>user_id</code> for each row is unique.</p> <p><pre><code>SELECT max(row_count) &lt;= 1 AS no_of_duplicate_ids FROM(\n    SELECT user_id, count(*) AS row_count\n    FROM deduped_users\n    GROUP BY user_id\n)\n</code></pre> - true -&gt; if no duplicate ids - false -&gt; if dup ids are there</p> <p>Checking if each user has at most one email id</p> <pre><code>SELECT max(row_count) &lt;= 1 no_of_duplicate_email FROM (\n    SELECT email,COUNT(user_id) AS user_id_count\n    FROM deduped_users\n    WHERE email IS NOT NULL \n    GROUP BY email\n)\n</code></pre> <p>In Python the same thing is done via:</p> <pre><code>display(dedupedDF\n    .where(col(\"email\").isNotNull())\n    .groupby(\"email\")\n    .agg(count(\"user_id\").alias(\"user_id_count\"))\n    .select((max(\"user_id_count\") &lt;= 1).alias(\"at_most_one_id\")))\n</code></pre>"},{"location":"databricks/#working-with-regex","title":"Working with RegEx","text":"<ul> <li>Correctly scale and cast the <code>user_first_touch_timestamp</code></li> <li>Extract the calendar date and time in a human readable format</li> <li>Use <code>regexp_extract</code> to fetch the email domains. Docs</li> </ul> <pre><code>SELECT *,\n    date_format(first_touch,\"MM DD,YYYY\",first_touch_date),\n    date_format(first_touch,\"HH:mm:ss\",first_touch_time),\n    regexp_extract(email,\"?&lt;=@.+\") AS email_domain\nFROM (\n    SELECT *,\n        CAST(user_first_touch_timestamp/1e6 AS time_stamp) AS first_touch\n    FROM deduped_users\n)\n</code></pre> <p>Why divide by 1e6 to convert timestamp to a date?</p> <p>In Python</p> <pre><code>from pyspark.sql.functions import date_format, regexp_extract\n\ndisplay(dedupedDF\n    .withColumn(\"first_touch\", (col(\"user_first_touch_timestamp\") / 1e6).cast(\"timestamp\"))\n    .withColumn(\"first_touch_date\", date_format(\"first_touch\", \"MMM d, yyyy\"))\n    .withColumn(\"first_touch_time\", date_format(\"first_touch\", \"HH:mm:ss\"))\n    .withColumn(\"email_domain\", regexp_extract(\"email\", \"(?&lt;=@).+\", 0))\n)\n</code></pre>"},{"location":"databricks/#complex-transformations-on-json-data","title":"Complex Transformations on JSON data","text":"<p><pre><code>from pyspark.sql.functions import col\n\nevents_trigger_df = (spark\n    .table(\"events_raw\"),\n    .select(col(\"key\").cast(\"string\"),\n            col(\"value\").cast(\"string\"))\n)\ndisplay(events_trigger_df)\n</code></pre>  The value column in the events data is nested.</p>"},{"location":"databricks/#working-with-nested-data","title":"Working With Nested Data","text":"<p>Table </p> <p>The code cell below queries the converted strings to view an example JSON object without null fields (we'll need this for the next section).</p> <p>NOTE: Spark SQL has built-in functionality to directly interact with nested data stored as JSON strings or struct types. - Use <code>:</code> syntax in queries to access subfields in JSON strings - Use <code>.</code> syntax in queries to access subfields in struct types</p> <p>Task: Check where the event name is finalized</p> <pre><code>SELECT * FROM events_strings WHERE value:event_name = \"finalize\" ORDER BY key LIMIT 1\n</code></pre> <pre><code>display(events_string_df\n    .where(\"value:event_name = 'finalize'\")\n    .orderBy(\"key\")\n    .limit(1)\n)\n</code></pre> <p>Extracting the schema of the JSON</p> <p><pre><code>SELECT schema_of_json('{\"device\":\"Linux\",\"ecommerce\":{\"purchase_revenue_in_usd\":1075.5,\"total_item_quantity\":1,\"unique_items\":1},\"event_name\":\"finalize\",\"event_previous_timestamp\":1593879231210816,\"event_timestamp\":1593879335779563,\"geo\":{\"city\":\"Houston\",\"state\":\"TX\"},\"items\":[{\"coupon\":\"NEWBED10\",\"item_id\":\"M_STAN_K\",\"item_name\":\"Standard King Mattress\",\"item_revenue_in_usd\":1075.5,\"price_in_usd\":1195.0,\"quantity\":1}],\"traffic_source\":\"email\",\"user_first_touch_timestamp\":1593454417513109,\"user_id\":\"UA000000106116176\"}') AS schema\n</code></pre> </p> <p>Task: Convert the JSON data to table/view</p> <p><pre><code>CREATE OR REPLACE TEMP VIEW parsed_events AS SELECT json.* FROM\n(\n    SELECT from_json(value, '&lt;the schemaabove&gt;') AS json\n    FROM event_strings\n) \n</code></pre> Check 3:19 in  for the result...</p> <pre><code>SELECT * FROM parsed_events;\n</code></pre> <p>Some more code examples for <code>from_json</code></p> <p><pre><code>#Convert JSON string column to Map type\nfrom pyspark.sql.types import MapType,StringType\nfrom pyspark.sql.functions import from_json\ndf2=df.withColumn(\"value\",from_json(df.value,MapType(StringType(),StringType())))\ndf2.printSchema()\ndf2.show(truncate=False)\n</code></pre> Docs for Map Type</p> <p>Docs for from_json</p>"},{"location":"databricks/#array-manipulation-functions","title":"Array Manipulation Functions","text":"<ul> <li> <p><code>explode()</code> separates the elements of an array into multiple rows; this creates a new row for each element.</p> </li> <li> <p><code>size()</code> provides a count for the number of elements in an array for each row.</p> </li> </ul> <p>The code below explodes the <code>items</code> field (an array of structs) into multiple rows and shows events containing arrays with 3 or more items.</p> <pre><code>CREATE OR REPLACE TEMP VIEW exploded_events AS\nSELECT *, explode(items) AS item\nFROM parsed_events\n</code></pre> <p><pre><code>SELECT * FROM exploded_events WHERE SIZE(items) &gt; 2\n</code></pre> Each element of the items column which is in json format is now in a separate row. </p> <p>In Python,</p> <pre><code>from pyspark.sql.functions import explode, size\n\nexploded_eventsDF = (parsed_eventsDF\n    .withColumn(\"item\", explode(\"items\"))\n)\n\ndisplay(exploded_eventsDF.where(size(\"items\") &gt; 2))\n</code></pre>"},{"location":"databricks/#complex-array-manipulation-functions","title":"Complex Array Manipulation Functions","text":"<p><code>collect_set</code> collects unique values for a field including those within arrays also.</p> <p><code>flatten()</code> combines various values from multiple arrays in a single array.</p> <p><code>array_distinct()</code> removes duplicate values from the array.</p> <p>Task: Pull out cart history details from the events table</p> <p>Step 1 : Collect all event names from the table for each user id</p> <p><pre><code>SELECT user_id, collect_set(event_name) AS event_history\nFROM exploded_events\nGROUP BY user_id\n</code></pre> </p> <p>Step 2 : Explode event_hiistory</p> <p><pre><code>SELECT user_id, explode(collect_set(event_name)) AS event_history\nFROM exploded_events\nGROUP BY user_id\n</code></pre> </p> <p>Step 3 : Collect all item ids by fetching them from the items json column</p> <p><pre><code>SELECT user_id,\n  collect_set(event_name) AS event_history,\n  collect_set(items.item_id) AS cart_history\nFROM exploded_events\nGROUP BY user_id\n</code></pre> </p> <p>Step 4 : Flatten the above cart_history results</p> <p><pre><code>SELECT user_id,\n  collect_set(event_name) AS event_history,\n  flatten(collect_set(items.item_id)) AS cart_history\nFROM exploded_events\nGROUP BY user_id\n</code></pre> </p> <pre><code>SELECT user_id,\n       collect_set(event_name) AS event_history,\n       array_distince(flatten(collect_set(items.item_id))) AS cart_history\nFROM exploded_events\nGROUP BY user_id\n</code></pre>"},{"location":"databricks/#sql-udf-functions","title":"SQL UDF Functions","text":"<p>User Defined Functions (UDFs) in Spark SQL allow you to register custom SQL logic as functions in a database, making these methods reusable anywhere SQL can be run on Databricks. These functions are registered natively in SQL and maintain all of the optimizations of Spark when applying custom logic to large datasets.</p> <p>At minimum, creating a SQL UDF requires a function name, optional parameters, the type to be returned, and some custom logic.</p> <p>Below, a simple function named <code>sale_announcement</code> takes an <code>item_name</code> and <code>item_price</code> as parameters. It returns a string that announces a sale for an item at 80% of its original price.</p> <pre><code>CREATE OR REPLACE FUNCTION sales_announcement(item_name STRING,item_price INT)\nRETURN STRING\nRETURN concat(\"The \",item_name,\"is on sale for $\",round(item_price*0.8,0))\n</code></pre> <p>This function is applied to all the columns at once.</p> <p>Here is a Jupyter notebook with all the common SQL UDF Functions.</p> <ul> <li>Persist between execution environments (which can include notebooks, DBSQL queries, and jobs).</li> <li>Exist as objects in the metastore and are governed by the same Table ACLs as databases, tables, or views.</li> <li>To create a SQL UDF, you need <code>USE CATALOG</code> on the catalog, and <code>USE SCHEMA</code> and <code>CREATE FUNCTION</code> on the schema.</li> <li>To use a SQL UDF, you need <code>USE CATALOG</code> on the catalog, <code>USE SCHEMA</code> on the schema, and <code>EXECUTE</code> on the function.</li> </ul> <p>We can use <code>DESCRIBE FUNCTION</code> to see where a function was registered and basic information about expected inputs and what is returned (and even more information with <code>DESCRIBE FUNCTION EXTENDED</code>).</p>"},{"location":"databricks/#case-when-statements-in-sql-udf","title":"Case When Statements in SQL UDF","text":"<pre><code>CREATE OR REPLACE FUNCTION item_preference(name STRING, price INT)\nRETURNS STRING\nRETURN CASE \n  WHEN name = \"Standard Queen Mattress\" THEN \"This is my default mattress\"\n  WHEN name = \"Premium Queen Mattress\" THEN \"This is my favorite mattress\"\n  WHEN price &gt; 100 THEN concat(\"I'd wait until the \", name, \" is on sale for $\", round(price * 0.8, 0))\n  ELSE concat(\"I don't need a \", name)\nEND;\n\nSELECT *, item_preference(name, price) FROM item_lookup\n</code></pre>"},{"location":"databricks/#python-udfs","title":"Python UDFs","text":""},{"location":"databricks/#user-defined-function-udf","title":"User-Defined Function (UDF)","text":"<p>A custom column transformation function</p> <ul> <li> <p>Can\u2019t be optimized by Catalyst Optimizer</p> </li> <li> <p>Function is serialized and sent to executors</p> </li> <li> <p>Row data is deserialized from Spark's native binary format to pass to the UDF, and the results are serialized back into Spark's native format</p> </li> <li> <p>For Python UDFs, additional interprocess communication overhead between the executor and a Python interpreter running on each worker node</p> </li> </ul> <p>Define a Function</p> <pre><code>def first_letter_function(email):\n    return email[0]\n</code></pre> <p>Create User Defined Function</p> <ul> <li>First serialize the function and then send it to the executors to be applied to the DataFrame records.</li> </ul> <pre><code>first_letter_udf = udf(first_letter_function)\n</code></pre> <p>Apply the UDF on the email column</p> <p><pre><code>from pyspark.sql.functions import col\ndisplay(sales_df.select(first_letter_udf(col(\"email\"))))\n</code></pre> Register UDF to be used in SQL</p> <pre><code>sales_df.createOrReplaceTempView(\"sales\")\nfirst_letter_udf = spark.udf.register(\"sql_udf\",fist_letter_function)\n</code></pre> <p>Use it in SQL</p> <pre><code>SELECT sql_udf(email) AS first_letter FROM sales\n</code></pre> <p>Using Decorator Syntax</p> <p>Alternatively, you can define and register a UDF using Python decorator syntax. The <code>@udf</code> decorator parameter is the Column datatype the function returns.</p> <pre><code>@udf(\"string\")\ndef first_letter_udf(str):\nreturn email[0]\n</code></pre>"},{"location":"databricks/#normal-python-udfs-vs-pandas-udfs","title":"Normal Python UDFs vs Pandas UDFs","text":"<p>Pandas UDFs are available in Python to improve the efficiency of UDFs. Pandas UDFs utilize Apache Arrow to speed up computation.</p> <ul> <li>Blog post</li> <li>Documentation</li> </ul> <p></p> <p>The user-defined functions are executed using:  * Apache Arrow, an in-memory columnar data format that is used in Spark to efficiently transfer data between JVM and Python processes with near-zero (de)serialization cost * Pandas inside the function, to work with Pandas instances and APIs</p> <p>Normal Python UDF</p> <p><pre><code>from pyspark.sql.functions import udf\n\n# Use udf to define a row-at-a-time udf\n@udf('double')\n# Input/output are both a single double value\ndef plus_one(v):\n      return v + 1\n\ndf.withColumn('v2', plus_one(df.v))\n</code></pre> Pandas UDFs : Row at a time</p> <pre><code>from pyspark.sql.functions import pandas_udf, PandasUDFType\n@pandas_udf('double', PandasUDFType.SCALAR)\ndef pandas_plus_one(v):\n    return v + 1\ndf.withColumn('v2', pandas_plus_one(df.v))\n</code></pre> <p>In the row-at-a-time version, the user-defined function takes a double \"v\" and returns the result of \"v + 1\" as a double. In the Pandas version, the user-defined function takes a  <code>pandas.Series</code>  \"v\" and returns the result of \"v + 1\" as a  <code>pandas.Series</code>. Because \"v + 1\" is vectorized on  <code>pandas.Series</code>, the Pandas version is much faster than the row-at-a-time version.</p> <p>Pandas Vectorized UDF</p> <pre><code>import pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n\n# We have a string input/output\n@pandas_udf(\"string\")\ndef vectorized_udf(email: pd.Series) -&gt; pd.Series:\n    return email.str[0]\n</code></pre> <p>Registering UDF for usage in SQL Namespace</p> <pre><code>spark.udf.register(\"sql_vectorized_udf\", vectorized_udf)\n</code></pre> <p>Using UDF in SQL Statement</p> <pre><code>SELECT sql_vectorized_udf(email) AS firstLetter FROM sales\n</code></pre>"},{"location":"databricks/#managing-data-with-delta-lake","title":"Managing Data with Delta Lake","text":"<p>Delta Lake enables building a data lakehouse on top of the existing cloud storage. Its not a database service or data warehouse. It's built for scalable metadata handling. Delta Lake brings ACID transaction guarantees to object storage.</p> <p></p> <p></p> <p></p> <p>What is ACID? </p>"},{"location":"databricks/#problems-solved-by-acid","title":"Problems Solved by ACID","text":"<ul> <li>Hard to append data</li> <li>Modification of existing data is difficult</li> <li>Jobs fail mid way</li> <li>Costly to keep historical data versions.</li> </ul> <p>Its the default format to create tables in Databricks</p>"},{"location":"databricks/#schemas-and-tables","title":"Schemas and Tables","text":"<p>Creating Schema in the default directory <code>dbfs:/user/hive/warehouse</code></p> <pre><code>CREATE SCHEMA IF NOT EXISTS ${da.schema_name}_default_location;\n</code></pre> <p>Creating Schema in a custom location </p> <pre><code>CREATE SCHEMA IF NOT EXISTS ${da.schema_name}_custom_location LOCATION '${da.paths.working_dir}/${da.schema_name}_custom_location.db'\n</code></pre>"},{"location":"databricks/#creating-managed-tables","title":"Creating Managed Tables","text":"<p>We dont need to mention the location of the tables.</p> <pre><code>USE ${da.schema_name}_default_location;\n\nCREATE OR REPLACE TABLE managed_table (width INT, length INT, height INT);\nINSERT INTO managed_table \nVALUES (3, 2, 1);\nSELECT * FROM managed_table;\n</code></pre> <p>To find the location of the managed table we can use the <code>DESCRIBE DETAIL managed_table</code> command. Output is <code>dbfs:/user/hive/warehouse/vedanthvbaliga_gnc9_da_delp_default_location.db/managed_table</code></p> <p>The default format of the table is delta.</p> <p>If we drop the managed table, only the schema will be there, the table and data will be deleted.</p> <p>Checking if the schema still exists</p> <pre><code>schema_default_location = spark.sql(f\"DESCRIBE SCHEMA {DA.schema_name}_default_location\").collect()[3].database_description_value\nprint(schema_default_location)\ndbutils.fs.ls(schema_default_location)\n</code></pre> <p>Output : dbfs:/user/hive/warehouse/vedanthvbaliga_gnc9_da_delp_default_location.db</p>"},{"location":"databricks/#creating-external-tables","title":"\u26a0\ufe0f Creating External Tables","text":"<p><pre><code>USE ${da.schema_name}_default_location;\n\nCREATE OR REPLACE TEMPORARY VIEW temp_delays \nUSING CSV OPTIONS (\n    path = \"${da.paths.datasets}/flights/delay_departures.csv\",\n    header = \"true\",\n    mode = \"FAILFAST\"\n);\n\nCREATE OR REPLACE EXTERNAL TABLE external_table LOCATION '${da.path.working_dir}/external_table' AS \n    SELECT * FROM temp_delays;\n\nSELECT * FROM external_table\n</code></pre> </p> <p>Dropping the external table deletes the table definition but the data is still there.</p> <pre><code>tbl_path = f\"{DA.paths.working_dir}/external_table\"\nfiles = dbutils.fs.ls(tbl_path)\ndisplay(files)\n</code></pre> <p></p> <p>To drop external table schema use :</p> <pre><code>DROP SCHEMA {da.schema_name}_custom_location CASCADE;\n</code></pre> <p>If the schema is managed by the workspace-level Hive metastore, dropping a schema using CASCADE recursively deletes all files in the specified location, regardless of the table type (managed or external).</p>"},{"location":"databricks/#setting-up-delta-tables","title":"Setting Up Delta Tables","text":"<p>After extracting data from external data sources, load data into the Lakehouse to ensure that all of the benefits of the Databricks platform can be fully leveraged.</p> <p>While different organizations may have varying policies for how data is initially loaded into Databricks, we typically recommend that early tables represent a mostly raw version of the data, and that validation and enrichment occur in later stages. </p> <p>This pattern ensures that even if data doesn't match expectations with regards to data types or column names, no data will be dropped, meaning that programmatic or manual intervention can still salvage data in a partially corrupted or invalid state.</p>"},{"location":"databricks/#ctas-statements","title":"CTAS Statements","text":"<p>Used to populate the delta tables using data from an input query</p> <pre><code>CREATE OR REPLACE TABLE sales AS\nSELECT * FROM parquet.`${DA.paths.datasets}/ecommerce/raw/sales-historical`;\n\nDESCRIBE EXTENDED sales;\n</code></pre> <p>Note</p> <p>CTAS statements automatically infer schema information from query results and do not support manual schema declaration. </p> <p>This means that CTAS statements are useful for external data ingestion from sources with well-defined schema, such as Parquet files and tables.</p> <p>CTAS statements also do not support specifying additional file options.</p>"},{"location":"databricks/#ingesting-csv-with-ctas","title":"Ingesting csv with CTAS","text":"<pre><code>CREATE OR REPLACE TABLE sales_unparsed AS\nSELECT * FROM csv.`${da.paths.datasets}/ecommerce/raw/sales-csv`;\n\nSELECT * FROM sales_unparsed;\n</code></pre> <p>Output is as follows: </p> <p>To fix this we use a reference to the files that allows us to specify the options.</p> <p>We will specify options to a temp view and then use this as a source for a CTAS statement to register the Delta Table</p> <pre><code>CREATE OR REPLACE TEMP VIEW sales_tmp_vw\n  (order_id LONG, email STRING, transactions_timestamp LONG, total_item_quantity INTEGER, purchase_revenue_in_usd DOUBLE, unique_items INTEGER, items STRING)\nUSING CSV\nOPTIONS (\n  path = \"${da.paths.datasets}/ecommerce/raw/sales-csv\",\n  header = \"true\",\n  delimiter = \"|\"\n);\n\nCREATE TABLE sales_delta AS\n  SELECT * FROM sales_tmp_vw;\n\nSELECT * FROM sales_delta\n</code></pre> <p></p>"},{"location":"databricks/#filtering-and-renaming-columns-from-existing-tables","title":"Filtering and Renaming columns from existing tables","text":"<pre><code>CREATE OR REPLACE TABLE purchases AS\nSELECT order_id AS id, transaction_timestamp, purchase_revenue_in_usd AS price\nFROM sales;\n\nSELECT * FROM purchases\n</code></pre>"},{"location":"databricks/#declare-schema-with-generated-columns","title":"Declare Schema with Generated Columns","text":"<p> As noted previously, CTAS statements do not support schema declaration. We note above that the timestamp column appears to be some variant of a Unix timestamp, which may not be the most useful for our analysts to derive insights. This is a situation where generated columns would be beneficial.</p> <p>Generated columns are a special type of column whose values are automatically generated based on a user-specified function over other columns in the Delta table.</p> <pre><code>CREATE OR REPLACE TABLE purchase_dates (\n  id STRING, \n  transaction_timestamp STRING, \n  price STRING,\n  date DATE GENERATED ALWAYS AS (\n    cast(cast(transaction_timestamp/1e6 AS TIMESTAMP) AS DATE))\n    COMMENT \"generated based on `transactions_timestamp` column\")\n</code></pre>"},{"location":"databricks/#mergin-data","title":"Mergin Data","text":"<p>Check how many records are in purchase_dates?</p> <p><pre><code>SELECT * FROM purchase_dates;\n</code></pre> There are no records in the table.</p> <p>Check how many records are in purchases?</p> <p><pre><code>SELECT COUNT(*) FROM purchases;\n</code></pre> There are 10,510 records.</p> <pre><code>SET spark.databricks.delta.schema.autoMerge.enabled=true; \n\nMERGE INTO purchase_dates a\nUSING purchases b\nON a.id = b.id\nWHEN NOT MATCHED THEN\n  INSERT *\n</code></pre> <p>The SET command ensures that autoMerge is enabled we dont need to <code>REFRESH</code> after merging into the purchase_dates table.</p> <p>It's important to note that if a field that would otherwise be generated is included in an insert to a table, this insert will fail if the value provided does not exactly match the value that would be derived by the logic used to define the generated column.</p>"},{"location":"databricks/#adding-constraints","title":"Adding Constraints","text":"<p>CHECK constraint</p> <p><pre><code>ALTER TABLE purchase_dates ADD CONSTRAINT valid_date CHECK (date &gt; '2020-01-01');\n</code></pre> </p>"},{"location":"databricks/#additional-options-and-metadata","title":"Additional Options and Metadata","text":"<p>Our <code>SELECT</code> clause leverages two built-in Spark SQL commands useful for file ingestion: * <code>current_timestamp()</code> records the timestamp when the logic is executed * <code>input_file_name()</code> records the source data file for each record in the table</p> <p>We also include logic to create a new date column derived from timestamp data in the source.</p> <p>The <code>CREATE TABLE</code> clause contains several options: * A <code>COMMENT</code> is added to allow for easier discovery of table contents * A <code>LOCATION</code> is specified, which will result in an external (rather than managed) table * The table is <code>PARTITIONED BY</code> a date column; this means that the data from each data will exist within its own directory in the target storage location.</p> <pre><code>CREATE OR REPLACE TABLE users_pii\nCOMMENT \"Contains PII\"\nLOCATION \"${da.paths.working_dir}/tmp/users_pii\"\nPARTITIONED BY (first_touch_date)\nAS\n  SELECT *, \n    cast(cast(user_first_touch_timestamp/1e6 AS TIMESTAMP) AS DATE) first_touch_date, \n    current_timestamp() updated,\n    input_file_name() source_file\n  FROM parquet.`${da.paths.datasets}/ecommerce/raw/users-historical/`;\n\nSELECT * FROM users_pii;\n</code></pre> <p></p> <p>Listing all the files</p> <pre><code>files = dbutils.fs.ls(f\"{DA.paths.working_dir}/tmp/users_pii\")\ndisplay(files)\n</code></pre> <p></p>"},{"location":"databricks/#cloning-delta-lake-tables","title":"Cloning Delta Lake Tables","text":"<p>Delta Lake has two options for efficiently copying Delta Lake tables.</p> <p><code>DEEP CLONE</code> fully copies data and metadata from a source table to a target. This copy occurs incrementally, so executing this command again can sync changes from the source to the target location.</p> <pre><code>CREATE OR REPLACE TABLE purchases_clone\nDEEP CLONE purchases\n</code></pre> <p>If you wish to create a copy of a table quickly to test out applying changes without the risk of modifying the current table, <code>SHALLOW CLONE</code> can be a good option. Shallow clones just copy the Delta transaction logs, meaning that the data doesn't move.</p> <pre><code>CREATE OR REPLACE TABLE purchases_shallow_clone\nSHALLOW CLONE purchases\n</code></pre>"},{"location":"databricks/#loading-data-into-tables","title":"Loading Data Into Tables","text":""},{"location":"databricks/#complete-overwrites","title":"Complete Overwrites","text":"<p>We can use overwrites to atomically replace all of the data in a table. There are multiple benefits to overwriting tables instead of deleting and recreating tables:</p> <ul> <li> <p>Overwriting a table is much faster because it doesn\u2019t need to list the directory recursively or delete any files.</p> </li> <li> <p>The old version of the table still exists; can easily retrieve the old data using Time Travel.</p> </li> <li> <p>It\u2019s an atomic operation. Concurrent queries can still read the table while you are deleting the table.</p> </li> <li> <p>Due to ACID transaction guarantees, if overwriting the table fails, the table will be in its previous state.</p> </li> </ul> <p>Spark SQL provides two easy methods to accomplish complete overwrites.</p> <pre><code>CREATE OR REPLACE TABLE events AS\nSELECT * FROM parquet.`${da.paths.datasets}/ecommerce/raw/events-historical`\n</code></pre> <p>Reviewing the Table History</p> <p><pre><code>DESCRIBE HISTORY events\n</code></pre> </p>"},{"location":"databricks/#insert-overwrite","title":"Insert Overwrite","text":"<p><code>INSERT OVERWRITE</code> provides a nearly identical outcome as above: data in the target table will be replaced by data from the query. </p> <ul> <li> <p>Can only overwrite an existing table, not create a new one like our CRAS statement.</p> </li> <li> <p>Can overwrite only with new records that match the current table schema -- and thus can be a \"safer\" technique for overwriting an existing table without disrupting downstream consumers.</p> </li> <li> <p>Can overwrite individual partitions.</p> </li> </ul> <p>Metrics that are defined during Insert Overwrite on running <code>DESCRIBE HISTORY SALES</code> is different.</p> <p>Whereas a CRAS statement will allow us to completely redefine the contents of our target table, <code>INSERT OVERWRITE</code> will fail if we try to change our schema (unless we provide optional settings). </p> <p>Uncomment and run the cell below to generate an expected error message.</p> <p>This gives an error</p> <pre><code>INSERT OVERWRITE sales\nSELECT *, current_timestamp() FROM parquet.`${da.paths.datasets}/ecommerce/raw/sales-historical`\n</code></pre>"},{"location":"databricks/#appending-data","title":"Appending Data","text":"<p>We can use <code>INSERT INTO</code> to atomically append new rows to an existing Delta table. This allows for incremental updates to existing tables, which is much more efficient than overwriting each time.</p> <p>Append new sale records to the <code>sales</code> table using <code>INSERT INTO</code></p> <p>Note that <code>INSERT INTO</code> does not have any built-in guarantees to prevent inserting the same records multiple times. Re-executing the above cell would write the same records to the target table, resulting in duplicate records.</p>"},{"location":"databricks/#merging-updates","title":"Merging Updates","text":"<p><code> MERGE INTO target a USING source b ON {merge_condition} WHEN MATCHED THEN {matched_action} WHEN NOT MATCHED THEN {not_matched_action} </code></p> <p>We will use the <code>MERGE</code> operation to update historic users data with updated emails and new users.</p> <p>Step 1 : Check <code>users_30m</code> parquet</p> <pre><code>SELECT * FROM PARQUET.`${da.paths.datasets}/ecommerce/raw/users-30m\n</code></pre> <p></p> <p>Step 2 : Create view <code>users_update</code> and add data from <code>users_30m</code> dataset</p> <pre><code>CREATE OR REPLACE TEMP VIEW users_update AS \nSELECT *, current_timestamp() AS updated \nFROM parquet.`${da.paths.datasets}/ecommerce/raw/users-30m`\n</code></pre> <p>Step 3 : Check <code>users</code> and <code>users_updated</code> dataset</p> <p><pre><code>SELECT * FROM users;\n</code></pre> </p> <p><pre><code>SELECT * FROM users_update;\n</code></pre> </p> <p>Step 4 : If the email in <code>users</code> is null and in <code>users_update</code> is not null then set email in users to <code>users.email</code> and <code>users.updated</code> to <code>users_updated.updated</code> , else insert whatever record is in users_update.</p> <pre><code>MERGE INTO users a\nUSING users_update b\nON a.user_id = b.user_id\nWHEN MATCHED AND a.email IS NULL AND b.email IS NOT NULL THEN\n  UPDATE SET email = b.email, updated = b.updated\nWHEN NOT MATCHED THEN INSERT *\n</code></pre> <p></p>"},{"location":"databricks/#insert-only-merge-for-data-deduplication","title":"Insert-Only Merge For Data Deduplication \u26a0\ufe0f","text":"<p>A common ETL use case is to collect logs or other every-appending datasets into a Delta table through a series of append operations. </p> <p>Many source systems can generate duplicate records. With merge, you can avoid inserting the duplicate records by performing an insert-only merge.</p> <p>This optimized command uses the same <code>MERGE</code> syntax but only provided a <code>WHEN NOT MATCHED</code> clause.</p> <p>Below, we use this to confirm that records with the same <code>user_id</code> and <code>event_timestamp</code> aren't already in the <code>events</code> table.</p> <pre><code>MERGE INTO events a\nUSING events_update b\nON a.user_id = b.user_id AND a.event_timestamp = b.event_timestamp\nWHEN NOT MATCHED AND b.traffic_source = 'email' THEN \n  INSERT *\n</code></pre> <p>Logs Example</p> <pre><code>MERGE INTO logs\nUSING newDedupedLogs\nON logs.uniqueId = newDedupedLogs.uniqueId\nWHEN NOT MATCHED\n  THEN INSERT *\n</code></pre> <p>The dataset containing the new logs needs to be deduplicated within itself. By the SQL semantics of merge, it matches and deduplicates the new data with the existing data in the table, but if there is duplicate data within the new dataset, it is inserted. Hence, deduplicate the new data before merging into the table.</p> <p>If you know that you may get duplicate records only for a few days, you can optimize your query further by partitioning the table by date, and then specifying the date range of the target table.</p> <pre><code>MERGE INTO logs\nUSING newDedupedLogs\nON logs.uniqueId = newDedupedLogs.uniqueId AND logs.date &gt; current_date() - INTERVAL 7 DAYS\nWHEN NOT MATCHED AND newDedupedLogs.date &gt; current_date() - INTERVAL 7 DAYS\n  THEN INSERT *\n</code></pre>"},{"location":"databricks/#incremental-loading","title":"Incremental Loading","text":"<p><code>COPY INTO</code> provides SQL engineers an idempotent option to incrementally ingest data from external systems.</p> <p>Note that this operation does have some expectations: - Data schema should be consistent - Duplicate records should try to be excluded or handled downstream</p> <p>This operation is potentially much cheaper than full table scans for data that grows predictably.</p> <pre><code>COPY INTO sales\nFROM \"${da.paths.datasets}/ecommerce/raw/sales-30m\"\nFILEFORMAT = PARQUET\n</code></pre>"},{"location":"databricks/#versioning-optimizing-and-vacuuming","title":"Versioning, Optimizing and Vacuuming","text":"<p>Create an example table with operations</p> <pre><code>CREATE TABLE students\n  (id INT, name STRING, value DOUBLE);\n\nINSERT INTO students VALUES (1, \"Yve\", 1.0);\nINSERT INTO students VALUES (2, \"Omar\", 2.5);\nINSERT INTO students VALUES (3, \"Elia\", 3.3);\n\nINSERT INTO students\nVALUES \n  (4, \"Ted\", 4.7),\n  (5, \"Tiffany\", 5.5),\n  (6, \"Vini\", 6.3);\n\nUPDATE students \nSET value = value + 1\nWHERE name LIKE \"T%\";\n\nDELETE FROM students \nWHERE value &gt; 6;\n\nCREATE OR REPLACE TEMP VIEW updates(id, name, value, type) AS VALUES\n  (2, \"Omar\", 15.2, \"update\"),\n  (3, \"\", null, \"delete\"),\n  (7, \"Blue\", 7.7, \"insert\"),\n  (11, \"Diya\", 8.8, \"update\");\n\nMERGE INTO students b\nUSING updates u\nON b.id=u.id\nWHEN MATCHED AND u.type = \"update\"\n  THEN UPDATE SET *\nWHEN MATCHED AND u.type = \"delete\"\n  THEN DELETE\nWHEN NOT MATCHED AND u.type = \"insert\"\n  THEN INSERT *;\n</code></pre> <p>This table gets stored in <code>dbfs:/user/hive/warehouse/students</code></p> <p>The table is not a relational entity but a set of files stored in the cloud object storage.</p> <p><pre><code>display(dbutils.fs.ls(f\"{DA.paths.user_db}/students\"))\n</code></pre> </p> <p>There is a directory called <code>_delta_log</code> where transactions on the Delta Lake Tables are stored</p> <p><pre><code>display(dbutils.fs.ls(f\"{DA.paths.user_db}/students/_delta_log\"))\n</code></pre> There are a total of 8 transaction logs in json format </p> <p>For large datasets we would have more parquet files. We can see that there are 4 files currently in students. </p> <p>So what are the other files present for?</p> <p>Rather than overwriting or immediately deleting files containing changed data, Delta Lake uses the transaction log to indicate whether or not files are valid in a current version of the table.</p> <p>Here, we'll look at the transaction log corresponding the <code>MERGE</code> statement above, where records were inserted, updated, and deleted.</p> <pre><code>display(spark.sql(f\"SELECT * FROM json.`{DA.paths.user_db}/students/_delta_log/00000000000000000007.json`\"))\n</code></pre> <p></p> <p>The <code>add</code> column contains a list of all the new files written to our table; the <code>remove</code> column indicates those files that no longer should be included in our table.</p> <p>When we query a Delta Lake table, the query engine uses the transaction logs to resolve all the files that are valid in the current version, and ignores all other data files.</p>"},{"location":"databricks/#optimizing-and-indexing","title":"Optimizing and Indexing","text":"<p>When we use large datasets, we may run into problems of a large number of files.</p> <p>Here since we did many operations that only changed/modified a small number of rows, there were more number of files.</p> <p>Files will be combined toward an optimal size (scaled based on the size of the table) by using the <code>OPTIMIZE</code> command.</p> <p><code>OPTIMIZE</code> will replace existing data files by combining records and rewriting the results.</p> <p>When executing <code>OPTIMIZE</code>, users can optionally specify one or several fields for <code>ZORDER</code> indexing. While the specific math of Z-order is unimportant, it speeds up data retrieval when filtering on provided fields by colocating data with similar values within data files.</p> <pre><code>OPTIMIZE students\nZORDER BY id\n</code></pre> <p>By looking at the output we can motice that 1 file was added and 4 were removed.  </p> <p>As expected, <code>OPTIMIZE</code> created another version of our table, meaning that version 8 is our most current version.</p> <p>Remember all of those extra data files that had been marked as removed in our transaction log? These provide us with the ability to query previous versions of our table.</p> <p>These time travel queries can be performed by specifying either the integer version or a timestamp.</p> <p>NOTE: In most cases, you'll use a timestamp to recreate data at a time of interest. For our demo we'll use version, as this is deterministic (whereas you may be running this demo at any time in the future).</p> <p>Going back to a previous state</p> <pre><code>SELECT * \nFROM students VERSION AS OF 3\n</code></pre> <p>What's important to note about time travel is that we're not recreating a previous state of the table by undoing transactions against our current version; rather, we're just querying all those data files that were indicated as valid as of the specified version.</p>"},{"location":"databricks/#rollback-to-previous-version","title":"Rollback to Previous Version","text":"<p>Suppose we are typing a query to manually delete some records from the table and by mistake delete the entire table. We can rollback to the previous version by rolling back the commit.</p> <pre><code>RESTORE TABLE students TO VERSION AS OF 8 \n</code></pre>"},{"location":"databricks/#cleaning-up-stale-files-and-vacuum","title":"Cleaning Up Stale Files and Vacuum","text":"<p>Databricks will automatically clean up stale log files (&gt; 30 days by default) in Delta Lake tables. Each time a checkpoint is written, Databricks automatically cleans up log entries older than this retention interval.</p> <p>While Delta Lake versioning and time travel are great for querying recent versions and rolling back queries, keeping the data files for all versions of large production tables around indefinitely is very expensive (and can lead to compliance issues if PII is present).</p> <p>If you wish to manually purge old data files, this can be performed with the <code>VACUUM</code> operation.</p> <p>Uncomment the following cell and execute it with a retention of <code>0 HOURS</code> to keep only the current version:</p> <p>By default, <code>VACUUM</code> will prevent you from deleting files less than 7 days old, just to ensure that no long-running operations are still referencing any of the files to be deleted. If you run <code>VACUUM</code> on a Delta table, you lose the ability time travel back to a version older than the specified data retention period.  In our demos, you may see Databricks executing code that specifies a retention of <code>0 HOURS</code>. This is simply to demonstrate the feature and is not typically done in production.  </p> <p>In the following cell, we: 1. Turn off a check to prevent premature deletion of data files 2. Make sure that logging of <code>VACUUM</code> commands is enabled 3. Use the <code>DRY RUN</code> version of vacuum to print out all records to be deleted</p> <p>To disable the retention duration of 0 safety mechanism just enable these parameters to false and true.</p> <pre><code>SET spark.databricks.delta.retentionDurationCheck.enabled = false;\nSET spark.databricks.delta.vacuum.logging.enabled = true;\n</code></pre> <pre><code>VACUUM students RETAIN 0 HOURS DRY RUN\n</code></pre> <p>By vacuuming the files, we are permanantly deleting the versions of the files and we cannot get it back.</p> <p>After deletion, only the delta file with log of transactions remains.</p>"},{"location":"databricks/#data-pipelines-with-delta-live-tables","title":"Data Pipelines with Delta Live Tables","text":""},{"location":"databricks/#the-medallion-architecture","title":"The Medallion Architecture","text":""},{"location":"databricks/#the-bronze-layer","title":"The Bronze Layer","text":""},{"location":"databricks/#the-silver-layer","title":"The Silver Layer","text":""},{"location":"databricks/#the-gold-layer","title":"The Gold Layer","text":""},{"location":"databricks/#the-multi-hop-architecture","title":"The Multi Hop Architecture","text":""},{"location":"databricks/#how-dlt-solves-problems","title":"How DLT Solves Problems","text":"<p>Usually the bronze, silver and gold layers will not be in a linear dependency format. </p>"},{"location":"databricks/#what-exactly-is-a-live-table","title":"What exactly is a live table?","text":"<p>Streaming Live Tables </p>"},{"location":"databricks/#steps-to-create-a-dlt-pipeline","title":"Steps to Create a DLT Pipeline","text":""},{"location":"databricks/#development-vs-production-pipelines","title":"Development Vs Production pipelines","text":"<p>We use job clusters in prod pipelines.</p> <p>Hence if the pipeline in the prod needs to be run multiple times, then the cluster object has to be created multiple times.</p> <p>But in the case of dev pipeines, we can keep the clusters running for faster debugging.</p>"},{"location":"databricks/#dependencies-in-the-pipeline","title":"Dependencies in the Pipeline","text":"<p>All the tables in the pipeline have the same LIVE schema, so we need to mention the keyword <code>LIVE.events</code></p> <p>This feature allows us to migrate the pipelines between databases in the environment.</p> <p>When we are moving from dev to prod, then just change the schema from dev to prod and we can migrate very quickly.</p>"},{"location":"databricks/#data-quality-with-expectations","title":"Data Quality with Expectations","text":""},{"location":"databricks/#why-event-logs-are-important","title":"Why Event Logs are Important","text":""},{"location":"databricks/#spark-structured-streaming-ingest-from-cloud","title":"Spark Structured Streaming [Ingest From Cloud]","text":""},{"location":"databricks/#streaming-from-an-existing-table","title":"Streaming from an existing table","text":"<p> Usally the table that we are streaming from has data coming in from Kafka/Kinesis.</p>"},{"location":"databricks/#parameters-in-dlt","title":"Parameters in DLT","text":""},{"location":"databricks/#change-data-capture","title":"Change Data Capture","text":"<p> Here the source is <code>city_updates</code> and it must be a stream.</p> <p>We need unique key like id that can idenitify the data that can be included in teh updates A sequence no is required to apply changes in the current order.</p> <p>Example  Initially cities table is empty, here we can see that berkley was misspelled in the first entry of city_updates table, so when we fix it by keeping the same id and different timestamp its updated in the cities table also.</p>"},{"location":"databricks/#what-does-dlt-automate","title":"What does DLT automate?","text":""},{"location":"databricks/#creating-pipelines","title":"Creating Pipelines","text":"<ol> <li> <p>Setup the parameters like in the Delta Live Tables UI Notebook.</p> </li> <li> <p>Then click '+' -&gt; New DLT Pipeline.</p> </li> <li> <p>Create the pipeline using the steps mentioned here</p> </li> <li> <p>This is the final pipeline config link</p> </li> <li> <p>This is the final dashboard </p> </li> </ol> <p>In prod mode we delete the cluster resources after the pipeline completes.</p> <p>I cannot run the pipelines due to restrictions in student account. </p> <p>Here is the snapshot of the running pipeline from the course. </p>"},{"location":"databricks/#fundamental-dlt-sql-syntax","title":"Fundamental DLT SQL Syntax","text":"<p>This notebook demonstrates using Delta Live Tables (DLT) to process raw data from JSON files landing in cloud object storage through a series of tables to drive analytic workloads in the lakehouse. Here we demonstrate a medallion architecture, where data is incrementally transformed and enriched as it flows through a pipeline. This notebook focuses on the SQL syntax of DLT rather than this architecture, but a brief overview of the design:</p> <ul> <li>The bronze table contains raw records loaded from JSON enriched with data describing how records were ingested</li> <li>The silver table validates and enriches the fields of interest</li> <li>The gold table contains aggregate data to drive business insights and dashboarding</li> </ul> <p>DLT syntax is not intended for interactive execution in a notebook. This notebook will need to be scheduled as part of a DLT pipeline for proper execution. </p> <p>If you do execute a DLT notebook cell interactively, you should see a message that your statement is syntactically valid. Note that while some syntax checks are performed before returning this message, it is not a guarantee that your query will perform as desired. We'll discuss developing and troubleshooting DLT code later in the course.</p> <p>Delta Live Tables adapts standard SQL queries to combine DDL (data definition language) and DML (data manipulation language) into a unified declarative syntax.</p>"},{"location":"databricks/#table-as-query-results","title":"Table as Query Results","text":"<p>There are two distinct types of persistent tables that can be created with DLT: * Live tables are materialized views for the lakehouse; they will return the current results of any query with each refresh * Streaming live tables are designed for incremental, near-real time data processing</p> <p>Note that both of these objects are persisted as tables stored with the Delta Lake protocol (providing ACID transactions, versioning, and many other benefits). We'll talk more about the differences between live tables and streaming live tables later in the notebook.</p>"},{"location":"databricks/#auto-loader","title":"Auto Loader","text":"<p>Databricks has developed the Auto Loader functionality to provide optimized execution for incrementally loading data from cloud object storage into Delta Lake. Using Auto Loader with DLT is simple: just configure a source data directory, provide a few configuration settings, and write a query against your source data. </p> <p>Auto Loader will automatically detect new data files as they land in the source cloud object storage location, incrementally processing new records without the need to perform expensive scans and recomputing results for infinitely growing datasets.</p> <p>The <code>cloud_files()</code> method enables Auto Loader to be used natively with SQL. This method takes the following positional parameters:</p> <ul> <li>The source location, which should be cloud-based object storage</li> <li>The source data format, which is JSON in this case</li> <li>An arbitrarily sized comma-separated list of optional reader options. In this case, we set <code>cloudFiles.inferColumnTypes</code> to <code>true</code></li> </ul> <p>In the query below, in addition to the fields contained in the source, Spark SQL functions for the <code>current_timestamp()</code> and <code>input_file_name()</code> as used to capture information about when the record was ingested and the specific file source for each record.</p> <pre><code>CREATE OR REFRESH STREAMING LIVE TABLE orders_bronze\nAS SELECT current_timestamp() processing_time, input_file_name() source_file, *\nFROM cloud_files(\"${source}/orders\", \"json\", map(\"cloudFiles.inferColumnTypes\", \"true\"))\n</code></pre>"},{"location":"databricks/#validating-and-enriching-the-data","title":"Validating and Enriching the Data","text":"<p>The select statement contains the core logic of your query. In this example, we: * Cast the field <code>order_timestamp</code> to the timestamp type * Select all of the remaining fields (except a list of 3 we're not interested in, including the original <code>order_timestamp</code>)</p> <p>Note that the <code>FROM</code> clause has two constructs that you may not be familiar with: * The <code>LIVE</code> keyword is used in place of the schema name to refer to the target schema configured for the current DLT pipeline * The <code>STREAM</code> method allows users to declare a streaming data source for SQL queries</p> <p>Note that if no target schema is declared during pipeline configuration, your tables won't be published (that is, they won't be registered to the metastore and made available for queries elsewhere). </p> <p>The target schema can be easily changed when moving between different execution environments, meaning the same code can easily be deployed against regional workloads or promoted from a dev to prod environment without needing to hard-code schema names.</p> <pre><code>CREATE OR REFRESH STREAMING LIVE TABLE orders_silver\n(CONSTRAINT valid_date EXPECT (order_timestamp &gt; \"2021-01-01\") ON VIOLATION FAIL UPDATE)\nCOMMENT \"Append only orders with valid timestamps\"\nTBLPROPERTIES (\"quality\" = \"silver\")\nAS SELECT timestamp(order_timestamp) AS order_timestamp, * EXCEPT (order_timestamp, source_file, _rescued_data)\nFROM STREAM(LIVE.orders_bronze)\n</code></pre> <p>Here, in the end of the statement, we have <code>LIVE.orders_bronze</code>. We have to specify <code>LIVE.</code> because it refers to the target schema that we defined before in the configuration settings.</p> <p>The table <code>order_silver</code> is a STREAMING table becuase it takes in data from another streaming table <code>orders_bronze</code></p> <p>If the expectation fails, then we can have two main choices <code>UPDATE</code> will drop all the rows that were part of the insertion even if only one row fails the constraint.</p> <p>If we use <code>ROW</code> then it drops only the row that failed the update</p>"},{"location":"databricks/#live-tables-vs-streaming-live-tables","title":"Live Tables vs. Streaming Live Tables \u26a0\ufe0f","text":"<p>Below are some of the differences between these types of tables.</p> <p>Live Tables</p> <ul> <li>Always \"correct\", meaning their contents will match their definition after any update.</li> <li>Return same results as if table had just been defined for first time on all data.</li> <li>Should not be modified by operations external to the DLT Pipeline (you'll either get undefined answers or your change will just be undone).</li> </ul> <p>Streaming Live Tables</p> <ul> <li>Only supports reading from \"append-only\" streaming sources.</li> <li>Only reads each input batch once, no matter what (even if joined dimensions change, or if the query definition changes, etc).</li> <li>Can perform operations on the table outside the managed DLT Pipeline (append data, perform GDPR, etc).</li> </ul> <p>A live table or view always reflects the results of the query that defines it, including when the query defining the table or view is updated, or an input data source is updated. Like a traditional materialized view, a live table or view may be entirely computed when possible to optimize computation resources and time.</p> <p>A streaming live table or view processes data that has been added only since the last pipeline update. Streaming tables and views are stateful; if the defining query changes, new data will be processed based on the new query and existing data is not recomputed.</p>"},{"location":"databricks/#creating-the-gold-layer","title":"Creating The Gold Layer","text":"<pre><code>CREATE OR REFRESH LIVE TABLE orders_by_date\nAS SELECT date(order_timestamp) AS order_date, count(*) AS total_daily_orders\nFROM LIVE.orders_silver\nGROUP BY date(order_timestamp)\n</code></pre>"},{"location":"databricks/#orders-pipeline-in-python","title":"Orders Pipeline in Python","text":""},{"location":"databricks/#importing-the-libraries","title":"Importing the libraries","text":"<pre><code>import dlt\nimport pyspark.sql.functions as F\n\nsource = spark.conf.get(\"source\")\n</code></pre>"},{"location":"databricks/#creating-the-bronze-table","title":"Creating the Bronze Table","text":"<p>Delta Live Tables introduces a number of new Python functions that extend familiar PySpark APIs.</p> <p>At the heart of this design, the decorator <code>@dlt.table</code> is added to any Python function that returns a Spark DataFrame. (NOTE: This includes Koalas DataFrames, but these won't be covered in this course.)</p> <p>If you're used to working with Spark and/or Structured Streaming, you'll recognize the majority of the syntax used in DLT. The big difference is that you'll never see any methods or options for DataFrame writers, as this logic is handled by DLT.</p> <p>As such, the basic form of a DLT table definition will look like:</p> <p><code>@dlt.table</code> <code>def &lt;function-name&gt;():</code> <code>return (&lt;query&gt;)</code></p> <pre><code>@dlt.table\ndef orders_bronze():\n    return (\n        spark.readStream\n            .format(\"cloudFiles\")\n            .option(\"cloudFiles.format\", \"json\")\n            .option(\"cloudFiles.inferColumnTypes\", True)\n            .load(f\"{source}/orders\")\n            .select(\n                F.current_timestamp().alias(\"processing_time\"), \n                F.input_file_name().alias(\"source_file\"), \n                \"*\"\n            )\n    )\n</code></pre>"},{"location":"databricks/#creating-the-silver-table","title":"Creating the Silver Table","text":"<pre><code>@dlt.table(\n    comment = \"Append only orders with valid timestamps\",\n    table_properties = {\"quality\": \"silver\"})\n@dlt.expect_or_fail(\"valid_date\", F.col(\"order_timestamp\") &gt; \"2021-01-01\")\ndef orders_silver():\n    return (\n        dlt.read_stream(\"orders_bronze\")\n            .select(\n                \"processing_time\",\n                \"customer_id\",\n                \"notifications\",\n                \"order_id\",\n                F.col(\"order_timestamp\").cast(\"timestamp\").alias(\"order_timestamp\")\n            )\n    )\n</code></pre>"},{"location":"databricks/#defining-the-gold-table","title":"Defining the Gold Table","text":"<pre><code>@dlt.table\ndef orders_by_date():\n    return (\n        dlt.read(\"orders_silver\")\n            .groupBy(F.col(\"order_timestamp\").cast(\"date\").alias(\"order_date\"))\n            .agg(F.count(\"*\").alias(\"total_daily_orders\"))\n    )\n</code></pre>"},{"location":"databricks/#customers-pipeline","title":"Customers Pipeline","text":""},{"location":"databricks/#objectives","title":"Objectives","text":"<ul> <li>Raw records represent change data capture (CDC) information about customers </li> <li>The bronze table again uses Auto Loader to ingest JSON data from cloud object storage</li> <li>A table is defined to enforce constraints before passing records to the silver layer</li> <li><code>APPLY CHANGES INTO</code> is used to automatically process CDC data into the silver layer as a Type 1 slowly changing dimension (SCD) table <li>A gold table is defined to calculate an aggregate from the current version of this Type 1 table</li> <li>A view is defined that joins with tables defined in another notebook</li>"},{"location":"databricks/#what-are-slowly-changing-dimensions","title":"What are Slowly Changing Dimensions?","text":"<p>A slowly changing dimension (SCD) in data management and data warehousing is a dimension which contains relatively static data which can change slowly but unpredictably, rather than according to a regular schedule. Some examples of typical slowly changing dimensions are entities such as names of geographical locations, customers, or products.</p>"},{"location":"databricks/#type-1-scd","title":"Type 1 SCD","text":""},{"location":"databricks/#ingest-data-with-auto-loader","title":"Ingest Data with Auto Loader","text":"<pre><code>CREATE OR REFRESH STREAMING LIVE TABLE customers_bronze\nCOMMENT \"Raw data from customers CDC feed\"\nAS SELECT current_timestamp() processing_time, input_file_name() source_file, *\nFROM cloud_files(\"${source}/customers\", \"json\")\n</code></pre>"},{"location":"databricks/#quality-checks","title":"Quality Checks","text":"<p>The query below demonstrates: * The 3 options for behavior when constraints are violated * A query with multiple constraints * Multiple conditions provided to one constraint * Using a built-in SQL function in a constraint</p> <p>About the data source: * Data is a CDC feed that contains <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> operations.  * Update and insert operations should contain valid entries for all fields. * Delete operations should contain <code>NULL</code> values for all fields other than the timestamp, <code>customer_id</code>, and operation fields.</p> <p>In order to ensure only good data makes it into our silver table, we'll write a series of quality enforcement rules that ignore the expected null values in delete operations.</p> <pre><code>CREATE STREAMING LIVE TABLE customers_bronze_clean\n(CONSTRAINT valid_id EXPECT (customer_id IS NOT NULL) ON VIOLATION FAIL UPDATE,\nCONSTRAINT valid_operation EXPECT (operation IS NOT NULL) ON VIOLATION DROP ROW,\nCONSTRAINT valid_name EXPECT (name IS NOT NULL or operation = \"DELETE\"),\nCONSTRAINT valid_address EXPECT (\n  (address IS NOT NULL and \n  city IS NOT NULL and \n  state IS NOT NULL and \n  zip_code IS NOT NULL) or\n  operation = \"DELETE\"),\nCONSTRAINT valid_email EXPECT (\n  rlike(email, '^([a-zA-Z0-9_\\\\-\\\\.]+)@([a-zA-Z0-9_\\\\-\\\\.]+)\\\\.([a-zA-Z]{2,5})$') or \n  operation = \"DELETE\") ON VIOLATION DROP ROW)\nAS SELECT *\n  FROM STREAM(LIVE.customers_bronze)\n</code></pre>"},{"location":"databricks/#requirements-that-apply-changes-into-provides","title":"Requirements that <code>APPLY CHANGES INTO</code> Provides","text":"<ul> <li> <p>Performs incremental/streaming ingestion of CDC data.</p> </li> <li> <p>Provides simple syntax to specify one or many fields as the primary key for a table.</p> </li> <li> <p>Default assumption is that rows will contain inserts and updates.</p> </li> <li> <p>Can optionally apply deletes.</p> </li> <li> <p>Automatically orders late-arriving records using user-provided sequencing key.</p> </li> <li> <p>Uses a simple syntax for specifying columns to ignore with the <code>EXCEPT</code> keyword.</p> </li> <li> <p>Will default to applying changes as Type 1 SCD.</p> </li> </ul>"},{"location":"databricks/#processing-cdc-data-from-bronze_cleaned-to-customers_silver-table","title":"Processing CDC Data From <code>bronze_cleaned</code> to <code>customers_silver</code> table","text":"<ul> <li> <p>Creates the <code>customers_silver</code> table; <code>APPLY CHANGES INTO</code> requires the target table to be declared in a separate statement.</p> </li> <li> <p>Identifies the <code>customers_silver</code> table as the target into which the changes will be applied.</p> </li> <li> <p>Specifies the table <code>customers_bronze_clean</code> as the streaming source.</p> </li> <li> <p>Identifies the <code>customer_id</code> as the primary key.</p> </li> <li> <p>Specifies that records where the <code>operation</code> field is <code>DELETE</code> should be applied as deletes.</p> </li> <li> <p>Specifies the <code>timestamp</code> field for ordering how operations should be applied.</p> </li> <li> <p>Indicates that all fields should be added to the target table except <code>operation</code>, <code>source_file</code>, and <code>_rescued_data</code>.</p> </li> </ul> <pre><code>CREATE OR REFRESH STREAMING LIVE TABLE customers_silver;\n\nAPPLY CHANGES INTO LIVE.customers_silver` `\n  FROM STREAM(LIVE.customers_bronze_clean)\n  KEYS (customer_id)\n  APPLY AS DELETE WHEN operation = \"DELETE\"\n  SEQUENCE BY timestamp\n  COLUMNS * EXCEPT (operation, source_file, _rescued_data)\n</code></pre>"},{"location":"databricks/#querying-tables-with-applied-changes","title":"Querying Tables with Applied Changes","text":""},{"location":"databricks/#why-downstream-table-cant-perform-streaming-operations","title":"Why Downstream Table Can't Perform Streaming Operations?","text":"<p>While the target of our operation in the previous cell was defined as a streaming live table, data is being updated and deleted in this table (and so breaks the append-only requirements for streaming live table sources). As such, downstream operations cannot perform streaming queries against this table. </p> <p>This pattern ensures that if any updates arrive out of order, downstream results can be properly recomputed to reflect updates. It also ensures that when records are deleted from a source table, these values are no longer reflected in tables later in the pipeline.</p> <pre><code>CREATE LIVE TABLE customer_counts_state\n  COMMENT \"Total active customers per state\"\nAS SELECT state, count(*) as customer_count, current_timestamp() updated_at\n  FROM LIVE.customers_silver\n  GROUP BY state\n</code></pre>"},{"location":"databricks/#views-in-dlt","title":"Views in DLT","text":"<p>The query below defines a DLT view by replacing <code>TABLE</code> with the <code>VIEW</code> keyword.</p> <p>Views in DLT differ from persisted tables, and can optionally be defined as <code>STREAMING</code>.</p> <p>Views have the same update guarantees as live tables, but the results of queries are not stored to disk.</p> <p>Unlike views used elsewhere in Databricks, DLT views are not persisted to the metastore, meaning that they can only be referenced from within the DLT pipeline they are a part of. (This is similar scoping to temporary views in most SQL systems.)</p> <p>Views can still be used to enforce data quality, and metrics for views will be collected and reported as they would be for tables.</p>"},{"location":"databricks/#joining-and-referencing-tables","title":"Joining and Referencing Tables","text":"<p>In the query below, we create a new view by joining the silver tables from our <code>orders</code> and <code>customers</code> datasets. Note that this view is not defined as streaming; as such, we will always capture the current valid <code>email</code> for each customer, and will automatically drop records for customers after they've been deleted from the <code>customers_silver</code> table.</p>"},{"location":"databricks/#final-pipeline","title":"Final Pipeline","text":""},{"location":"databricks/#python-vs-sql","title":"Python vs SQL","text":""},{"location":"databricks/#pipeline-results-and-internals-of-dlt","title":"Pipeline Results and Internals of DLT","text":""},{"location":"databricks/#checking-list-of-all-tables","title":"Checking List of All Tables","text":"<p><pre><code>USE ${DA.schema_name};\n\nSHOW TABLES;\n</code></pre> </p>"},{"location":"databricks/#querying-orders-bronze-table","title":"Querying Orders Bronze Table","text":"<p><pre><code>SELECT * FROM orders_bronze\n</code></pre>  Recall that <code>orders_bronze</code> was defined as a streaming live table in DLT, but our results here are static.</p> <p>Because DLT uses Delta Lake to store all tables, each time a query is executed, we will always return the most recent version of the table. But queries outside of DLT will return snapshot results from DLT tables, regardless of how they were defined.</p>"},{"location":"databricks/#querying-customers_silver-table","title":"Querying <code>customers_silver</code> table","text":"<pre><code>SELECT * FROM customers_silver\n</code></pre> <p>This table dowes not have the additional fields like <code>__TimeStamp</code>, <code>__deleteVersion</code> and <code>__updateVersion</code>.</p> <p>The customers_silver table is actually a view oof another hidden table called <code>__apply_changes_storage_customer_silver</code>.</p> <p>This is seen when we run the describe command.</p> <pre><code>DESCRIBE EXTENDED customers_silver\n</code></pre> <p>Its being read from the <code>__apply_changes_storage_customer_silver</code> table </p>"},{"location":"databricks/#checking-the-__apply_changes_storage_customer_silver-table-records","title":"Checking the <code>__apply_changes_storage_customer_silver</code> table records","text":"<p><pre><code>SELECT * FROM __apply_changes_storage_customers_silver\n</code></pre> </p>"},{"location":"databricks/#what-is-in-the-storage-location","title":"What is in the storage location?","text":"<p><pre><code>files = dbutils.fs.ls(DA.paths.storage_location)\ndisplay(files)\n</code></pre> </p> <p>The autoloader and checkpoint directories contain data used to manage incremental data processing with Structured Streaming.</p> <p>The system directory captures events associated with the pipeline.</p>"},{"location":"databricks/#event-logs","title":"Event Logs","text":"<pre><code>files = dbutils.fs.ls(f\"{DA.paths.storage_location}/system/events\")\ndisplay(files)\n</code></pre> <p>Querying the Event Logs gives us lot of information</p> <p><pre><code>display(spark.sql(f\"SELECT * FROM delta.`{DA.paths.storage_location}/system/events`\"))\n</code></pre> </p> <p></p>"},{"location":"databricks/#pipeline-event-logs-deep-dive","title":"Pipeline Event Logs Deep Dive","text":""},{"location":"databricks/#query-the-event-log","title":"Query the Event Log","text":"<pre><code>event_log_path = f\"{DA.paths.storage_location}/system/events\"\n\nevent_log = spark.read.format('delta').load(event_log_path)\nevent_log.createOrReplaceTempView(\"event_log_raw\")\n\ndisplay(event_log)\n</code></pre> <p>The dataset includes an id for each transaction performed. </p>"},{"location":"databricks/#check-the-latest-update-id","title":"Check the Latest Update Id","text":"<pre><code>latest_update_id = spark.sql(\"\"\"\n    SELECT origin.update_id\n    FROM event_log_raw\n    WHERE event_type = 'create_update'\n    ORDER BY timestamp DESC LIMIT 1\"\"\").first().update_id\n\nprint(f\"Latest Update ID: {latest_update_id}\")\n\n# Push back into the spark config so that we can use it in a later query.\nspark.conf.set('latest_update.id', latest_update_id)\n</code></pre>"},{"location":"databricks/#perform-audit-logging","title":"Perform Audit Logging","text":"<p>Events related to running pipelines and editing configurations are captured as <code>user_action</code>.</p> <p>Yours should be the only <code>user_name</code> for the pipeline you configured during this lesson.</p> <pre><code>SELECT timestamp, details:user_action:action, details:user_action:user_name\nFROM event_log_raw \nWHERE event_type = 'user_action'\n</code></pre> <p></p>"},{"location":"databricks/#examining-data-lineage","title":"Examining Data Lineage","text":"<pre><code>SELECT details:flow_definition.output_dataset, details:flow_definition.input_datasets \nFROM event_log_raw \nWHERE event_type = 'flow_definition' AND \n      origin.update_id = '${latest_update.id}'\n</code></pre> <p>DLT provides built-in lineage information for how data flows through your table.</p> <p>While the query below only indicates the direct predecessors for each table, this information can easily be combined to trace data in any table back to the point it entered the lakehouse.</p> <p></p> <p></p>"},{"location":"databricks/#checking-data-quality-metrics","title":"Checking Data Quality Metrics \u26a0\ufe0f","text":"<p>If you define expectations on datasets in your pipeline, the data quality metrics are stored in the details:flow_progress.data_quality.expectations object. Events containing information about data quality have the event type flow_progress. The following example queries the data quality metrics for the last pipeline update:</p> <pre><code>SELECT row_expectations.dataset as dataset,\n       row_expectations.name as expectation,\n       SUM(row_expectations.passed_records) as passing_records,\n       SUM(row_expectations.failed_records) as failing_records\nFROM\n  (SELECT explode(\n            from_json(details :flow_progress :data_quality :expectations,\n                      \"array&lt;struct&lt;name: string, dataset: string, passed_records: int, failed_records: int&gt;&gt;\")\n          ) row_expectations\n   FROM event_log_raw\n   WHERE event_type = 'flow_progress' AND \n         origin.update_id = '${latest_update.id}'\n  )\nGROUP BY row_expectations.dataset, row_expectations.name\n</code></pre> <p></p>"},{"location":"databricks/#databricks-workflows","title":"Databricks Workflows","text":""},{"location":"databricks/#workflows-vs-dlt-pipelines","title":"Workflows vs DLT Pipelines","text":"<p>Workflows orchestrate all types of tasks(any kind of sql,spark and ml models)</p> <p>DLT is used to create streaming data pipelines using Python/SQL. It has quality controls and monitoring.</p> <p>These two can be integrated. DLT pipeline can be executed as a task in a workflow.</p> <p></p>"},{"location":"databricks/#differences","title":"Differences","text":""},{"location":"databricks/#use-cases","title":"Use Cases","text":""},{"location":"databricks/#features-of-workflows","title":"Features of Workflows","text":""},{"location":"databricks/#how-to-leverage-workflows","title":"How to Leverage Workflows?","text":""},{"location":"databricks/#common-workflow-patterns","title":"Common Workflow Patterns","text":"<p>The Fan-Out Pattern can be used when we have a single API from which data comes in but there are various data stores that the data must be stored in different shapes.</p>"},{"location":"databricks/#example-pipeline","title":"Example Pipeline","text":""},{"location":"databricks/#workflow-job-components","title":"Workflow Job Components","text":"<p>Shared Job Clusters provide flexibility by providing the ability to use same job cluster for more than one task. </p>"},{"location":"databricks/#defining-tasks","title":"Defining Tasks","text":""},{"location":"databricks/#scheduling-and-alerts","title":"Scheduling and Alerts","text":""},{"location":"databricks/#access-controls","title":"Access Controls","text":""},{"location":"databricks/#job-rrun-history","title":"Job Rrun History","text":""},{"location":"databricks/#repairing-jobs","title":"Repairing Jobs","text":"<p> In the above figure we can only rerun from the Silvers job and not the bronze one since its executed properly.</p>"},{"location":"databricks/#demo-of-workflow","title":"Demo of Workflow","text":"<p>Go to Workflows &gt; Create new workflow</p> <p></p> <p>Here is the workflow run from the course. I cant run it on my workkspace due to resource constraints.</p> <p></p> <p>Go to the same notebookDE 5.1.1 and Run the script under <code>Generate Pipeline</code></p> <p>Creating a DLT Pipeline in the Workflow </p> <p>For more info on workflows check this</p>"},{"location":"databricks/#unity-catalog","title":"Unity Catalog","text":"<ul> <li> <p>There is something called Unity Catalog Metastore that is different from the Hive Metastore and has advanced data lineage, security and auditing capabilities.</p> </li> <li> <p>Metadata like data about the tables, columns and ACL for the objects is stored in the Control Plane</p> </li> <li>Data related objects that are managed by the metastore are stored in the Cloud Storage.</li> <li>Once we connect to Unity Catalog, it connects the Hive Metastore as a special catalog named <code>hive_metastore</code></li> <li>Assets within the hive metastore can be easily referenced from Unity Catalog.</li> <li>Unity Catalog won't control access to the hive metastore but we can use the traditional ACLs</li> </ul>"},{"location":"databricks/#components-of-unity-catalog","title":"Components of Unity Catalog","text":"<ul> <li>Catalogs - Containers that only contain schemas</li> <li>Schema - Its a container for data bearing assets.</li> <li>Tables - They have two main information associated with them : data and metadata</li> <li>Views - perform SQL transformation of other tables or views. They do not have the ability to modify the other tables or views.</li> <li>Storage Credential - Allows Unity Catalog to access the external cloud storage via access creds.</li> <li>External Location - Allow users to divide the containers into smaller pieces and exercise control over it, They are mainly used to support external tables.</li> <li>Shares - They are used to define a read only logical collection of tables. These can be shared with a data reader outside the org.</li> </ul>"},{"location":"databricks/#unity-catalog-architecture","title":"Unity Catalog Architecture","text":"<ul> <li>In case before UC, we should provide different ACL's for each workspace and it must be shared.</li> <li>If the compute resources are not properly configured then the access rules can be bypassed very easily.</li> <li>In case of Unity Catalog, we can take out the entire User and Config Management outside workspaces.</li> <li>We just need to take care of the Compute Resources in the Workspaces. Any changes in the UC is automatically reflected in the Workspaces.</li> </ul>"},{"location":"databricks/#query-lifecycle","title":"Query Lifecycle","text":"<ul> <li>Queries can be issued via a data warehouse or BI tool. The compute resource begins processing the query.</li> <li>The UC then accepts the query, logs it and checks the security constraints.</li> <li>For each object of the query, UC assumes the IAM role or service principal governing the object as provided by a cloud admin.</li> <li>UC then generates a short term token and returns that token to the principal with the access url.</li> <li>Now the principal can request data using the URL from the cloud storage with the token.</li> <li>Data is then sent back from the cloud storage.</li> <li>Last mile row or column filtering can now be applied on the sql warehouse data.</li> </ul>"},{"location":"databricks/#compute-resources-and-unity-catalog","title":"Compute Resources and Unity Catalog","text":"<ul> <li>Dynamic Views are not supported on Single User Cluster.</li> <li>Cluster level installations don't work on Shared Clusters</li> <li>Dynamic Views offer row and column protection.</li> </ul>"},{"location":"databricks/#roles-and-admins-in-unity-catalog","title":"Roles and Admins in Unity Catalog","text":"<p>We can assign the roles via access connectors and there is no manual intervention needed.</p> <p></p> <ul> <li>Account and Metastore admins have full access to grant privileges and have the access to data objects.</li> <li>The Metastore admins have same privileges as Cloud Admin but only within the metastore that they own.</li> <li>There is also a Data Owner that controls and owns only the data objects that they created.</li> </ul> <p></p>"},{"location":"databricks/#identities-in-unity-catalog","title":"Identities in Unity Catalog","text":"<ul> <li>Service Principal is an individual identity for use with automated tools to run jobs and applications.</li> <li>They are assigned a name by the creator but are uniquely identified by the Global Unique Identifier ID.</li> <li>An access token can be used by the Service Principal using an API to access the data or use Databricks workflows.</li> <li>The Service Principals can be elevated to have admin privileges.</li> </ul>"},{"location":"databricks/#groups-in-unity-catalog","title":"Groups in Unity Catalog","text":"<ul> <li>Basically its a set of individual users gathered in one place to simplify the access.</li> <li> <p>Any grants given to group are inherited by the users.</p> </li> <li> <p>Groups can define who can access what data objects and how simplifying data governance policies.</p> </li> </ul> <p></p>"},{"location":"databricks/#multiple-nested-groups","title":"Multiple Nested Groups","text":""},{"location":"databricks/#identity-federation","title":"Identity Federation","text":"<ul> <li>There are two main identities, account identity and workspace identity.</li> <li>They are linked by a common identity like the email id of a user.</li> <li>So its important to have the same email in Account and Workspace, otherwise users can login to the workspace using one email but may not be able to access any data.</li> <li>To simplify this identity federation is used where the users, groups and their access controls are defined once in the Account Console and then they can be assigned to one or more workspaces as needed.</li> </ul>"},{"location":"databricks/#data-access-privileges","title":"Data Access Privileges","text":"<p>The access privileges are not implied or imperative in the case of Databricks.</p> <p>CREATE - Allows us to create child data objects like views, table and functions within the catalog.</p> <p>USAGE - Allows the person to traverse the child objects. To access a table we need usage access on the containing schema and the catalog.</p> <p>The privileges are propagated to child objects. For example, granting privileges to a catalog gives us the access to all the tables within the catalog.</p> <p>SELECT - allows querying of the table.</p> <p>MODIFY - allows modification of the table.</p> <p>VIEWS - users don't need access to the underlying source tables to access the view.</p> <p>EXECUTE - allows us to use the functions.</p> <p>STORAGE CREDENTIALS and EXTERNAL LOCATION - support three privileges, READ FILES, WRITE FILES and CREATE TABLE.</p> <p>SHARE - supports select statements only.</p> <p></p>"},{"location":"databricks/#privilege-on-various-objects","title":"Privilege on various objects","text":""},{"location":"databricks/#dynamic-views","title":"Dynamic Views","text":"<p>Dropping objects in any scenario can be done only by the owner.</p>"},{"location":"databricks/#external-locations-and-storage-credentials","title":"External Locations and Storage Credentials","text":"<p>We can refer to a single storage credential from various external locations.</p> <p>Because there can be many external locations that use the same storage credentials, DB recommends defining access using the external locations.</p> <p></p>"},{"location":"databricks/#best-practices-using-unity-catalog","title":"Best Practices using Unity Catalog","text":"<ol> <li>One Unity Catalog per region</li> <li>We can implement table sharing across many regions. But when we are sharing the tables, they appear as read only in the destination metastore.</li> <li>ACL's are not implemented in Region B, so they need to be setup separately.</li> <li>It may be costly to do this because data is queried across regions, we can rather ingest the data into region B and then query it.</li> </ol>"},{"location":"databricks/#data-segregation","title":"Data Segregation","text":"<ol> <li>We should not use Metastores to segregate data, because switching metastores needs workspace reassignment so the access and creds get spread across several roles in the workspaces.</li> <li>Metastores are actually a thin layer that references the meta data cloud storage object. Using UC container constructs [schemas and catalogs], enables the entire access and credentials to be handled by the metastore admins and the other catalog and workspace admins dont need to get involved.</li> </ol>"},{"location":"databricks/#methods-of-data-segregation","title":"Methods of Data Segregation","text":"<ul> <li>Workspace only identities will not have access to data access within unity catalog.</li> <li>But in June 2022, DB elevated all workspace and service principal users to account level privileges.</li> <li>No one should run     production jobs in the prod environment. This risks overwriting the prod data. Users should never be allowed modify access on prod tables.</li> </ul>"},{"location":"databricks/#storage-credential-vs-external-location","title":"Storage Credential vs External Location","text":"<p>The same access credentials that are part of the storage location is provided to the External Locations.</p> <p></p>"},{"location":"databricks/#unity-catalog_1","title":"Unity Catalog","text":"<ul> <li> <p>There is something called Unity Catalog Metastore that is different from the Hive Metastore and has advanced data lineage, security and auditing capabilities.</p> </li> <li> <p>Metadata like data about the tables, columns and ACL for the objects is stored in the Control Plane</p> </li> <li>Data related objects that are managed by the metastore are stored in the Cloud Storage.</li> <li>Once we connect to Unity Catalog, it connects the Hive Metastore as a special catalog named <code>hive_metastore</code></li> <li>Assets within the hive metastore can be easily referenced from Unity Catalog.</li> <li>Unity Catalog won't control access to the hive metastore but we can use the traditional ACLs</li> </ul>"},{"location":"databricks/#components-of-unity-catalog_1","title":"Components of Unity Catalog","text":"<ul> <li>Catalogs - Containers that only contain schemas</li> <li>Schema - Its a container for data bearing assets.</li> <li>Tables - They have two main information associated with them : data and metadata</li> <li>Views - perform SQL transformation of other tables or views. They do not have the ability to modify the other tables or views.</li> <li>Storage Credential - Allows Unity Catalog to access the external cloud storage via access creds.</li> <li>External Location - Allow users to divide the containers into smaller pieces and exercise control over it, They are mainly used to support external tables.</li> <li>Shares - They are used to define a read only logical collection of tables. These can be shared with a data reader outside the org.</li> </ul>"},{"location":"databricks/#unity-catalog-architecture_1","title":"Unity Catalog Architecture","text":"<ul> <li>In case before UC, we should provide different ACL's for each workspace and it must be shared.</li> <li>If the compute resources are not properly configured then the access rules can be bypassed very easily.</li> <li>In case of Unity Catalog, we can take out the entire User and Config Management outside workspaces.</li> <li>We just need to take care of the Compute Resources in the Workspaces. Any changes in the UC is automatically reflected in the Workspaces.</li> </ul>"},{"location":"databricks/#query-lifecycle_1","title":"Query Lifecycle","text":"<ul> <li>Queries can be issued via a data warehouse or BI tool. The compute resource begins processing the query.</li> <li>The UC then accepts the query, logs it and checks the security constraints.</li> <li>For each object of the query, UC assumes the IAM role or service principal governing the object as provided by a cloud admin.</li> <li>UC then generates a short term token and returns that token to the principal with the access url.</li> <li>Now the principal can request data using the URL from the cloud storage with the token.</li> <li>Data is then sent back from the cloud storage.</li> <li>Last mile row or column filtering can now be applied on the sql warehouse data.</li> </ul>"},{"location":"databricks/#compute-resources-and-unity-catalog_1","title":"Compute Resources and Unity Catalog","text":"<ul> <li>Dynamic Views are not supported on Single User Cluster.</li> <li>Cluster level installations don't work on Shared Clusters</li> <li>Dynamic Views offer row and column protection.</li> </ul>"},{"location":"databricks/#roles-and-admins-in-unity-catalog_1","title":"Roles and Admins in Unity Catalog","text":"<p>We can assign the roles via access connectors and there is no manual intervention needed.</p> <p></p> <ul> <li>Account and Metastore admins have full access to grant privileges and have the access to data objects.</li> <li>The Metastore admins have same privileges as Cloud Admin but only within the metastore that they own.</li> <li>There is also a Data Owner that controls and owns only the data objects that they created.</li> </ul> <p></p>"},{"location":"databricks/#identities-in-unity-catalog_1","title":"Identities in Unity Catalog","text":"<ul> <li>Service Principal is an individual identity for use with automated tools to run jobs and applications.</li> <li>They are assigned a name by the creator but are uniquely identified by the Global Unique Identifier ID.</li> <li>An access token can be used by the Service Principal using an API to access the data or use Databricks workflows.</li> <li>The Service Principals can be elevated to have admin privileges.</li> </ul>"},{"location":"databricks/#groups-in-unity-catalog_1","title":"Groups in Unity Catalog","text":"<ul> <li>Basically its a set of individual users gathered in one place to simplify the access.</li> <li> <p>Any grants given to group are inherited by the users.</p> </li> <li> <p>Groups can define who can access what data objects and how simplifying data governance policies.</p> </li> </ul> <p></p>"},{"location":"databricks/#multiple-nested-groups_1","title":"Multiple Nested Groups","text":""},{"location":"databricks/#identity-federation_1","title":"Identity Federation","text":"<ul> <li>There are two main identities, account identity and workspace identity.</li> <li>They are linked by a common identity like the email id of a user.</li> <li>So its important to have the same email in Account and Workspace, otherwise users can login to the workspace using one email but may not be able to access any data.</li> <li>To simplify this identity federation is used where the users, groups and their access controls are defined once in the Account Console and then they can be assigned to one or more workspaces as needed.</li> </ul>"},{"location":"databricks/#data-access-privileges_1","title":"Data Access Privileges","text":"<p>The access privileges are not implied or imperative in the case of Databricks.</p> <p>CREATE - Allows us to create child data objects like views, table and functions within the catalog.</p> <p>USAGE - Allows the person to traverse the child objects. To access a table we need usage access on the containing schema and the catalog.</p> <p>The privileges are propagated to child objects. For example, granting privileges to a catalog gives us the access to all the tables within the catalog.</p> <p>SELECT - allows querying of the table.</p> <p>MODIFY - allows modification of the table.</p> <p>VIEWS - users don't need access to the underlying source tables to access the view.</p> <p>EXECUTE - allows us to use the functions.</p> <p>STORAGE CREDENTIALS and EXTERNAL LOCATION - support three privileges, READ FILES, WRITE FILES and CREATE TABLE.</p> <p>SHARE - supports select statements only.</p> <p></p>"},{"location":"databricks/#privilege-on-various-objects_1","title":"Privilege on various objects","text":""},{"location":"databricks/#dynamic-views_1","title":"Dynamic Views","text":"<p>Dropping objects in any scenario can be done only by the owner.</p>"},{"location":"databricks/#external-locations-and-storage-credentials_1","title":"External Locations and Storage Credentials","text":"<p>We can refer to a single storage credential from various external locations.</p> <p>Because there can be many external locations that use the same storage credentials, DB recommends defining access using the external locations.</p> <p></p>"},{"location":"databricks/#best-practices-using-unity-catalog_1","title":"Best Practices using Unity Catalog","text":"<ol> <li>One Unity Catalog per region</li> <li>We can implement table sharing across many regions. But when we are sharing the tables, they appear as read only in the destination metastore.</li> <li>ACL's are not implemented in Region B, so they need to be setup separately.</li> <li>It may be costly to do this because data is queried across regions, we can rather ingest the data into region B and then query it.</li> </ol>"},{"location":"databricks/#data-segregation_1","title":"Data Segregation","text":"<ol> <li>We should not use Metastores to segregate data, because switching metastores needs workspace reassignment so the access and creds get spread across several roles in the workspaces.</li> <li>Metastores are actually a thin layer that references the meta data cloud storage object. Using UC container constructs [schemas and catalogs], enables the entire access and credentials to be handled by the metastore admins and the other catalog and workspace admins dont need to get involved.</li> </ol>"},{"location":"databricks/#methods-of-data-segregation_1","title":"Methods of Data Segregation","text":"<ul> <li>Workspace only identities will not have access to data access within unity catalog.</li> <li>But in June 2022, DB elevated all workspace and service principal users to account level privileges.</li> <li>No one should run     production jobs in the prod environment. This risks overwriting the prod data. Users should never be allowed modify access on prod tables.</li> </ul>"},{"location":"databricks/#storage-credential-vs-external-location_1","title":"Storage Credential vs External Location","text":"<p>The same access credentials that are part of the storage location is provided to the External Locations.</p> <p></p>"},{"location":"databricks/#practical-example","title":"Practical Example","text":"<p>I cannot create metastore in my account due to privilege problems. Just check the code to understand. Here is a video from the course regarding the example.</p>"},{"location":"databricks/#create-a-new-catalog","title":"Create a New Catalog","text":"<p>Let's create a new catalog in our metastore. The variable <code>${DA.my_new_catalog}</code> was displayed by the setup cell above, containing a unique string generated based on your username.</p> <p>Run the <code>CREATE</code> statement below, and click the Data icon in the left sidebar to confirm this new catalog was created.</p> <pre><code>CREATE CATALOG IF NOT EXISTS ${DA.my_new_catalog}\n</code></pre>"},{"location":"databricks/#selecting-the-default-catalog","title":"Selecting the Default Catalog","text":"<p>SQL developers will probably also be familiar with the <code>USE</code> statement to select a default schema, thereby shortening queries by not having to specify it all the time. To extend this convenience while dealing with the extra level in the namespace, Unity Catalog augments the language with two additional statements, shown in the examples below:</p> <pre><code>USE CATALOG mycatalog;\nUSE SCHEMA myschema;\n</code></pre> <p>Let's select the newly created catalog as the default. Now, any schema references will be assumed to be in this catalog unless explicitly overridden by a catalog reference.</p> <pre><code>USE CATALOG ${DA.my_new_catalog}\n</code></pre>"},{"location":"databricks/#create-a-new-schema","title":"Create a New Schema","text":"<p>Next, let's create a schema in this new catalog. We won't need to generate another unique name for this schema, since we're now using a unique catalog that is isolated from the rest of the metastore. Let's also set this as the default schema. Now, any data references will be assumed to be in the catalog and schema we created, unless explicitely overridden by a two- or three-level reference.</p> <p>Run the code below, and click the Data icon in the left sidebar to confirm this schema was created in the new catalog we created.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS example;\nUSE SCHEMA example\n</code></pre>"},{"location":"databricks/#set-up-tables-and-views","title":"Set Up Tables and Views","text":"<p>With all the necessary containment in place, let's set up tables and views. For this example, we'll use mock data to create and populate a silver managed table with synthetic patient heart rate data and a gold view that averages heart rate data per patient on a daily basis.</p> <p>Run the cells below, and click the Data icon in the left sidebar to explore the contents of the example schema. Note that we don't need to specify three levels when specifying the table or view names below, since we selected a default catalog and schema.</p> <pre><code>CREATE OR REPLACE TABLE heartrate_device (device_id INT, mrn STRING, name STRING, time TIMESTAMP, heartrate DOUBLE);\n\nINSERT INTO heartrate_device VALUES\n  (23, \"40580129\", \"Nicholas Spears\", \"2020-02-01T00:01:58.000+0000\", 54.0122153343),\n  (17, \"52804177\", \"Lynn Russell\", \"2020-02-01T00:02:55.000+0000\", 92.5136468131),\n  (37, \"65300842\", \"Samuel Hughes\", \"2020-02-01T00:08:58.000+0000\", 52.1354807863),\n  (23, \"40580129\", \"Nicholas Spears\", \"2020-02-01T00:16:51.000+0000\", 54.6477014191),\n  (17, \"52804177\", \"Lynn Russell\", \"2020-02-01T00:18:08.000+0000\", 95.033344842);\n\nSELECT * FROM heartrate_device\n</code></pre> <p></p> <pre><code>CREATE OR REPLACE VIEW agg_heartrate AS (\n  SELECT mrn, name, MEAN(heartrate) avg_heartrate, DATE_TRUNC(\"DD\", time) date\n  FROM heartrate_device\n  GROUP BY mrn, name, DATE_TRUNC(\"DD\", time)\n);\nSELECT * FROM agg_heartrate\n</code></pre> <p></p> <p>Querying the table above works as expected since we are the data owner. That is, we have ownership of the data object we're querying. Querying the view also works because we are the owner of both the view and the table it's referencing. Thus, no object-level permissions are required to access these resources.</p>"},{"location":"databricks/#the-accounts_user_group","title":"The <code>accounts_user_group</code>","text":"<p>In accounts with Unity Catalog enabled, there is an account users group. This group contains all users that have been assigned to the workspace from the Databricks account. We are going to use this group to show how data object access can be different for users in different groups.</p>"},{"location":"databricks/#grant-access-to-data-objects","title":"Grant Access to Data Objects","text":"<p>Unity Catalog employs an explicit permission model by default; no permissions are implied or inherited from containing elements. Therefore, in order to access any data objects, users will need USAGE permission on all containing elements; that is, the containing schema and catalog.</p> <p>Now let's allow members of the account users group to query the gold view. In order to do this, we need to grant the following permissions: 1. USAGE on the catalog and schema 1. SELECT on the data object (e.g. view)</p> <p>We need the USAGE command to actually make sure that the user reaches the point through the tree level structure to get to where the catalog is stored.</p> <p>Grant Privileges</p> <pre><code>GRANT USAGE ON CATALOG ${DA.my_new_catalog} TO `account users`;\nGRANT USAGE ON SCHEMA example TO `account users`;\nGRANT SELECT ON VIEW agg_heartrate to `account users`\n</code></pre>"},{"location":"databricks/#generate-a-query-and-access-the-data","title":"Generate a Query and access the data","text":"<p><pre><code>SELECT \"SELECT * FROM ${DA.my_new_catalog}.example.agg_heartrate\" AS Query\n</code></pre> </p>"},{"location":"databricks/#can-we-query-the-silver-table","title":"Can we query the silver table?","text":"<p>Back in the same query in the Databricks SQL session, let's replace gold with silver and run the query. This time it fails, because we never set up permissions on the silver table. </p> <p>Querying gold works because the query represented by a view is essentially executed as the owner of the view. This important property enables some interesting security use cases; in this way, views can provide users with a restricted view of sensitive data, without providing access to the underlying data itself. We will see more of this shortly.</p>"},{"location":"databricks/#granting-access-to-udf","title":"Granting Access to UDF","text":"<pre><code>CREATE OR REPLACE FUNCTION mask(x STRING)\n  RETURNS STRING\n  RETURN CONCAT(REPEAT(\"*\", LENGTH(x) - 2), RIGHT(x, 2)\n); \nSELECT mask('sensitive data') AS data\n</code></pre> <p>The above function masks the last two characters of the string <code>sensitive_data</code></p> <p>Now let's grant access to the function</p> <pre><code>GRANT EXECUTE ON FUNCTION mask to `account users`\n</code></pre> <p>Run the function using</p> <pre><code>SELECT \"SELECT ${DA.my_new_catalog}.example.mask('sensitive data') AS data\" AS Query\n</code></pre> <p>Now run the query in the SQL Editor you can see that the last two characters are redacted.</p>"},{"location":"databricks/#data-engineering-professional-learning-pathway","title":"Data Engineering Professional Learning Pathway","text":""},{"location":"databricks/#top-level-concepts","title":"Top Level Concepts","text":"<p>This is the course material that's part of Databricks AI Summit Learning Festival</p> <p></p>"},{"location":"databricks/#advantages-of-stream-processing","title":"Advantages of Stream Processing","text":""},{"location":"databricks/#stream-processing-architecture","title":"Stream Processing Architecture","text":""},{"location":"databricks/#challenges-with-streaming","title":"Challenges with Streaming","text":""},{"location":"databricks/#what-is-structured-streaming","title":"What is Structured Streaming?","text":""},{"location":"databricks/#unbounded-tables-for-streaming","title":"Unbounded Tables For Streaming","text":""},{"location":"databricks/#execution-mode-in-streaming","title":"Execution Mode in Streaming","text":""},{"location":"databricks/#anatomy-of-a-streaming-query","title":"Anatomy of a Streaming Query","text":"<p>Source, Input Tables, Result tables and storage layer in streaming. </p>"},{"location":"databricks/#step-1-read-data-from-any-streaming-source","title":"Step 1: Read data from any streaming source","text":""},{"location":"databricks/#step-2-transform-the-data","title":"Step 2 : Transform the data","text":""},{"location":"databricks/#step-3-sink","title":"Step 3 : Sink","text":""},{"location":"databricks/#step-4-trigger-mechanism","title":"Step 4 : Trigger Mechanism","text":"<p>Checkpoint is there to ensure fault tolerance. </p> <p>The API definitions for batch and streaming is the same.</p>"},{"location":"databricks/#types-of-triggers","title":"Types of Triggers","text":""},{"location":"databricks/#output-modes","title":"Output Modes","text":""},{"location":"databricks/#demo-streaming-data-query","title":"Demo : Streaming Data Query","text":"<p>\"readStream\" instead of \"read\" the tranformations are the same.  </p> <p>Data Metrics </p>"},{"location":"databricks/#writing-data-to-a-delta-lake-sink","title":"Writing Data to a Delta Lake Sink","text":"<p>We can see the status of the query</p> <p></p> <p>... and also see the metrics from the previous query</p> <p></p>"},{"location":"databricks/#using-delta-tables-as-a-streaming-source","title":"Using Delta tables as a Streaming Source","text":""},{"location":"databricks/#tuning-the-parameters-for-a-delta-streaming-source","title":"Tuning the parameters for a delta streaming source","text":""},{"location":"databricks/#streaming-to-a-delta-table","title":"Streaming to a delta table","text":"<p>For any arbitary aggregations on streaming data, use complete mode. </p>"},{"location":"databricks/#aggregations-time-windows-and-watermark","title":"Aggregations, Time Windows and Watermark","text":""},{"location":"databricks/#types-of-stream-processing","title":"Types of Stream Processing","text":"<p>There are two types of Stream Processing</p> <ul> <li>Stateless</li> <li>Stateful</li> </ul> <p></p>"},{"location":"databricks/#intermediate-state-to-keep-track","title":"Intermediate State to Keep Track","text":""},{"location":"databricks/#aggregations-over-time-windows","title":"Aggregations over Time Windows","text":""},{"location":"databricks/#event-time-vs-processing-time","title":"Event Time vs Processing Time","text":"<p>The unbounded tables must always be processed in order. </p>"},{"location":"databricks/#tumbling-window-vs-sliding-window","title":"Tumbling Window vs Sliding Window","text":""},{"location":"databricks/#sliding-window-example","title":"Sliding Window Example","text":"<p>Here is a 10 min window with 5 min overlap </p>"},{"location":"databricks/#challenges-with-sliding-window","title":"Challenges with Sliding Window","text":"<p>There is a lot of Memory Pressure with using sliding window because all the data is stored in executor memory.</p> <p></p> <p>We can tackle this by storing the data off heap </p>"},{"location":"databricks/#watermarking-late-threshold","title":"Watermarking / Late Threshold","text":"<p>Watermarking is a technique that basically tells how long is too late?</p> <p>For example if data record with id = 100 came in now at 12:05am then if we keep window as 10 min then we will wait until 12:15am and if no data arrives for that id we purge the state.</p> <p>The dog record in the below pic has eventTime of 12:04 but its beyond our 10 min window (12:05 - 12:15) so we dont consider that record and miss the count. </p> <p>Late arriving data state is dropped and we save memory</p> <p></p>"},{"location":"databricks/#demo-time-window-aggregations-with-watermarking","title":"Demo : Time window aggregations with Watermarking","text":""},{"location":"databricks/#read-and-process-streaming-source","title":"Read and Process Streaming Source","text":"<p>Output </p> <p>Windowing</p> <p>Goal is to find the revenue in USD in a 60 min time window.</p> <p></p> <ul> <li>Here the window is of 60 min but we add a watermark of 90 min to cater to any late arriving data.</li> <li>Then we group by both eventTimestamp and the city to find the revenue.</li> </ul> <p>Output </p>"},{"location":"databricks/#writing-data-in-append-mode","title":"Writing Data in Append Mode","text":"<p>The catch with append mode is that the data is not going to be written into the sink until and unless we finish the hour (window duration)</p> <p>The availableNow trigger processes all the data currently available in the source and then stops. It\u2019s like a one-time cleanup of everything in your inbox.</p> <p><pre><code>writeStreamAvailableNow = (\n    df.writeStream.format(\"delta\")\n    .option(\"checkpointLocation\", f\"{checkpoint_location}\")\n    .trigger(availableNow=True)\n    .outputMode(\"append\")\n    .queryName(\"AvailableNow\")\n    .toTable(\"default.streaming_circuits\")\n)\n</code></pre> </p> <p>Result: here, the query processes all data available in the source and then terminates. This is a great choice when you want to process data in a \u201cstream-like\u201d fashion but only once.</p> <p>There is always some delay between creating streaming table and the commits happening to it.</p> <p></p>"},{"location":"databricks/#writing-data-in-update-mode","title":"Writing Data in update mode","text":""},{"location":"databricks/#step-1-create-a-table","title":"Step 1 : Create a table","text":""},{"location":"databricks/#step-2-write-a-merge-query-to-merge-the-data-based-on-start-and-end-time","title":"Step 2 : Write a MERGE Query to merge the data based on start and end time","text":""},{"location":"databricks/#data-ingestion-patterns","title":"Data Ingestion Patterns","text":""},{"location":"databricks/#how-to-deal-with-ephemeral-data","title":"How to deal with ephemeral data?","text":"<p>We can deal with transient data by creating STREAMING LIVE tables that contain raw data from source.</p> <p></p> <p>Then we can update other tables like silver tables (also live) with transformations.</p>"},{"location":"databricks/#simplex-vs-multiplex-ingestion","title":"Simplex vs Multiplex Ingestion","text":""},{"location":"databricks/#dont-use-kafka-as-bronze-table","title":"Dont use Kafka as bronze table","text":""},{"location":"databricks/#solution","title":"Solution","text":"<ul> <li>First simplex and store data in bronze</li> <li>Then multiplex it and transform into multiple silver tables. </li> </ul>"},{"location":"databricks/#dlt-demo","title":"DLT Demo","text":""},{"location":"databricks/#autoloader-for-bronze-ingestion","title":"Autoloader for bronze ingestion","text":"<p>Here are the conf parameters</p> <p></p> <p>This is the syntax of a dlt table.</p> <ul> <li>we first define the parameters of the table and dont allow reset on it.</li> </ul> <p></p> <ul> <li>now we stream data from cloud files using autoloader. </li> </ul> <p>Final dlt pipeline</p> <p></p>"},{"location":"databricks/#transforming-the-data","title":"Transforming the data","text":""},{"location":"databricks/#quality-enforcement-in-dlt","title":"Quality Enforcement in DLT","text":""},{"location":"databricks/#quality-enforcement-in-bronze","title":"Quality enforcement in Bronze","text":""},{"location":"databricks/#quality-enforcement-in-silver","title":"Quality enforcement in Silver","text":""},{"location":"databricks/#schema-enforcement-patterns","title":"Schema Enforcement Patterns","text":""},{"location":"databricks/#alternate-enforcement-methods","title":"Alternate Enforcement Methods","text":""},{"location":"databricks/#defining-expectations-in-dlt","title":"Defining Expectations in DLT","text":""},{"location":"databricks/#demo-enforcing-validations-on-the-data","title":"Demo: Enforcing Validations on the Data","text":""},{"location":"databricks/#quarantining-records","title":"Quarantining Records","text":"<p>We say expect all or drop which indicates that all rules must pass else drop records. </p> <p>Now we can use those rules</p> <p></p> <p>The Data Quality metrics in the UI</p> <p></p>"},{"location":"databricks/#databricks-data-privacy","title":"Databricks Data Privacy","text":""},{"location":"databricks/#storing-data-securely","title":"Storing Data Securely","text":""},{"location":"databricks/#regulatory-compliance","title":"Regulatory Compliance","text":"<p>GDPR and CCPA</p> <p></p>"},{"location":"databricks/#databricks-simplifying-compliance","title":"Databricks Simplifying Compliance","text":""},{"location":"databricks/#3-key-aspects-of-data-privacy","title":"3 Key Aspects of Data Privacy","text":""},{"location":"databricks/#advanced-unity-catalog-concepts","title":"Advanced Unity Catalog Concepts","text":""},{"location":"databricks/#components-of-unity-catalog_2","title":"Components of Unity Catalog","text":"<p>How unity catalog provides single governance model? </p>"},{"location":"databricks/#access-control-lists","title":"Access Control Lists","text":""},{"location":"databricks/#managing-acls","title":"Managing ACLs","text":""},{"location":"databricks/#tagging-and-ai-docs","title":"Tagging and AI Docs","text":""},{"location":"databricks/#search-objects-with-tags","title":"Search objects with tags","text":""},{"location":"databricks/#fine-grained-access-control","title":"Fine Grained Access Control","text":"<p>Dynamic Views Fine Grained Access Control</p> <p></p> <p><code>current_user</code> </p> <p></p> <p><code>is_member()</code></p> <p></p>"},{"location":"databricks/#row-level-security-and-column-masking","title":"Row level security and Column Masking","text":"<p>What are row filters?</p> <p>Row filters allow you to apply a filter to a table so that queries return only rows that meet the filter criteria. You implement a row filter as a SQL user-defined function (UDF). Python and Scala UDFs are also supported, but only when they are wrapped in SQL UDFs.</p> <p>What are column masks? Column masks let you apply a masking function to a table column. The masking function evaluates at query runtime, substituting each reference of the target column with the results of the masking function. For most use cases, column masks determine whether to return the original column value or redact it based on the identity of the invoking user. Column masks are expressions written as SQL UDFs or as Python or Scala UDFs wrapped in SQL UDFs.</p> <p>Each table column can have only one masking function applied to it. The masking function takes the unmasked value of the column as input and returns the masked value as its result. The return value of the masking function should be the same type as the column being masked. The masking function can also take additional columns as input parameters and use those in its masking logic.</p>"},{"location":"databricks/#examples","title":"Examples","text":"<p>This example creates a SQL user-defined function that applies to members of the group admin in the region US.</p> <p>When this sample function is applied to the sales table, members of the admin group can access all records in the table. If the function is called by a non-admin, the RETURN_IF condition fails and the region='US' expression is evaluated, filtering the table to only show records in the US region.</p> <p><pre><code>CREATE FUNCTION us_filter(region STRING)\nRETURN IF(IS_ACCOUNT_GROUP_MEMBER('admin'), true, region='US');\n</code></pre> Use it on a table</p> <pre><code>CREATE TABLE sales (region STRING, id INT);\nALTER TABLE sales SET ROW FILTER us_filter ON (region);\n</code></pre>"},{"location":"databricks/#column-mask-examples","title":"Column Mask Examples","text":"<pre><code>CREATE FUNCTION ssn_mask(ssn STRING)\n  RETURN CASE WHEN is_account_group_member('HumanResourceDept') THEN ssn ELSE '***-**-****' END;\n</code></pre> <pre><code>--Create the `users` table and apply the column mask in a single step:\n\nCREATE TABLE users (\n  name STRING,\n  ssn STRING MASK ssn_mask);\n</code></pre>"},{"location":"databricks/#filtering-on-unspecified-columns","title":"Filtering on unspecified columns","text":"<pre><code>DROP FUNCTION IF EXISTS row_filter;\n\nCREATE FUNCTION row_filter()\n  RETURN EXISTS(\n    SELECT 1 FROM valid_users v\n    WHERE v.username = CURRENT_USER()\n);\n</code></pre> <pre><code>DROP TABLE IF EXISTS data_table;\n\nCREATE TABLE data_table\n  (x INT, y INT, z INT)\n  WITH ROW FILTER row_filter ON ();\n\nINSERT INTO data_table VALUES\n  (1, 2, 3),\n  (4, 5, 6),\n  (7, 8, 9);\n</code></pre>"},{"location":"databricks/#performance-considerations","title":"Performance Considerations","text":""},{"location":"databricks/#auditing-data","title":"Auditing Data","text":""},{"location":"databricks/#table-level-data","title":"Table Level Data","text":"<p>What was the last updated time? </p>"},{"location":"databricks/#user-level-data","title":"User Level Data","text":"<p>Who used which tables? </p>"},{"location":"databricks/#lineage-data","title":"Lineage Data","text":"<p>What is the lineage of the tables? </p>"},{"location":"databricks/#data-isolation","title":"Data Isolation","text":""},{"location":"databricks/#centralized-and-distributed-metastore-models","title":"Centralized and Distributed Metastore Models","text":"<p>Centralized</p> <p>The metastore owners govern all the objects. </p> <p>Distributed</p> <p>The governance is at the catalog level.</p>"},{"location":"databricks/#external-locations-and-storage-credentials_2","title":"External Locations and Storage Credentials","text":""},{"location":"databricks/#unity-catalog-model-to-access-data","title":"Unity Catalog Model to Access data","text":"<ul> <li>The data comes from the cloud storage to the user, not from the table. Unity catalog only checks the permissions and audit log.</li> </ul>"},{"location":"databricks/#encryption-by-default","title":"Encryption by Default","text":""},{"location":"databricks/#managing-access-to-pii","title":"Managing Access to PII","text":""},{"location":"databricks/#best-practices","title":"Best Practices","text":""},{"location":"databricks/#demo","title":"Demo","text":""},{"location":"databricks/#protecting-columns-and-rows","title":"Protecting Columns and Rows","text":""},{"location":"databricks/#dynamic-views_2","title":"Dynamic Views","text":""},{"location":"databricks/#example","title":"Example","text":""},{"location":"databricks/#row-filters-and-column-mask","title":"Row filters and Column Mask","text":"<p>Applying the filter</p> <p></p> <p>Creating Column Mask</p> <p></p> <p></p> <p>Table Tags </p>"},{"location":"databricks/#pii-data-security","title":"PII Data security","text":""},{"location":"databricks/#pseudonymization","title":"Pseudonymization","text":""},{"location":"databricks/#hashing","title":"Hashing","text":""},{"location":"databricks/#tokenization","title":"Tokenization","text":""},{"location":"databricks/#anonymization","title":"Anonymization","text":""},{"location":"databricks/#pii-data-security-demo","title":"PII Data Security : Demo","text":"<p>Two methods hashing and tokenization.</p>"},{"location":"databricks/#hashing_1","title":"Hashing","text":"<p>Create a dlt table </p> <p>Create a salt and hash </p> <p>Create user lookup table with alt id generated from above.</p> <p></p> <p>Now check user_id_hashed table</p> <p></p>"},{"location":"databricks/#tokenization_1","title":"Tokenization","text":"<p>Create a join with real and pseudo lookup table.</p> <p></p> <p>The lookup table </p> <p>The tokennized joined table </p>"},{"location":"databricks/#anonymization_1","title":"Anonymization","text":"<p>Irreversible </p> <p></p> <p>Setting up tables</p> <p></p> <p></p>"},{"location":"databricks/#schema-for-users-bronze-table","title":"Schema for users bronze table","text":"<p>Age bins function</p> <p></p> <p></p>"},{"location":"databricks/#change-data-feed","title":"Change Data Feed","text":""},{"location":"databricks/#solution-i-ignore-deletes-and-updates","title":"Solution I : Ignore deletes and updates","text":""},{"location":"databricks/#benefits-of-change-data-feed","title":"Benefits of Change Data Feed","text":""},{"location":"databricks/#cdc-vs-cdf","title":"CDC vs CDF","text":""},{"location":"databricks/#how-cdf-works","title":"How CDF works?","text":""},{"location":"databricks/#consuming-delta-cdf","title":"Consuming Delta CDF","text":"<p>If multiple updates come in one micro batch we need to select 1 of them.</p> <p></p>"},{"location":"databricks/#how-to-collect-changes","title":"How to collect changes?","text":""},{"location":"databricks/#deleting-pii-data","title":"Deleting PII data","text":""},{"location":"databricks/#data-vacuuming","title":"Data Vacuuming","text":"<p>Materialized views cannot be used for DML ops, we need to REFRESH manually</p> <p></p>"},{"location":"databricks/#cdf-demo","title":"CDF Demo","text":"<p>Step 1 : Create stream from source</p> <p></p> <p></p> <p>Step 2 : Create silver and upsert_to_delta function</p> <p></p> <p></p> <p></p> <p>Step 3 : Initiate the stream and check history</p> <p></p> <p></p> <p>We will have two images for any updates </p> <p>Step 4 : Insert New data</p> <p></p> <p>Step 5 : Check what is changed</p> <p></p> <p>We can get operational metrics also</p> <p></p> <p></p> <p>Check all rows that were updated</p> <p></p> <p></p> <p>Adding commit messages in history</p> <p></p> <p>Create a table called delete_requests</p> <p></p> <p>How to process delete requests?</p> <p></p> <p></p> <p></p> <p>Collect Deleted silver users to process with CDF</p> <p></p> <p>Propogate deletes to gold tables</p> <p></p> <p></p> <p>Are deletes fully commited?</p> <p>No, we can see them in previous versions of the data.</p> <p></p>"},{"location":"databricks/#performance-optimization-in-databricks","title":"Performance Optimization in Databricks","text":"<p>Some common problems</p> <p></p> <p>Avoiding small files problem</p> <p></p>"},{"location":"databricks/#demo-file-explosion","title":"Demo : File Explosion","text":"<p>Partitioning by Id</p> <p></p> <p>Now we do some aggregation on non partitioned columns. It takes 7 seconds to compute.</p> <p></p> <p>Because we partitioned by Id the query is looking into 2500 files</p> <p></p> <p>Instead if we dont add partitioning then the reads are much faster.</p>"},{"location":"databricks/#data-skipping","title":"Data Skipping","text":""},{"location":"databricks/#z-ordering","title":"Z-Ordering","text":"<p>We have stored data in min and max column basis.</p> <p></p> <p></p>"},{"location":"databricks/#some-considerations","title":"Some Considerations","text":""},{"location":"databricks/#liquid-clustering","title":"Liquid Clustering","text":"<p>\ud83e\uddf1 Partitioning Partitioning physically organizes data into separate folders on storage based on the values of one or more columns. For example, partitioning a sales table by region and year results in directories like /region=US/year=2024/. This allows partition pruning: when queries filter on those columns, Databricks reads only relevant partitions, improving performance.</p> <p>Partitioning works best when:</p> <ul> <li> <p>You have low-cardinality columns (few unique values).</p> </li> <li> <p>You know the access patterns ahead of time.</p> </li> <li> <p>You want predictable file organization.</p> </li> </ul> <p>However, partitioning becomes problematic with high-cardinality columns (e.g., user_id or product_id) because it creates too many folders. This is called partition explosion, which leads to small files, slow queries, and high metadata overhead.</p> <p>\ud83c\udf0a Liquid Clustering Liquid Clustering is a newer, automatic file organization technique that clusters data logically, not physically. Instead of creating folders, it reorganizes data within Delta files based on specified columns (like user_id, timestamp). It improves filtering performance without the downsides of static partitioning.</p> <ul> <li> <p>It works incrementally\u2014Databricks automatically reclusters the data in the background using OPTIMIZE jobs. This makes it ideal for:</p> </li> <li> <p>High-cardinality columns.</p> </li> <li> <p>Streaming or frequently updated datasets.</p> </li> <li> <p>Situations where partitioning would be too rigid or hard to manage.</p> </li> </ul> <p>With Liquid Clustering, you just define the clustering columns using table properties, and Databricks takes care of the rest. It\u2019s flexible, scalable, and low-maintenance.</p> <p>If we want to change liquid clustering columns, we dont need to rewrite whole table.</p> <p>Liquid clustering intelligently makes sure the files are of same size.</p> <p></p> <p></p>"},{"location":"databricks/#predictive-optimization","title":"Predictive Optimization","text":""},{"location":"databricks/#code-optimizations","title":"Code Optimizations","text":""},{"location":"databricks/#data-skew","title":"Data Skew","text":"<p>Salting approach by adding some suffix to each key</p> <p></p>"},{"location":"databricks/#data-shuffling","title":"Data Shuffling","text":"<p>Mitigate Shuffling</p> <p></p>"},{"location":"databricks/#demo-of-shuffling","title":"Demo of Shuffling","text":"<p>This data shows the shuffling of data, we can see that around 1.5 G of shuffling happenend. </p>"},{"location":"databricks/#broadcast-join","title":"Broadcast Join","text":"<p>After enabling broadcast join there are only few Kb worth of shuffling </p>"},{"location":"databricks/#spill","title":"Spill","text":"<p>When does skew occur</p> <p></p> <p>Spilling to memory and disk</p> <p></p> <p></p>"},{"location":"databricks/#mitigating-serialization-issues","title":"Mitigating Serialization Issues","text":""},{"location":"databricks/#demo-udf","title":"Demo : UDF","text":"<p>For 60 records it takes one minute because the data is not being run on diff cores, its being run one after another.</p> <p>Solution: We repartition the data</p> <p></p>"},{"location":"databricks/#how-sql-udf-is-better","title":"How SQL UDF is better?","text":"<p>Query is supported by Photon </p>"},{"location":"databricks/#cluster-optimization","title":"Cluster Optimization","text":""},{"location":"databricks/#photon_1","title":"Photon","text":""},{"location":"databricks/#cluster-optimization-techniques","title":"Cluster Optimization Techniques","text":"<p>\ud83c\udf88 Imagine you're at a birthday party...</p> <p>There are 200 kids playing a game where they all have to sort their candies by color. But there\u2019s a rule: All the red candies go to one basket, all the green to another, and so on.</p> <p>Now, to do this, the kids need to share and move candies around \u2014 this is like a shuffle in Spark. It\u2019s when data (candies) gets moved around to be grouped or sorted.</p> <p>\ud83e\uddfa Now, what is spark.sql.shuffle.partitions?</p> <p>It\u2019s like saying:</p> <p>\u201cHow many baskets should we use to sort all the candies?\u201d</p> <p>So if we set:</p> <p>spark.sql.shuffle.partitions = 200 \u2192 Spark uses 200 baskets</p> <p>spark.sql.shuffle.partitions = 50 \u2192 Uses 50 baskets</p> <p>\ud83c\udfa8 Why it matters:</p> <p>Too many baskets (e.g. 1000): Some baskets might only get 1 candy, but the kids still have to carry them \u2014 too much work!</p> <p>Too few baskets (e.g. 5): Baskets get too full, hard to carry \u2014 some kids may drop candies \ud83d\ude2c</p> <p>So we need a good number of baskets so all the kids can sort fast, without making a mess!</p> <p>\ud83e\udde0 What Spark does:</p> <p>When Spark runs a big job (like sorting, grouping, or joining), it shuffles data. Then it needs to know:</p> <p>\u201cInto how many parts (baskets) should I split the shuffled data?\u201d</p> <p>That\u2019s what spark.sql.shuffle.partitions controls.</p> <p>\ud83e\uddc1 In short:</p> <p>It's like how many baskets Spark uses to sort data after mixing it.</p> <p>Default is 200 baskets.</p> <p>If your job is small \u2192 use fewer baskets.</p> <p>If your job is huge \u2192 maybe more baskets help.</p> <p>Or better yet, let Spark decide by itself using Adaptive Query Execution \u2014 like having a smart friend who picks the perfect number of baskets for each game \ud83c\udfaf</p>"},{"location":"databricks/#databricks-ci-cd","title":"Databricks CI / CD","text":""},{"location":"databricks/#databricks-asset-bundles","title":"Databricks Asset Bundles","text":""},{"location":"databricks/#typical-ci-cd-overview","title":"Typical CI CD Overview","text":""},{"location":"databricks/#how-are-dabs-structured","title":"How are DAB's structured?","text":""},{"location":"databricks/#keys-in-the-mapping-yaml-file","title":"Keys in the Mapping yaml file","text":"<p><code>target</code> keys</p> <p></p>"},{"location":"databricks/#steps-to-deploy","title":"Steps to Deploy","text":""},{"location":"databricks/#variables-in-dabs","title":"Variables in DABs","text":"<p>Lookup Variables</p> <p></p> <p></p> <p>By default the DAB deployed jobs will have this name in dev</p> <p></p>"},{"location":"databricks/#defining-the-dab","title":"Defining the DAB","text":""},{"location":"databricks/#running-the-job-from-the-dab","title":"Running the job from the DAB","text":"<p><code>databricks bundle run -t development demo01_simple_dab</code></p>"},{"location":"databricks/#destryoing-the-bundle","title":"Destryoing the Bundle","text":"<p><code>databricks bundle destory --auto-approve</code></p>"},{"location":"databricks/#modularizing-the-code","title":"Modularizing the code","text":"<ul> <li>In the resources folder we can keep the config for the jobs and use it as an include in <code>databricks.yml</code></li> </ul> <p>Using lookup variable for cluster</p> <p><pre><code>    my-cluster-id:\n        description: Get lab user id using lookup variable\n        lookup:\n            cluster: vedanth\n</code></pre> The above code will fetch the cluster_id automatically</p> <p>Now remember we have defined our job yml in the resources folder, we are going to add existing_cluster_id (override) it using our lookup variable</p> <p></p> <p>For production we want to use serverless so we dont override.</p> <p></p> <p>How to check json version?</p> <p><code>databricks bundle validate --output json</code></p> <p>Each environment will have separate folder in <code>.bundle</code> folder.</p> <p></p>"},{"location":"databricks/#dab-project-templates","title":"DAB Project Templates","text":""},{"location":"databricks/#ci-cd-with-databricks-asset-bundles","title":"CI CD with Databricks Asset Bundles","text":""},{"location":"databricks/#expectations-for-integration-tests","title":"Expectations for Integration Tests","text":""},{"location":"databricks/#demo-full-project-with-dab","title":"Demo : Full Project with DAB","text":"<p>Project Architecture</p> <p></p> <p>Folder Structure</p> <p>Top Level structure</p> <p></p> <p></p> <p></p> <p><code>variables.yaml</code></p> <p></p> <p><code>workflow_job.yaml</code></p> <p></p> <p><code>health_etl_pipeline.yml</code></p> <p></p>"},{"location":"databricks/#deployment-with-github-actions","title":"Deployment with GitHub Actions","text":""},{"location":"streaming/","title":"Streaming Architecture","text":""},{"location":"streaming/#redpanda","title":"Redpanda","text":""},{"location":"streaming/#course-1-streaming-fundamentals","title":"Course 1 : Streaming fundamentals","text":""},{"location":"streaming/#streaming-data-processing","title":"Streaming Data Processing","text":""},{"location":"streaming/#real-life-example","title":"Real Life Example","text":"<p>Many traditional systems have taught us to think of data in its resting state. Relational databases, distributed file systems, data lakes and data warehouses, all store bounded sets of data that are typically accessed on an ad hoc or scheduled basis. We typically think of this data as just a pile of bytes, that we can query whenever we need to.</p> <p>The data in these systems are often modeled as objects or nouns, and these systems are good at certain tasks. For example, want to know how many users have signed up for your website? Well, you could run a query on the current snapshot of your users table to answer that question.</p> <p>But viewing data in a resting state has limitations. If we want to process the data continuously, we have to make several trips to the storage layer, usually at some fixed and arbitrary interval, to create a report, make a business decision, or simply retrieve the data. This is called batch processing, and it\u2019s often associated with high latency and slow decision making.</p> <p>Also, these systems typically only concern themselves with the current state of the data, and lose track of the historical changes that took place beforehand. Extending our previous example - if you go to the users table, you\u2019ll see the current snapshot or state of all the users in your system. But how did that state evolve overtime? And what intermediate state changes took place before we arrived at the current snapshot of our data? These questions are difficult to answer with traditional systems, hindering our ability to understand more complex behaviors in our data.</p> <p>But in recent years, as data has become more ubiquitous and as our ability to collect data has improved, data systems have evolved beyond the data at rest model. Today, new systems have been developed to handle data in motion.</p> <p>You see, more often than not, data is actually flowing in continuous and unbounded streams, and we need to process the data as soon as it becomes available.</p> <p>Some examples of continuous data streams include:</p> <ul> <li>IoT sensor data</li> <li>Financial transactions</li> <li>Social media feeds</li> <li>Website activity</li> <li>Infrastructure events</li> </ul> <p>Being able to process and store these kinds of infinite and fast-moving data streams has given rise to a new type of system: streaming data platforms.</p> <p>Streaming data platforms are optimized for low-latency and high throughput, allowing us to store and access data quickly and continuously. They also allow us to process data as soon as it becomes available; This is called stream processing. But there are some other differences between streaming data platforms and traditional storage systems, as well.</p> <p>First, instead of modeling data as nouns or things, event streaming involves the continuous collection, consumption, and processing of events. Events are exactly what they sound like. They describe something that occurs at a particular point in time.</p> <p>For example, do you remember that time on your birthday where you ate too much cake? Eating too much cake on your birthday is an event. It\u2019s something that happened at a particular point in time.</p> <p>Organizations have events too. A marketplace like Etsy may have user_login events, add_to_cart events, and item_purchase events. A financial institution may have account_open events, funds_deposited and funds_withdrawn events. Video streaming services like Netflix may have events like video_watched, video_rated, or even events generated by machine learning models; for example - network_prediction events for adaptive video quality. Regardless of what industry you find yourself in, there are events all around you, you just have to know where to look.</p> <p>Events drive intelligence, analytics, and product features. And it\u2019s not uncommon for even a moderately sized organization to generate thousands or even millions of events per second. And again, these event streams are continuously flowing and theoretically infinite.</p> <p>Another difference between traditional systems and streaming data platforms, is that instead of only being concerned with the current state of our data, streaming data platforms allow us to maintain the entire historical record of everything that happened beforehand. In other words - the intermediate state changes of our data. This pattern of capturing intermediate state changes using events is called event sourcing.</p> <p>This is made possible thanks to the way event streaming platforms like Redpanda model events: as immutable, timestamped facts.</p> <p>Now you may be asking, why are events immutable?</p> <p>Well, events are immutable because history is important, and event sourcing allows us to preserve the history of our data. Immutability respects a simple fact:  we can\u2019t change something that happened in the past. We all know that you ate too much cake on your birthday, and even if you\u2019re in a better state now, historical events still provide us with a lot of good context about the past. Without the historical record, we wouldn\u2019t know about that weird cake phase you went through, we would just know about your current state.</p> <p>From an organizational perspective, the historical event record is important for a couple of reasons:</p> <p>Some of the most important data insights lie within the historical record of your data. How did the data change over time? What events preceded the current event? What behaviors are present in our data? These questions are much easier to answer in an event-driven system that maintains the complete historical record.</p> <p>Think about an ecommerce scenario. A user adds an item to their cart and then removes it. The current state shows an empty cart for this user. But knowing what happened before - i.e. that the user added a certain item to their cart, gives us some insight into what this user may be interested in. The current state doesn\u2019t show that, but the historical state does.</p> <p>Also, there is a huge operational advantage to event sourcing: we can easily rebuild the state of our downstream systems by re-processing the historical sequence of events. To understand the value of this, consider another example. What if you realize that after a few weeks of running your awesome data processing application, there\u2019s a bug in your system and the data needs to be reprocessed?</p> <p>This is extremely difficult and often impossible to do in traditional systems. But, with an event streaming platform like Redpanda, we have an immutable record of all the individual events that took place, which makes rebuilding the state much easier. But with an immutable record of all of the individual events that took place, it\u2019s much easier with an event streaming platform like Redpanda.</p> <p>Now, immutability doesn\u2019t prevent us from representing the current snapshot of our data. We can still do that with streaming data platforms, just like we can with more traditional systems, as well.</p> <p>Streaming data platforms just give a new capability, which is the ability to capture the current state of our data and the historical record of state changes.</p>"},{"location":"streaming/#communication-patterns","title":"Communication Patterns","text":"<p>Perhaps the most common form of communication between systems is the client-server model. This form of point-to-point communication involves two parties talking directly to each other. In other words, a client sends a request to a server, and waits for a response. In order to talk to each other, the systems in a client-server architecture have to:</p> <p>Know about each other so that they can exchange information directly</p> <p>Anticipate and handle a wide range of errors that can occur when talking to each other</p> <p>And because the systems talk to each other directly, they are said to be tightly coupled.</p> <p>When the number of systems is small, this is somewhat manageable. However, it becomes difficult to scale as the number of systems begin to increase. As you can see in the following diagram, we can easily end up with complicated call chains or elaborate webs of communication.</p> <p>These issues have become even more pronounced over recent years, especially as organizations adopt microservice architectures, which involves standing up many separate and specialized services.</p> <p>And, while many service mesh technologies have been created to try to hide some of this complexity, they often add to the operational burden while only partially addressing the issues of point-to-point communication.</p> <p>Now, as we mentioned earlier, events are the centerpiece of event-driven architectures. But in client-server architectures, the data being communicated is less clear, and in fact, most of the attention has not been focused on the data itself, but on requests and responses. However, if we actually look at what data is being communicated,  we can see that they are usually generic messages, or commands. We\u2019ll refer to these as commands going forward.</p> <p>As opposed to an event, which is just a simple, timestamped fact, commands are data packets that are intended to trigger some action. For example, Client A may tell a REST API to GET the items for a page, or PUT something in a database. Command-driven architectures reflect the tightly-coupled nature of the underlying systems, since you have one system telling another what to do, directly.</p> <p>Events, on the other hand, are just facts. Just like in the physical world, facts can be observed, reacted to, and processed by any number of systems. And this simple realization is the starting point for how event-driven systems communicate.</p> <p>The basic principle is that you can\u2019t anticipate everything that downstream systems may want to do with your data, so why even try? It\u2019s best to just publish the event somewhere, and allow any system that is interested in it to simply subscribe to the stream in order to read the data.</p> <p>This is called the publish-subscribe communication pattern, or pub/sub for short. In this form of communication, there are two main entities: producers, which are responsible for publishing messages to a stream, and consumers, which read the messages from any stream they happen to be interested in. We\u2019ll go into more detail in the next video, but the key idea is that producers and consumers don\u2019t talk to each other directly, and aren\u2019t even aware of each other. This is a much more scalable form of communication, and it is leveraged by event streaming platforms like Redpanda.</p> <p>A user places an order. You may have one system that needs to process the order, a separate analytical system that enriches the data and makes it available for reporting, and a third system that acts as a product recommendation engine, which needs to consume orders so that it can make recommendations about different products.</p> <p>There\u2019s one event and three different systems that need to process it. A command-driven approach would involve issuing a separate command to each system, which is error-prone and inefficient.</p> <p>It would be much better to represent the order as a simple event, and publish it somewhere that other systems can observe and process at will. The producing application shouldn\u2019t need to know about each consumer application that needs to read the data.</p> <p>Event-driven systems are designed around this simple concept. When we begin to model data as pure events, this is called event-first thinking.</p> <p>There are a few other issues with point-to-point communication, as well. When systems talk to each other directly, receiving systems can easily get overwhelmed since they operate at the whims of whatever system is making a request. Therefore, high-volume or bursty traffic patterns can cause issues.</p> <p>It would be much better to have a communication buffer that allows downstream systems to react to data asynchronously and at their own pace. And this is exactly what event streaming platforms like Redpanda offer, and we\u2019ll dig more into that in the next video.</p> <p>Finally,  point-to-point communication also comes with weaker delivery guarantees. If the receiving system is offline, the requesting system will need to retry and may eventually even give up. This is another problem that Redpanda addresses, which tolerates failures much better. We\u2019ll see examples of this throughout this course.</p>"},{"location":"streaming/#queue-architecture","title":"Queue Architecture","text":"<p>For example, RabbitMQ is often compared with pub/sub systems like Kafka. </p> <p>However, queuing systems are very different. They have a single-consumer model and destructive consumer semantics. So once a message is processed, it is \u201cpopped\u201d off the queue and unavailable to other systems, unless you explicitly duplicate that data.</p> <p>Communication in queuing systems is similar to the command-driven approach we discussed earlier. Producers write messages to specific and sometimes separate locations to kick off specific tasks. Even though communication is asynchronous with queues, data producers and consumers are still tightly coupled, just like they are with the request-response pattern.</p> <p>Also, queues have traditionally been much more difficult to scale. The brokers in a queueing system often take on extra computational burdens, and are sometimes called \u201csmart brokers\u201d to reflect this. For example, these brokers often have to manage cursor positions for the queues, manage locks for the messages that are out for delivery, execute more complex routing logic, and more. These burdens require a greater number of resources to achieve the same kind of throughput as pub/sub systems with simpler broker models.</p> <p>Revisiting the previous Etsy example, a queuing system would not be appropriate since multiple systems need to process the event. Since queues have destructive consumer semantics and a single consumer model, you\u2019d have to duplicate the event multiple times, in separate queues, for each downstream system that needs to read it. In this case, we\u2019d need to duplicate the data in an order processing queue, an analytics queue, and finally in a recommendation queue. That\u2019s inefficient.</p>"},{"location":"streaming/#landscape-of-streaming","title":"Landscape of Streaming","text":""},{"location":"streaming/#cloud-provider-options","title":"Cloud Provider Options","text":""},{"location":"streaming/#apache-pulsar","title":"Apache Pulsar","text":"<p>One thing that is great about Pulsar though is that it has something called Pulsar functions, which allow you to run stream processing apps on the Pulsar cluster, without deploying a separate system. So despite being less battletested than some of the other options we'll cover, we do like that feature.</p> <p></p>"},{"location":"streaming/#apache-kafka","title":"Apache Kafka","text":"<p>Kafka is a dominating force in the event streaming space. It was developed at LinkedIn, has a pull-based pub/sub communication model that is optimized for low-latency and high throughput, has a great API, a huge ecosystem of integrations and tooling, and has been sufficiently battle tested.</p> <p>However, Kafka has a reputation for being complicated to manage, and its architecture, while less complex than Pulsar\u2019s, is still somewhat complex. For example, it\u2019s JVM-based, which often entails complex tuning, GC pauses, and low-level monitoring to make sure the systems and JVM processes are healthy. Also, while Zookeeper has been slated for removal, it was still recommended for production deployments of Kafka at the time of this recording, which adds to the operational overhead.</p> <p>Fully managed options are available to help alleviate the operational concerns, including Confluent Cloud, AWS MSK, and Aiven, to name a few. But they often come at a high cost, since at the end of the day, it is still difficult to manage, but you are paying someone else to take on this burden.</p> <p>Kafka also lacks Pulsar\u2019s strongest feature, which is the ability to do stream processing and transformations where the data lives, instead of shipping large amounts of data to some separate system for processing. These network trips add up and can introduce undesirable latency when it comes time to process your event data.</p>"},{"location":"streaming/#redpanda_1","title":"Redpanda","text":""},{"location":"streaming/#redpanda-overview","title":"Redpanda Overview","text":"<p>we learned how Redpanda, like many other data streaming platforms, leverages the pub/sub communication pattern. In other words, instead of two systems exchanging data directly, systems exchange data using a centralized communication hub. In the case of Redpanda, the communication hub is the Redpanda cluster.</p> <p>A Redpanda cluster is made up of one or more servers, called brokers. As their name suggests, brokers are responsible for mediating communication between systems that produce data and systems that consume data. This allows us to build systems that are completely decoupled from each other, since data producers and consumers don\u2019t need to know about each other, much less worry about all of the pitfalls involved in direct communication. Instead, systems communicate with the brokers, and Redpanda makes sure the data is communicated to the correct parties.</p> <p>In addition to being the communication point for data producers and consumers, brokers are also responsible for storing data. This is different from more complex architectures, like what we find in Apache Pulsar, where brokers only handle communication and another set of servers is responsible for storing the data. Redpanda optimizes for operational simplicity, and this is apparent even in its broker model.</p> <p>Most of the time, you\u2019ll usually have multiple brokers in a production Redpanda cluster. This will provide redundancy and improve performance by distributing the work across many servers. While it's possible to have a cluster with just a single broker, that's fairly unusual in production.</p> <p>The data that is stored on brokers is organized into topics. A topic is a named stream or channel where related events are published. For example, you might have a \"payments\" topic that contains the payment events for a given business. In general, a topic is a similar level of granularity to tables in a database. We can create any number of topics, which are identified by a human-readable name.</p> <p>Each record within a topic is represented as a key-value pair, with a timestamp and optional set of headers. The key of each record can be used to group related events, which we\u2019ll see in the next video, and the value contains the payload (for example, a byte-encoded JSON string), with all of the details about the event.</p> <p>Topics can be configured in many different ways, and we\u2019ll talk about this in more detail in the Redpanda Operations course, but they are usually distinguished by their data retention strategy. Compacted topics retain the latest record for each unique record key, while uncompacted topics delete data after a configurable retention period.</p> <p>As you might imagine, Redpanda topics can get very large, so we need some way of horizontally scaling topics across many brokers in a Redpanda cluster. Otherwise, if topics could only be stored on a single machine, their maximum size would be limited to the largest machine in the cluster. This wouldn\u2019t be ideal. Instead, we need to leverage the power of multiple machines to handle large amounts of event data, and subsequently, large topics.</p> <p>But it\u2019s hard to distribute and share just one of something, which is why topics are subdivided into smaller units, called partitions. Each topic partition contains a subset of the topic data (unless your topic was configured with just 1 partition, which isn\u2019t recommended in production since it hinders scalability). We\u2019ll get into more detail about the storage abstraction that partitions use in the next video, but for now, just know that the number of partitions is configurable when you create a topic, and generally a higher number of partitions improves our ability to scale.</p> <p>Since partitions only contain a subset of the topic data, we can easily distribute them across many different brokers, allowing our topics to grow freely. Data is also replicated at the partition level in a Redpanda cluster.</p> <p>So how does data actually make its way to a Redpanda topic? Well, in order to publish data to a topic, we use a producer. Producers don\u2019t need to know who will come along to read the data they are producing, they simply publish the data to a topic, making it available to any downstream system that happens to be interested in it. Furthermore, the same Apache Kafka Producer API that is used to publish events to Kafka topics can be used to publish data to Redpanda topics. This allows Redpanda to work seamlessly with other tools and integrations in the Kafka ecosystem.</p> <p>Finally, to retrieve data from Redpanda, we use a consumer. Consumers read data from one or more topics using a sequential read pattern. In other words, they read messages in the order they were produced. More on this in a minute.</p> <p>But the really powerful thing about consumers is that they can work together to read data from a topic. This is really useful for large topics that require multiple consumers in order to keep up with the message volume.</p> <p>A group of cooperating consumers is called a consumer group, and this is a key component that allows us to parallelize work and build high-throughput systems that process data efficiently with minimal latency. It\u2019s like the saying, \u201cmany hands make light work,\u201d but in Redpanda\u2019s case, \u201cmany consumers make light work.\u201d</p> <p>Now, let\u2019s take a minute to talk about how work is distributed across members of a consumer group. Each member of a consumer group is assigned one or more partitions in a Kafka topic. This is another important role of partitions - they not only allow us to scale from a storage perspective, but they also allow us to efficiently divide work across many consumers, and therefore scale our read workloads.</p> <p>A special consumer in each consumer group, called the group leader, will handle the partition assignments for each consumer. But consumers never read from the same partition as another member of the group, since this would lead to double-processing.</p> <p>However, consumers from different consumer groups can read from the same partition since they are working independently, and this is one of the main benefits of the pub/sub communication pattern that we discussed in the last chapter.</p> <p>Furthermore, partition assignment among consumers can change over time, depending on the health of each consumer in the group. If a consumer becomes unhealthy (for example, due to a bug in the code or hardware failure), its partitions will be revoked and reassigned to a healthy member of the group. This makes Redpanda consumers fault tolerant and highly available, since they can continue processing data even if some members of the group are unhealthy, or if consumers go offline for any reason (for example, during a rolling restart of your application).</p> <p>This brings us back to brokers. We already mentioned two roles of Redpanda brokers: storing data and mediating communication between producers and consumers. But brokers actually have another role, as well, which is monitoring the health of consumer groups. Consumers must heartbeat back to Redpanda brokers to prove that they are alive and well. If a consumer fails to heartbeat in time, the broker assigned to the consumer group, called the group coordinator, will trigger a rebalance, allowing the other consumers to cope with the failure and continue processing data. This involves sending a message to the group leader, who actually executes the rebalance.</p> <p>We\u2019ve covered a good bit, so let's put everything together with an example. We'll use the same example from our previous course -- orders at Etsy. Let's say we have a set of microservices : </p> <p>Checkout Service, which is the service users interact with to place orders</p> <p>Order Service, which handles the backend processing of order events (payments, shipping, etc)</p> <p>Reporting Service, which transforms and enriches order data for analytic purposes</p> <p>Recommendations Service, which uses the order data to build recommendations for future orders </p> <p>To start, we'll have a Redpanda cluster with four brokers. Remember, multiple brokers are recommended for production clusters so that we can easily scale and replicate our data.</p> <p>Within our Redpanda cluster, we\u2019ll have a topic named \"orders\". Like a table in a database, this topic allows us to group related records for easy storage and retrieval.</p> <p>Our topic has been configured with 32 partitions so that it can be split across multiple brokers, and so that we can split the read workloads across multiple consumers in each consumer group that needs to read from it.</p> <p>Every time a user places an order, CheckoutService publishes an event to the orders topic. The OrderService, ReportingService, and RecommendationsService, all have consumers listening to the orders topic. As orders come in, each service will process the order events for their unique business cases. Since each service is consuming from the orders topic using a unique consumer group, they can easily parallelize work and handle failures.</p>"}]}